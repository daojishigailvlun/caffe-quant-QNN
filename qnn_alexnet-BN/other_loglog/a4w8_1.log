I0111 17:54:24.333189 20349 caffe.cpp:275] Use GPU with device ID 2
I0111 17:54:24.364511 20349 caffe.cpp:279] GPU device name: GeForce GTX 1080 Ti
I0111 17:54:25.171393 20349 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0111 17:54:25.171476 20349 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_5_TRAIN
I0111 17:54:25.171857 20349 net.cpp:51] Initializing net from parameters: 
name: "AlexNet-BN"
state {
  phase: TEST
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.68
  }
  data_param {
    source: "/home/DATA/imagenet_resize/lmdb_resize/ilsvrc12_val_lmdb"
    batch_size: 200
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "QuanConvolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 96
    pad: 2
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.872089
    range_high: 0.927025
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "pool1"
  top: "pool1"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 13.965383
  }
}
layer {
  name: "conv2"
  type: "QuanConvolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.806018
    range_high: 1.5182
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "pool2"
  top: "pool2"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 6.74956
  }
}
layer {
  name: "conv3"
  type: "QuanConvolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.621858
    range_high: 0.673858
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv3"
  top: "conv3"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 8.48454
  }
}
layer {
  name: "conv4"
  type: "QuanConvolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.376634
    range_high: 0.341829
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv4"
  top: "conv4"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 7.3369722
  }
}
layer {
  name: "conv5"
  type: "QuanConvolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.347658
    range_high: 0.330042
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "scale5"
  type: "Scale"
  bottom: "conv5"
  top: "conv5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "pool5"
  top: "pool5"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 8.7577133
  }
}
layer {
  name: "fc6"
  type: "QuanInnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.160061
    range_high: 0.148064
  }
}
layer {
  name: "bn6"
  type: "BatchNorm"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "scale6"
  type: "Scale"
  bottom: "fc6"
  top: "fc6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "fc6"
  top: "fc6"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 1.659358
  }
}
layer {
  name: "fc7"
  type: "QuanInnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.162636
    range_high: 0.197684
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "scale7"
  type: "Scale"
  bottom: "fc7"
  top: "fc7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "fc7"
  top: "fc7"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 7.5598521
  }
}
layer {
  name: "fc8"
  type: "QuanInnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.141857
    range_high: 0.274249
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0111 17:54:25.172168 20349 layer_factory.hpp:77] Creating layer data
I0111 17:54:25.198315 20349 db_lmdb.cpp:35] Opened lmdb /home/DATA/imagenet_resize/lmdb_resize/ilsvrc12_val_lmdb
I0111 17:54:25.207599 20349 net.cpp:84] Creating Layer data
I0111 17:54:25.207653 20349 net.cpp:380] data -> data
I0111 17:54:25.207708 20349 net.cpp:380] data -> label
I0111 17:54:25.238942 20349 data_layer.cpp:45] output data size: 200,3,224,224
I0111 17:54:25.670658 20349 net.cpp:122] Setting up data
I0111 17:54:25.670727 20349 net.cpp:129] Top shape: 200 3 224 224 (30105600)
I0111 17:54:25.670737 20349 net.cpp:129] Top shape: 200 (200)
I0111 17:54:25.670745 20349 net.cpp:137] Memory required for data: 120423200
I0111 17:54:25.670784 20349 layer_factory.hpp:77] Creating layer label_data_1_split
I0111 17:54:25.670814 20349 net.cpp:84] Creating Layer label_data_1_split
I0111 17:54:25.670826 20349 net.cpp:406] label_data_1_split <- label
I0111 17:54:25.670857 20349 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0111 17:54:25.670878 20349 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0111 17:54:25.670892 20349 net.cpp:380] label_data_1_split -> label_data_1_split_2
I0111 17:54:25.671031 20349 net.cpp:122] Setting up label_data_1_split
I0111 17:54:25.671084 20349 net.cpp:129] Top shape: 200 (200)
I0111 17:54:25.671097 20349 net.cpp:129] Top shape: 200 (200)
I0111 17:54:25.671110 20349 net.cpp:129] Top shape: 200 (200)
I0111 17:54:25.671119 20349 net.cpp:137] Memory required for data: 120425600
I0111 17:54:25.671133 20349 layer_factory.hpp:77] Creating layer conv1
I0111 17:54:25.671175 20349 net.cpp:84] Creating Layer conv1
I0111 17:54:25.671186 20349 net.cpp:406] conv1 <- data
I0111 17:54:25.671205 20349 net.cpp:380] conv1 -> conv1
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.872089;  range_high=0.927025
I0111 17:54:25.780403 20349 net.cpp:122] Setting up conv1
I0111 17:54:25.780455 20349 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 17:54:25.780478 20349 net.cpp:137] Memory required for data: 352745600
I0111 17:54:25.780546 20349 layer_factory.hpp:77] Creating layer bn1
I0111 17:54:25.780580 20349 net.cpp:84] Creating Layer bn1
I0111 17:54:25.780594 20349 net.cpp:406] bn1 <- conv1
I0111 17:54:25.780611 20349 net.cpp:367] bn1 -> conv1 (in-place)
I0111 17:54:25.780911 20349 net.cpp:122] Setting up bn1
I0111 17:54:25.780941 20349 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 17:54:25.780949 20349 net.cpp:137] Memory required for data: 585065600
I0111 17:54:25.780972 20349 layer_factory.hpp:77] Creating layer scale1
I0111 17:54:25.780992 20349 net.cpp:84] Creating Layer scale1
I0111 17:54:25.781002 20349 net.cpp:406] scale1 <- conv1
I0111 17:54:25.781014 20349 net.cpp:367] scale1 -> conv1 (in-place)
I0111 17:54:25.781111 20349 layer_factory.hpp:77] Creating layer scale1
I0111 17:54:25.781311 20349 net.cpp:122] Setting up scale1
I0111 17:54:25.781340 20349 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 17:54:25.781348 20349 net.cpp:137] Memory required for data: 817385600
I0111 17:54:25.781364 20349 layer_factory.hpp:77] Creating layer relu1
I0111 17:54:25.781419 20349 net.cpp:84] Creating Layer relu1
I0111 17:54:25.781430 20349 net.cpp:406] relu1 <- conv1
I0111 17:54:25.781443 20349 net.cpp:367] relu1 -> conv1 (in-place)
I0111 17:54:25.781457 20349 net.cpp:122] Setting up relu1
I0111 17:54:25.781479 20349 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 17:54:25.781487 20349 net.cpp:137] Memory required for data: 1049705600
I0111 17:54:25.781494 20349 layer_factory.hpp:77] Creating layer pool1
I0111 17:54:25.781512 20349 net.cpp:84] Creating Layer pool1
I0111 17:54:25.781520 20349 net.cpp:406] pool1 <- conv1
I0111 17:54:25.781545 20349 net.cpp:380] pool1 -> pool1
I0111 17:54:25.781625 20349 net.cpp:122] Setting up pool1
I0111 17:54:25.781638 20349 net.cpp:129] Top shape: 200 96 27 27 (13996800)
I0111 17:54:25.781643 20349 net.cpp:137] Memory required for data: 1105692800
I0111 17:54:25.781661 20349 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 17:54:25.781672 20349 net.cpp:84] Creating Layer quantized_conv1
I0111 17:54:25.781680 20349 net.cpp:406] quantized_conv1 <- pool1
I0111 17:54:25.781690 20349 net.cpp:367] quantized_conv1 -> pool1 (in-place)
I0111 17:54:25.781703 20349 net.cpp:122] Setting up quantized_conv1
I0111 17:54:25.781713 20349 net.cpp:129] Top shape: 200 96 27 27 (13996800)
I0111 17:54:25.781724 20349 net.cpp:137] Memory required for data: 1161680000
I0111 17:54:25.781736 20349 layer_factory.hpp:77] Creating layer conv2
I0111 17:54:25.781764 20349 net.cpp:84] Creating Layer conv2
I0111 17:54:25.781780 20349 net.cpp:406] conv2 <- pool1
I0111 17:54:25.781810 20349 net.cpp:380] conv2 -> conv2
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.806018;  range_high=1.5182
I0111 17:54:25.797142 20349 net.cpp:122] Setting up conv2
I0111 17:54:25.797173 20349 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 17:54:25.797188 20349 net.cpp:137] Memory required for data: 1310979200
I0111 17:54:25.797216 20349 layer_factory.hpp:77] Creating layer bn2
I0111 17:54:25.797242 20349 net.cpp:84] Creating Layer bn2
I0111 17:54:25.797256 20349 net.cpp:406] bn2 <- conv2
I0111 17:54:25.797276 20349 net.cpp:367] bn2 -> conv2 (in-place)
I0111 17:54:25.797611 20349 net.cpp:122] Setting up bn2
I0111 17:54:25.797631 20349 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 17:54:25.797641 20349 net.cpp:137] Memory required for data: 1460278400
I0111 17:54:25.797667 20349 layer_factory.hpp:77] Creating layer scale2
I0111 17:54:25.797683 20349 net.cpp:84] Creating Layer scale2
I0111 17:54:25.797693 20349 net.cpp:406] scale2 <- conv2
I0111 17:54:25.797710 20349 net.cpp:367] scale2 -> conv2 (in-place)
I0111 17:54:25.797792 20349 layer_factory.hpp:77] Creating layer scale2
I0111 17:54:25.798017 20349 net.cpp:122] Setting up scale2
I0111 17:54:25.798039 20349 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 17:54:25.798049 20349 net.cpp:137] Memory required for data: 1609577600
I0111 17:54:25.798089 20349 layer_factory.hpp:77] Creating layer relu2
I0111 17:54:25.798105 20349 net.cpp:84] Creating Layer relu2
I0111 17:54:25.798115 20349 net.cpp:406] relu2 <- conv2
I0111 17:54:25.798127 20349 net.cpp:367] relu2 -> conv2 (in-place)
I0111 17:54:25.798141 20349 net.cpp:122] Setting up relu2
I0111 17:54:25.798153 20349 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 17:54:25.798163 20349 net.cpp:137] Memory required for data: 1758876800
I0111 17:54:25.798172 20349 layer_factory.hpp:77] Creating layer pool2
I0111 17:54:25.798198 20349 net.cpp:84] Creating Layer pool2
I0111 17:54:25.798209 20349 net.cpp:406] pool2 <- conv2
I0111 17:54:25.798223 20349 net.cpp:380] pool2 -> pool2
I0111 17:54:25.798292 20349 net.cpp:122] Setting up pool2
I0111 17:54:25.798310 20349 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 17:54:25.798319 20349 net.cpp:137] Memory required for data: 1793488000
I0111 17:54:25.798328 20349 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 17:54:25.798357 20349 net.cpp:84] Creating Layer quantized_conv1
I0111 17:54:25.798370 20349 net.cpp:406] quantized_conv1 <- pool2
I0111 17:54:25.798388 20349 net.cpp:367] quantized_conv1 -> pool2 (in-place)
I0111 17:54:25.798429 20349 net.cpp:122] Setting up quantized_conv1
I0111 17:54:25.798440 20349 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 17:54:25.798446 20349 net.cpp:137] Memory required for data: 1828099200
I0111 17:54:25.798454 20349 layer_factory.hpp:77] Creating layer conv3
I0111 17:54:25.798472 20349 net.cpp:84] Creating Layer conv3
I0111 17:54:25.798480 20349 net.cpp:406] conv3 <- pool2
I0111 17:54:25.798491 20349 net.cpp:380] conv3 -> conv3
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.621858;  range_high=0.673858
I0111 17:54:25.818089 20349 net.cpp:122] Setting up conv3
I0111 17:54:25.818121 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.818135 20349 net.cpp:137] Memory required for data: 1880016000
I0111 17:54:25.818161 20349 layer_factory.hpp:77] Creating layer bn3
I0111 17:54:25.818186 20349 net.cpp:84] Creating Layer bn3
I0111 17:54:25.818202 20349 net.cpp:406] bn3 <- conv3
I0111 17:54:25.818226 20349 net.cpp:367] bn3 -> conv3 (in-place)
I0111 17:54:25.818562 20349 net.cpp:122] Setting up bn3
I0111 17:54:25.818581 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.818593 20349 net.cpp:137] Memory required for data: 1931932800
I0111 17:54:25.818621 20349 layer_factory.hpp:77] Creating layer scale3
I0111 17:54:25.818645 20349 net.cpp:84] Creating Layer scale3
I0111 17:54:25.818657 20349 net.cpp:406] scale3 <- conv3
I0111 17:54:25.818671 20349 net.cpp:367] scale3 -> conv3 (in-place)
I0111 17:54:25.818742 20349 layer_factory.hpp:77] Creating layer scale3
I0111 17:54:25.818927 20349 net.cpp:122] Setting up scale3
I0111 17:54:25.818945 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.818955 20349 net.cpp:137] Memory required for data: 1983849600
I0111 17:54:25.818971 20349 layer_factory.hpp:77] Creating layer relu3
I0111 17:54:25.818986 20349 net.cpp:84] Creating Layer relu3
I0111 17:54:25.818996 20349 net.cpp:406] relu3 <- conv3
I0111 17:54:25.819013 20349 net.cpp:367] relu3 -> conv3 (in-place)
I0111 17:54:25.819028 20349 net.cpp:122] Setting up relu3
I0111 17:54:25.819041 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.819051 20349 net.cpp:137] Memory required for data: 2035766400
I0111 17:54:25.819061 20349 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 17:54:25.819077 20349 net.cpp:84] Creating Layer quantized_conv1
I0111 17:54:25.819087 20349 net.cpp:406] quantized_conv1 <- conv3
I0111 17:54:25.819100 20349 net.cpp:367] quantized_conv1 -> conv3 (in-place)
I0111 17:54:25.819119 20349 net.cpp:122] Setting up quantized_conv1
I0111 17:54:25.819133 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.819144 20349 net.cpp:137] Memory required for data: 2087683200
I0111 17:54:25.819154 20349 layer_factory.hpp:77] Creating layer conv4
I0111 17:54:25.819175 20349 net.cpp:84] Creating Layer conv4
I0111 17:54:25.819185 20349 net.cpp:406] conv4 <- conv3
I0111 17:54:25.819205 20349 net.cpp:380] conv4 -> conv4
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.376634;  range_high=0.341829
I0111 17:54:25.846616 20349 net.cpp:122] Setting up conv4
I0111 17:54:25.846669 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.846689 20349 net.cpp:137] Memory required for data: 2139600000
I0111 17:54:25.846710 20349 layer_factory.hpp:77] Creating layer bn4
I0111 17:54:25.846745 20349 net.cpp:84] Creating Layer bn4
I0111 17:54:25.846755 20349 net.cpp:406] bn4 <- conv4
I0111 17:54:25.846770 20349 net.cpp:367] bn4 -> conv4 (in-place)
I0111 17:54:25.847072 20349 net.cpp:122] Setting up bn4
I0111 17:54:25.847103 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.847112 20349 net.cpp:137] Memory required for data: 2191516800
I0111 17:54:25.847131 20349 layer_factory.hpp:77] Creating layer scale4
I0111 17:54:25.847146 20349 net.cpp:84] Creating Layer scale4
I0111 17:54:25.847167 20349 net.cpp:406] scale4 <- conv4
I0111 17:54:25.847178 20349 net.cpp:367] scale4 -> conv4 (in-place)
I0111 17:54:25.847265 20349 layer_factory.hpp:77] Creating layer scale4
I0111 17:54:25.847470 20349 net.cpp:122] Setting up scale4
I0111 17:54:25.847486 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.847506 20349 net.cpp:137] Memory required for data: 2243433600
I0111 17:54:25.847569 20349 layer_factory.hpp:77] Creating layer relu4
I0111 17:54:25.847587 20349 net.cpp:84] Creating Layer relu4
I0111 17:54:25.847595 20349 net.cpp:406] relu4 <- conv4
I0111 17:54:25.847605 20349 net.cpp:367] relu4 -> conv4 (in-place)
I0111 17:54:25.847628 20349 net.cpp:122] Setting up relu4
I0111 17:54:25.847638 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.847647 20349 net.cpp:137] Memory required for data: 2295350400
I0111 17:54:25.847657 20349 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 17:54:25.847671 20349 net.cpp:84] Creating Layer quantized_conv1
I0111 17:54:25.847681 20349 net.cpp:406] quantized_conv1 <- conv4
I0111 17:54:25.847698 20349 net.cpp:367] quantized_conv1 -> conv4 (in-place)
I0111 17:54:25.847715 20349 net.cpp:122] Setting up quantized_conv1
I0111 17:54:25.847730 20349 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 17:54:25.847741 20349 net.cpp:137] Memory required for data: 2347267200
I0111 17:54:25.847750 20349 layer_factory.hpp:77] Creating layer conv5
I0111 17:54:25.847771 20349 net.cpp:84] Creating Layer conv5
I0111 17:54:25.847779 20349 net.cpp:406] conv5 <- conv4
I0111 17:54:25.847790 20349 net.cpp:380] conv5 -> conv5
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.347658;  range_high=0.330042
I0111 17:54:25.867092 20349 net.cpp:122] Setting up conv5
I0111 17:54:25.867128 20349 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 17:54:25.867136 20349 net.cpp:137] Memory required for data: 2381878400
I0111 17:54:25.867148 20349 layer_factory.hpp:77] Creating layer bn5
I0111 17:54:25.867171 20349 net.cpp:84] Creating Layer bn5
I0111 17:54:25.867184 20349 net.cpp:406] bn5 <- conv5
I0111 17:54:25.867204 20349 net.cpp:367] bn5 -> conv5 (in-place)
I0111 17:54:25.867532 20349 net.cpp:122] Setting up bn5
I0111 17:54:25.867561 20349 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 17:54:25.867569 20349 net.cpp:137] Memory required for data: 2416489600
I0111 17:54:25.867600 20349 layer_factory.hpp:77] Creating layer scale5
I0111 17:54:25.867630 20349 net.cpp:84] Creating Layer scale5
I0111 17:54:25.867640 20349 net.cpp:406] scale5 <- conv5
I0111 17:54:25.867650 20349 net.cpp:367] scale5 -> conv5 (in-place)
I0111 17:54:25.867746 20349 layer_factory.hpp:77] Creating layer scale5
I0111 17:54:25.867923 20349 net.cpp:122] Setting up scale5
I0111 17:54:25.867941 20349 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 17:54:25.867951 20349 net.cpp:137] Memory required for data: 2451100800
I0111 17:54:25.867966 20349 layer_factory.hpp:77] Creating layer relu5
I0111 17:54:25.867980 20349 net.cpp:84] Creating Layer relu5
I0111 17:54:25.867990 20349 net.cpp:406] relu5 <- conv5
I0111 17:54:25.868005 20349 net.cpp:367] relu5 -> conv5 (in-place)
I0111 17:54:25.868019 20349 net.cpp:122] Setting up relu5
I0111 17:54:25.868032 20349 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 17:54:25.868041 20349 net.cpp:137] Memory required for data: 2485712000
I0111 17:54:25.868052 20349 layer_factory.hpp:77] Creating layer pool5
I0111 17:54:25.868067 20349 net.cpp:84] Creating Layer pool5
I0111 17:54:25.868075 20349 net.cpp:406] pool5 <- conv5
I0111 17:54:25.868089 20349 net.cpp:380] pool5 -> pool5
I0111 17:54:25.868154 20349 net.cpp:122] Setting up pool5
I0111 17:54:25.868170 20349 net.cpp:129] Top shape: 200 256 6 6 (1843200)
I0111 17:54:25.868180 20349 net.cpp:137] Memory required for data: 2493084800
I0111 17:54:25.868191 20349 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 17:54:25.868206 20349 net.cpp:84] Creating Layer quantized_conv1
I0111 17:54:25.868216 20349 net.cpp:406] quantized_conv1 <- pool5
I0111 17:54:25.868229 20349 net.cpp:367] quantized_conv1 -> pool5 (in-place)
I0111 17:54:25.868247 20349 net.cpp:122] Setting up quantized_conv1
I0111 17:54:25.868263 20349 net.cpp:129] Top shape: 200 256 6 6 (1843200)
I0111 17:54:25.868274 20349 net.cpp:137] Memory required for data: 2500457600
I0111 17:54:25.868283 20349 layer_factory.hpp:77] Creating layer fc6
I0111 17:54:25.868295 20349 net.cpp:84] Creating Layer fc6
I0111 17:54:25.868302 20349 net.cpp:406] fc6 <- pool5
I0111 17:54:25.868345 20349 net.cpp:380] fc6 -> fc6
I0111 17:54:25.868363 20349 quan_inner_product_layer.cpp:21] 4096   4096
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.160061;  range_high=0.148064
I0111 17:54:26.600857 20349 net.cpp:122] Setting up fc6
I0111 17:54:26.600903 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.600910 20349 net.cpp:137] Memory required for data: 2503734400
I0111 17:54:26.600927 20349 layer_factory.hpp:77] Creating layer bn6
I0111 17:54:26.600952 20349 net.cpp:84] Creating Layer bn6
I0111 17:54:26.600962 20349 net.cpp:406] bn6 <- fc6
I0111 17:54:26.600987 20349 net.cpp:367] bn6 -> fc6 (in-place)
I0111 17:54:26.601258 20349 net.cpp:122] Setting up bn6
I0111 17:54:26.601274 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.601287 20349 net.cpp:137] Memory required for data: 2507011200
I0111 17:54:26.601300 20349 layer_factory.hpp:77] Creating layer scale6
I0111 17:54:26.601336 20349 net.cpp:84] Creating Layer scale6
I0111 17:54:26.601343 20349 net.cpp:406] scale6 <- fc6
I0111 17:54:26.601361 20349 net.cpp:367] scale6 -> fc6 (in-place)
I0111 17:54:26.601429 20349 layer_factory.hpp:77] Creating layer scale6
I0111 17:54:26.601585 20349 net.cpp:122] Setting up scale6
I0111 17:54:26.601600 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.601613 20349 net.cpp:137] Memory required for data: 2510288000
I0111 17:54:26.601622 20349 layer_factory.hpp:77] Creating layer relu6
I0111 17:54:26.601632 20349 net.cpp:84] Creating Layer relu6
I0111 17:54:26.601639 20349 net.cpp:406] relu6 <- fc6
I0111 17:54:26.601651 20349 net.cpp:367] relu6 -> fc6 (in-place)
I0111 17:54:26.601663 20349 net.cpp:122] Setting up relu6
I0111 17:54:26.601671 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.601678 20349 net.cpp:137] Memory required for data: 2513564800
I0111 17:54:26.601683 20349 layer_factory.hpp:77] Creating layer drop6
I0111 17:54:26.601704 20349 net.cpp:84] Creating Layer drop6
I0111 17:54:26.601711 20349 net.cpp:406] drop6 <- fc6
I0111 17:54:26.601722 20349 net.cpp:367] drop6 -> fc6 (in-place)
I0111 17:54:26.601764 20349 net.cpp:122] Setting up drop6
I0111 17:54:26.601778 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.601794 20349 net.cpp:137] Memory required for data: 2516841600
I0111 17:54:26.601802 20349 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 17:54:26.601840 20349 net.cpp:84] Creating Layer quantized_conv1
I0111 17:54:26.601850 20349 net.cpp:406] quantized_conv1 <- fc6
I0111 17:54:26.601861 20349 net.cpp:367] quantized_conv1 -> fc6 (in-place)
I0111 17:54:26.601873 20349 net.cpp:122] Setting up quantized_conv1
I0111 17:54:26.601883 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.601903 20349 net.cpp:137] Memory required for data: 2520118400
I0111 17:54:26.601912 20349 layer_factory.hpp:77] Creating layer fc7
I0111 17:54:26.601925 20349 net.cpp:84] Creating Layer fc7
I0111 17:54:26.601933 20349 net.cpp:406] fc7 <- fc6
I0111 17:54:26.601943 20349 net.cpp:380] fc7 -> fc7
I0111 17:54:26.601958 20349 quan_inner_product_layer.cpp:21] 4096   4096
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.162636;  range_high=0.197684
I0111 17:54:26.896082 20349 net.cpp:122] Setting up fc7
I0111 17:54:26.896152 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.896168 20349 net.cpp:137] Memory required for data: 2523395200
I0111 17:54:26.896210 20349 layer_factory.hpp:77] Creating layer bn7
I0111 17:54:26.896240 20349 net.cpp:84] Creating Layer bn7
I0111 17:54:26.896262 20349 net.cpp:406] bn7 <- fc7
I0111 17:54:26.896282 20349 net.cpp:367] bn7 -> fc7 (in-place)
I0111 17:54:26.896564 20349 net.cpp:122] Setting up bn7
I0111 17:54:26.896596 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.896610 20349 net.cpp:137] Memory required for data: 2526672000
I0111 17:54:26.896642 20349 layer_factory.hpp:77] Creating layer scale7
I0111 17:54:26.896679 20349 net.cpp:84] Creating Layer scale7
I0111 17:54:26.896687 20349 net.cpp:406] scale7 <- fc7
I0111 17:54:26.896705 20349 net.cpp:367] scale7 -> fc7 (in-place)
I0111 17:54:26.896800 20349 layer_factory.hpp:77] Creating layer scale7
I0111 17:54:26.897011 20349 net.cpp:122] Setting up scale7
I0111 17:54:26.897037 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.897112 20349 net.cpp:137] Memory required for data: 2529948800
I0111 17:54:26.897143 20349 layer_factory.hpp:77] Creating layer relu7
I0111 17:54:26.897166 20349 net.cpp:84] Creating Layer relu7
I0111 17:54:26.897176 20349 net.cpp:406] relu7 <- fc7
I0111 17:54:26.897188 20349 net.cpp:367] relu7 -> fc7 (in-place)
I0111 17:54:26.897203 20349 net.cpp:122] Setting up relu7
I0111 17:54:26.897213 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.897220 20349 net.cpp:137] Memory required for data: 2533225600
I0111 17:54:26.897228 20349 layer_factory.hpp:77] Creating layer drop7
I0111 17:54:26.897251 20349 net.cpp:84] Creating Layer drop7
I0111 17:54:26.897267 20349 net.cpp:406] drop7 <- fc7
I0111 17:54:26.897280 20349 net.cpp:367] drop7 -> fc7 (in-place)
I0111 17:54:26.897330 20349 net.cpp:122] Setting up drop7
I0111 17:54:26.897347 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.897356 20349 net.cpp:137] Memory required for data: 2536502400
I0111 17:54:26.897367 20349 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 17:54:26.897403 20349 net.cpp:84] Creating Layer quantized_conv1
I0111 17:54:26.897410 20349 net.cpp:406] quantized_conv1 <- fc7
I0111 17:54:26.897423 20349 net.cpp:367] quantized_conv1 -> fc7 (in-place)
I0111 17:54:26.897440 20349 net.cpp:122] Setting up quantized_conv1
I0111 17:54:26.897450 20349 net.cpp:129] Top shape: 200 4096 (819200)
I0111 17:54:26.897457 20349 net.cpp:137] Memory required for data: 2539779200
I0111 17:54:26.897464 20349 layer_factory.hpp:77] Creating layer fc8
I0111 17:54:26.897478 20349 net.cpp:84] Creating Layer fc8
I0111 17:54:26.897485 20349 net.cpp:406] fc8 <- fc7
I0111 17:54:26.897495 20349 net.cpp:380] fc8 -> fc8
I0111 17:54:26.897512 20349 quan_inner_product_layer.cpp:21] 1000   1000
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.141857;  range_high=0.274249
I0111 17:54:26.980592 20349 net.cpp:122] Setting up fc8
I0111 17:54:26.980639 20349 net.cpp:129] Top shape: 200 1000 (200000)
I0111 17:54:26.980648 20349 net.cpp:137] Memory required for data: 2540579200
I0111 17:54:26.980666 20349 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0111 17:54:26.980690 20349 net.cpp:84] Creating Layer fc8_fc8_0_split
I0111 17:54:26.980700 20349 net.cpp:406] fc8_fc8_0_split <- fc8
I0111 17:54:26.980731 20349 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0111 17:54:26.980753 20349 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0111 17:54:26.980777 20349 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_2
I0111 17:54:26.980866 20349 net.cpp:122] Setting up fc8_fc8_0_split
I0111 17:54:26.980880 20349 net.cpp:129] Top shape: 200 1000 (200000)
I0111 17:54:26.980901 20349 net.cpp:129] Top shape: 200 1000 (200000)
I0111 17:54:26.980907 20349 net.cpp:129] Top shape: 200 1000 (200000)
I0111 17:54:26.980913 20349 net.cpp:137] Memory required for data: 2542979200
I0111 17:54:26.980921 20349 layer_factory.hpp:77] Creating layer accuracy
I0111 17:54:26.980936 20349 net.cpp:84] Creating Layer accuracy
I0111 17:54:26.980942 20349 net.cpp:406] accuracy <- fc8_fc8_0_split_0
I0111 17:54:26.980952 20349 net.cpp:406] accuracy <- label_data_1_split_0
I0111 17:54:26.980962 20349 net.cpp:380] accuracy -> accuracy
I0111 17:54:26.980978 20349 net.cpp:122] Setting up accuracy
I0111 17:54:26.980988 20349 net.cpp:129] Top shape: (1)
I0111 17:54:26.980993 20349 net.cpp:137] Memory required for data: 2542979204
I0111 17:54:26.981001 20349 layer_factory.hpp:77] Creating layer accuracy_5
I0111 17:54:26.981016 20349 net.cpp:84] Creating Layer accuracy_5
I0111 17:54:26.981024 20349 net.cpp:406] accuracy_5 <- fc8_fc8_0_split_1
I0111 17:54:26.981034 20349 net.cpp:406] accuracy_5 <- label_data_1_split_1
I0111 17:54:26.981045 20349 net.cpp:380] accuracy_5 -> accuracy_5
I0111 17:54:26.981065 20349 net.cpp:122] Setting up accuracy_5
I0111 17:54:26.981073 20349 net.cpp:129] Top shape: (1)
I0111 17:54:26.981081 20349 net.cpp:137] Memory required for data: 2542979208
I0111 17:54:26.981088 20349 layer_factory.hpp:77] Creating layer loss
I0111 17:54:26.981099 20349 net.cpp:84] Creating Layer loss
I0111 17:54:26.981107 20349 net.cpp:406] loss <- fc8_fc8_0_split_2
I0111 17:54:26.981163 20349 net.cpp:406] loss <- label_data_1_split_2
I0111 17:54:26.981173 20349 net.cpp:380] loss -> loss
I0111 17:54:26.981189 20349 layer_factory.hpp:77] Creating layer loss
I0111 17:54:26.983100 20349 net.cpp:122] Setting up loss
I0111 17:54:26.983153 20349 net.cpp:129] Top shape: (1)
I0111 17:54:26.983160 20349 net.cpp:132]     with loss weight 1
I0111 17:54:26.983183 20349 net.cpp:137] Memory required for data: 2542979212
I0111 17:54:26.983196 20349 net.cpp:198] loss needs backward computation.
I0111 17:54:26.983214 20349 net.cpp:200] accuracy_5 does not need backward computation.
I0111 17:54:26.983223 20349 net.cpp:200] accuracy does not need backward computation.
I0111 17:54:26.983233 20349 net.cpp:198] fc8_fc8_0_split needs backward computation.
I0111 17:54:26.983242 20349 net.cpp:198] fc8 needs backward computation.
I0111 17:54:26.983249 20349 net.cpp:198] quantized_conv1 needs backward computation.
I0111 17:54:26.983256 20349 net.cpp:198] drop7 needs backward computation.
I0111 17:54:26.983263 20349 net.cpp:198] relu7 needs backward computation.
I0111 17:54:26.983270 20349 net.cpp:198] scale7 needs backward computation.
I0111 17:54:26.983278 20349 net.cpp:198] bn7 needs backward computation.
I0111 17:54:26.983285 20349 net.cpp:198] fc7 needs backward computation.
I0111 17:54:26.983292 20349 net.cpp:198] quantized_conv1 needs backward computation.
I0111 17:54:26.983299 20349 net.cpp:198] drop6 needs backward computation.
I0111 17:54:26.983306 20349 net.cpp:198] relu6 needs backward computation.
I0111 17:54:26.983314 20349 net.cpp:198] scale6 needs backward computation.
I0111 17:54:26.983320 20349 net.cpp:198] bn6 needs backward computation.
I0111 17:54:26.983327 20349 net.cpp:198] fc6 needs backward computation.
I0111 17:54:26.983335 20349 net.cpp:198] quantized_conv1 needs backward computation.
I0111 17:54:26.983342 20349 net.cpp:198] pool5 needs backward computation.
I0111 17:54:26.983350 20349 net.cpp:198] relu5 needs backward computation.
I0111 17:54:26.983356 20349 net.cpp:198] scale5 needs backward computation.
I0111 17:54:26.983363 20349 net.cpp:198] bn5 needs backward computation.
I0111 17:54:26.983371 20349 net.cpp:198] conv5 needs backward computation.
I0111 17:54:26.983384 20349 net.cpp:198] quantized_conv1 needs backward computation.
I0111 17:54:26.983398 20349 net.cpp:198] relu4 needs backward computation.
I0111 17:54:26.983407 20349 net.cpp:198] scale4 needs backward computation.
I0111 17:54:26.983414 20349 net.cpp:198] bn4 needs backward computation.
I0111 17:54:26.983422 20349 net.cpp:198] conv4 needs backward computation.
I0111 17:54:26.983429 20349 net.cpp:198] quantized_conv1 needs backward computation.
I0111 17:54:26.983436 20349 net.cpp:198] relu3 needs backward computation.
I0111 17:54:26.983444 20349 net.cpp:198] scale3 needs backward computation.
I0111 17:54:26.983451 20349 net.cpp:198] bn3 needs backward computation.
I0111 17:54:26.983458 20349 net.cpp:198] conv3 needs backward computation.
I0111 17:54:26.983466 20349 net.cpp:198] quantized_conv1 needs backward computation.
I0111 17:54:26.983474 20349 net.cpp:198] pool2 needs backward computation.
I0111 17:54:26.983480 20349 net.cpp:198] relu2 needs backward computation.
I0111 17:54:26.983487 20349 net.cpp:198] scale2 needs backward computation.
I0111 17:54:26.983494 20349 net.cpp:198] bn2 needs backward computation.
I0111 17:54:26.983501 20349 net.cpp:198] conv2 needs backward computation.
I0111 17:54:26.983510 20349 net.cpp:198] quantized_conv1 needs backward computation.
I0111 17:54:26.983516 20349 net.cpp:198] pool1 needs backward computation.
I0111 17:54:26.983523 20349 net.cpp:198] relu1 needs backward computation.
I0111 17:54:26.983530 20349 net.cpp:198] scale1 needs backward computation.
I0111 17:54:26.983537 20349 net.cpp:198] bn1 needs backward computation.
I0111 17:54:26.983544 20349 net.cpp:198] conv1 needs backward computation.
I0111 17:54:26.983552 20349 net.cpp:200] label_data_1_split does not need backward computation.
I0111 17:54:26.983561 20349 net.cpp:200] data does not need backward computation.
I0111 17:54:26.983606 20349 net.cpp:242] This network produces output accuracy
I0111 17:54:26.983618 20349 net.cpp:242] This network produces output accuracy_5
I0111 17:54:26.983625 20349 net.cpp:242] This network produces output loss
I0111 17:54:26.983662 20349 net.cpp:255] Network initialization done.
I0111 17:54:33.991268 20349 caffe.cpp:290] Running for 250 iterations.
I0111 17:54:36.816380 20349 caffe.cpp:314] Batch 0, accuracy = 0.36
I0111 17:54:36.816462 20349 caffe.cpp:314] Batch 0, accuracy_5 = 0.56
I0111 17:54:36.816473 20349 caffe.cpp:314] Batch 0, loss = 3.42549
I0111 17:54:39.423554 20349 caffe.cpp:314] Batch 1, accuracy = 0.375
I0111 17:54:39.423622 20349 caffe.cpp:314] Batch 1, accuracy_5 = 0.59
I0111 17:54:39.423638 20349 caffe.cpp:314] Batch 1, loss = 3.45096
I0111 17:54:42.098772 20349 caffe.cpp:314] Batch 2, accuracy = 0.345
I0111 17:54:42.098848 20349 caffe.cpp:314] Batch 2, accuracy_5 = 0.625
I0111 17:54:42.098858 20349 caffe.cpp:314] Batch 2, loss = 3.3773
I0111 17:54:44.765913 20349 caffe.cpp:314] Batch 3, accuracy = 0.35
I0111 17:54:44.765991 20349 caffe.cpp:314] Batch 3, accuracy_5 = 0.535
I0111 17:54:44.766000 20349 caffe.cpp:314] Batch 3, loss = 3.64967
I0111 17:54:47.018324 20349 caffe.cpp:314] Batch 4, accuracy = 0.335
I0111 17:54:47.018391 20349 caffe.cpp:314] Batch 4, accuracy_5 = 0.515
I0111 17:54:47.018412 20349 caffe.cpp:314] Batch 4, loss = 3.75522
I0111 17:54:49.444929 20349 caffe.cpp:314] Batch 5, accuracy = 0.405
I0111 17:54:49.445005 20349 caffe.cpp:314] Batch 5, accuracy_5 = 0.62
I0111 17:54:49.445013 20349 caffe.cpp:314] Batch 5, loss = 3.1636
I0111 17:54:51.540659 20349 caffe.cpp:314] Batch 6, accuracy = 0.345
I0111 17:54:51.540736 20349 caffe.cpp:314] Batch 6, accuracy_5 = 0.565
I0111 17:54:51.540745 20349 caffe.cpp:314] Batch 6, loss = 3.3694
I0111 17:54:53.977052 20349 caffe.cpp:314] Batch 7, accuracy = 0.36
I0111 17:54:53.977138 20349 caffe.cpp:314] Batch 7, accuracy_5 = 0.585
I0111 17:54:53.977147 20349 caffe.cpp:314] Batch 7, loss = 3.4054
I0111 17:54:56.443112 20349 caffe.cpp:314] Batch 8, accuracy = 0.34
I0111 17:54:56.443267 20349 caffe.cpp:314] Batch 8, accuracy_5 = 0.55
I0111 17:54:56.443279 20349 caffe.cpp:314] Batch 8, loss = 3.71789
I0111 17:54:58.729492 20349 caffe.cpp:314] Batch 9, accuracy = 0.395
I0111 17:54:58.729557 20349 caffe.cpp:314] Batch 9, accuracy_5 = 0.545
I0111 17:54:58.729578 20349 caffe.cpp:314] Batch 9, loss = 3.41585
I0111 17:55:01.072616 20349 caffe.cpp:314] Batch 10, accuracy = 0.275
I0111 17:55:01.072696 20349 caffe.cpp:314] Batch 10, accuracy_5 = 0.495
I0111 17:55:01.072705 20349 caffe.cpp:314] Batch 10, loss = 3.6724
I0111 17:55:03.169246 20349 caffe.cpp:314] Batch 11, accuracy = 0.34
I0111 17:55:03.169325 20349 caffe.cpp:314] Batch 11, accuracy_5 = 0.525
I0111 17:55:03.169334 20349 caffe.cpp:314] Batch 11, loss = 3.5457
I0111 17:55:05.554558 20349 caffe.cpp:314] Batch 12, accuracy = 0.365
I0111 17:55:05.554636 20349 caffe.cpp:314] Batch 12, accuracy_5 = 0.55
I0111 17:55:05.554646 20349 caffe.cpp:314] Batch 12, loss = 3.37628
I0111 17:55:07.524137 20349 caffe.cpp:314] Batch 13, accuracy = 0.36
I0111 17:55:07.524219 20349 caffe.cpp:314] Batch 13, accuracy_5 = 0.535
I0111 17:55:07.524229 20349 caffe.cpp:314] Batch 13, loss = 3.60948
I0111 17:55:10.037158 20349 caffe.cpp:314] Batch 14, accuracy = 0.38
I0111 17:55:10.037235 20349 caffe.cpp:314] Batch 14, accuracy_5 = 0.605
I0111 17:55:10.037245 20349 caffe.cpp:314] Batch 14, loss = 3.32266
I0111 17:55:12.649627 20349 caffe.cpp:314] Batch 15, accuracy = 0.375
I0111 17:55:12.649698 20349 caffe.cpp:314] Batch 15, accuracy_5 = 0.62
I0111 17:55:12.649708 20349 caffe.cpp:314] Batch 15, loss = 3.1586
I0111 17:55:14.795137 20349 caffe.cpp:314] Batch 16, accuracy = 0.33
I0111 17:55:14.795212 20349 caffe.cpp:314] Batch 16, accuracy_5 = 0.525
I0111 17:55:14.795228 20349 caffe.cpp:314] Batch 16, loss = 3.5421
I0111 17:55:16.932385 20349 caffe.cpp:314] Batch 17, accuracy = 0.295
I0111 17:55:16.932463 20349 caffe.cpp:314] Batch 17, accuracy_5 = 0.5
I0111 17:55:16.932471 20349 caffe.cpp:314] Batch 17, loss = 3.91318
I0111 17:55:19.068049 20349 caffe.cpp:314] Batch 18, accuracy = 0.355
I0111 17:55:19.068125 20349 caffe.cpp:314] Batch 18, accuracy_5 = 0.585
I0111 17:55:19.068133 20349 caffe.cpp:314] Batch 18, loss = 3.44058
I0111 17:55:21.280817 20349 caffe.cpp:314] Batch 19, accuracy = 0.395
I0111 17:55:21.280892 20349 caffe.cpp:314] Batch 19, accuracy_5 = 0.6
I0111 17:55:21.280901 20349 caffe.cpp:314] Batch 19, loss = 3.22351
I0111 17:55:23.293490 20349 caffe.cpp:314] Batch 20, accuracy = 0.29
I0111 17:55:23.293568 20349 caffe.cpp:314] Batch 20, accuracy_5 = 0.53
I0111 17:55:23.293576 20349 caffe.cpp:314] Batch 20, loss = 3.46433
I0111 17:55:25.350797 20349 caffe.cpp:314] Batch 21, accuracy = 0.335
I0111 17:55:25.350865 20349 caffe.cpp:314] Batch 21, accuracy_5 = 0.51
I0111 17:55:25.350883 20349 caffe.cpp:314] Batch 21, loss = 3.75631
I0111 17:55:27.353410 20349 caffe.cpp:314] Batch 22, accuracy = 0.27
I0111 17:55:27.353794 20349 caffe.cpp:314] Batch 22, accuracy_5 = 0.455
I0111 17:55:27.353893 20349 caffe.cpp:314] Batch 22, loss = 4.23923
I0111 17:55:29.301266 20349 caffe.cpp:314] Batch 23, accuracy = 0.275
I0111 17:55:29.301336 20349 caffe.cpp:314] Batch 23, accuracy_5 = 0.365
I0111 17:55:29.301352 20349 caffe.cpp:314] Batch 23, loss = 4.49684
I0111 17:55:31.277905 20349 caffe.cpp:314] Batch 24, accuracy = 0.31
I0111 17:55:31.277976 20349 caffe.cpp:314] Batch 24, accuracy_5 = 0.55
I0111 17:55:31.277983 20349 caffe.cpp:314] Batch 24, loss = 3.61992
I0111 17:55:33.244810 20349 caffe.cpp:314] Batch 25, accuracy = 0.365
I0111 17:55:33.244875 20349 caffe.cpp:314] Batch 25, accuracy_5 = 0.56
I0111 17:55:33.244890 20349 caffe.cpp:314] Batch 25, loss = 3.59991
I0111 17:55:35.579609 20349 caffe.cpp:314] Batch 26, accuracy = 0.33
I0111 17:55:35.579677 20349 caffe.cpp:314] Batch 26, accuracy_5 = 0.565
I0111 17:55:35.579694 20349 caffe.cpp:314] Batch 26, loss = 3.53716
I0111 17:55:37.961359 20349 caffe.cpp:314] Batch 27, accuracy = 0.315
I0111 17:55:37.961434 20349 caffe.cpp:314] Batch 27, accuracy_5 = 0.51
I0111 17:55:37.961443 20349 caffe.cpp:314] Batch 27, loss = 3.68505
I0111 17:55:40.057369 20349 caffe.cpp:314] Batch 28, accuracy = 0.31
I0111 17:55:40.057437 20349 caffe.cpp:314] Batch 28, accuracy_5 = 0.52
I0111 17:55:40.057446 20349 caffe.cpp:314] Batch 28, loss = 3.69553
I0111 17:55:42.082231 20349 caffe.cpp:314] Batch 29, accuracy = 0.325
I0111 17:55:42.082309 20349 caffe.cpp:314] Batch 29, accuracy_5 = 0.505
I0111 17:55:42.082319 20349 caffe.cpp:314] Batch 29, loss = 3.8601
I0111 17:55:44.095787 20349 caffe.cpp:314] Batch 30, accuracy = 0.28
I0111 17:55:44.095865 20349 caffe.cpp:314] Batch 30, accuracy_5 = 0.525
I0111 17:55:44.095875 20349 caffe.cpp:314] Batch 30, loss = 3.66827
I0111 17:55:46.339118 20349 caffe.cpp:314] Batch 31, accuracy = 0.33
I0111 17:55:46.339202 20349 caffe.cpp:314] Batch 31, accuracy_5 = 0.525
I0111 17:55:46.339212 20349 caffe.cpp:314] Batch 31, loss = 3.57112
I0111 17:55:48.822721 20349 caffe.cpp:314] Batch 32, accuracy = 0.315
I0111 17:55:48.822801 20349 caffe.cpp:314] Batch 32, accuracy_5 = 0.565
I0111 17:55:48.822811 20349 caffe.cpp:314] Batch 32, loss = 3.52626
I0111 17:55:51.369451 20349 caffe.cpp:314] Batch 33, accuracy = 0.28
I0111 17:55:51.369519 20349 caffe.cpp:314] Batch 33, accuracy_5 = 0.5
I0111 17:55:51.369536 20349 caffe.cpp:314] Batch 33, loss = 3.79321
I0111 17:55:53.516912 20349 caffe.cpp:314] Batch 34, accuracy = 0.31
I0111 17:55:53.516990 20349 caffe.cpp:314] Batch 34, accuracy_5 = 0.49
I0111 17:55:53.516999 20349 caffe.cpp:314] Batch 34, loss = 3.74236
I0111 17:55:55.595481 20349 caffe.cpp:314] Batch 35, accuracy = 0.3
I0111 17:55:55.595567 20349 caffe.cpp:314] Batch 35, accuracy_5 = 0.55
I0111 17:55:55.595577 20349 caffe.cpp:314] Batch 35, loss = 3.5591
I0111 17:55:57.706920 20349 caffe.cpp:314] Batch 36, accuracy = 0.325
I0111 17:55:57.707348 20349 caffe.cpp:314] Batch 36, accuracy_5 = 0.525
I0111 17:55:57.707365 20349 caffe.cpp:314] Batch 36, loss = 3.78603
I0111 17:55:59.912645 20349 caffe.cpp:314] Batch 37, accuracy = 0.295
I0111 17:55:59.912732 20349 caffe.cpp:314] Batch 37, accuracy_5 = 0.5
I0111 17:55:59.912744 20349 caffe.cpp:314] Batch 37, loss = 3.57504
I0111 17:56:02.397550 20349 caffe.cpp:314] Batch 38, accuracy = 0.32
I0111 17:56:02.397629 20349 caffe.cpp:314] Batch 38, accuracy_5 = 0.555
I0111 17:56:02.397640 20349 caffe.cpp:314] Batch 38, loss = 3.41976
I0111 17:56:04.909992 20349 caffe.cpp:314] Batch 39, accuracy = 0.335
I0111 17:56:04.910071 20349 caffe.cpp:314] Batch 39, accuracy_5 = 0.505
I0111 17:56:04.910081 20349 caffe.cpp:314] Batch 39, loss = 3.72536
I0111 17:56:07.044996 20349 caffe.cpp:314] Batch 40, accuracy = 0.325
I0111 17:56:07.045056 20349 caffe.cpp:314] Batch 40, accuracy_5 = 0.535
I0111 17:56:07.045078 20349 caffe.cpp:314] Batch 40, loss = 3.69983
I0111 17:56:09.039255 20349 caffe.cpp:314] Batch 41, accuracy = 0.335
I0111 17:56:09.039332 20349 caffe.cpp:314] Batch 41, accuracy_5 = 0.49
I0111 17:56:09.039342 20349 caffe.cpp:314] Batch 41, loss = 3.80603
I0111 17:56:11.158387 20349 caffe.cpp:314] Batch 42, accuracy = 0.28
I0111 17:56:11.158468 20349 caffe.cpp:314] Batch 42, accuracy_5 = 0.5
I0111 17:56:11.158478 20349 caffe.cpp:314] Batch 42, loss = 4.06034
I0111 17:56:13.466953 20349 caffe.cpp:314] Batch 43, accuracy = 0.325
I0111 17:56:13.467031 20349 caffe.cpp:314] Batch 43, accuracy_5 = 0.54
I0111 17:56:13.467039 20349 caffe.cpp:314] Batch 43, loss = 3.62709
I0111 17:56:16.011817 20349 caffe.cpp:314] Batch 44, accuracy = 0.265
I0111 17:56:16.011890 20349 caffe.cpp:314] Batch 44, accuracy_5 = 0.52
I0111 17:56:16.011900 20349 caffe.cpp:314] Batch 44, loss = 3.74904
I0111 17:56:18.496747 20349 caffe.cpp:314] Batch 45, accuracy = 0.335
I0111 17:56:18.496831 20349 caffe.cpp:314] Batch 45, accuracy_5 = 0.49
I0111 17:56:18.496840 20349 caffe.cpp:314] Batch 45, loss = 3.91919
I0111 17:56:20.666553 20349 caffe.cpp:314] Batch 46, accuracy = 0.4
I0111 17:56:20.666632 20349 caffe.cpp:314] Batch 46, accuracy_5 = 0.625
I0111 17:56:20.666641 20349 caffe.cpp:314] Batch 46, loss = 3.07931
I0111 17:56:22.700544 20349 caffe.cpp:314] Batch 47, accuracy = 0.425
I0111 17:56:22.700620 20349 caffe.cpp:314] Batch 47, accuracy_5 = 0.635
I0111 17:56:22.700630 20349 caffe.cpp:314] Batch 47, loss = 3.02474
I0111 17:56:24.810992 20349 caffe.cpp:314] Batch 48, accuracy = 0.345
I0111 17:56:24.811060 20349 caffe.cpp:314] Batch 48, accuracy_5 = 0.5
I0111 17:56:24.811081 20349 caffe.cpp:314] Batch 48, loss = 3.74047
I0111 17:56:27.175426 20349 caffe.cpp:314] Batch 49, accuracy = 0.335
I0111 17:56:27.175489 20349 caffe.cpp:314] Batch 49, accuracy_5 = 0.515
I0111 17:56:27.175511 20349 caffe.cpp:314] Batch 49, loss = 3.75527
I0111 17:56:29.650586 20349 caffe.cpp:314] Batch 50, accuracy = 0.33
I0111 17:56:29.650836 20349 caffe.cpp:314] Batch 50, accuracy_5 = 0.565
I0111 17:56:29.650883 20349 caffe.cpp:314] Batch 50, loss = 3.5717
I0111 17:56:32.111194 20349 caffe.cpp:314] Batch 51, accuracy = 0.345
I0111 17:56:32.111273 20349 caffe.cpp:314] Batch 51, accuracy_5 = 0.535
I0111 17:56:32.111282 20349 caffe.cpp:314] Batch 51, loss = 3.60192
I0111 17:56:34.231706 20349 caffe.cpp:314] Batch 52, accuracy = 0.365
I0111 17:56:34.231778 20349 caffe.cpp:314] Batch 52, accuracy_5 = 0.545
I0111 17:56:34.231799 20349 caffe.cpp:314] Batch 52, loss = 3.68271
I0111 17:56:36.252198 20349 caffe.cpp:314] Batch 53, accuracy = 0.365
I0111 17:56:36.252281 20349 caffe.cpp:314] Batch 53, accuracy_5 = 0.58
I0111 17:56:36.252291 20349 caffe.cpp:314] Batch 53, loss = 3.48507
I0111 17:56:38.420900 20349 caffe.cpp:314] Batch 54, accuracy = 0.31
I0111 17:56:38.420974 20349 caffe.cpp:314] Batch 54, accuracy_5 = 0.555
I0111 17:56:38.420984 20349 caffe.cpp:314] Batch 54, loss = 3.63431
I0111 17:56:40.878317 20349 caffe.cpp:314] Batch 55, accuracy = 0.33
I0111 17:56:40.878393 20349 caffe.cpp:314] Batch 55, accuracy_5 = 0.59
I0111 17:56:40.878403 20349 caffe.cpp:314] Batch 55, loss = 3.44522
I0111 17:56:43.322280 20349 caffe.cpp:314] Batch 56, accuracy = 0.395
I0111 17:56:43.322345 20349 caffe.cpp:314] Batch 56, accuracy_5 = 0.605
I0111 17:56:43.322362 20349 caffe.cpp:314] Batch 56, loss = 3.20826
I0111 17:56:45.959529 20349 caffe.cpp:314] Batch 57, accuracy = 0.335
I0111 17:56:45.959596 20349 caffe.cpp:314] Batch 57, accuracy_5 = 0.535
I0111 17:56:45.959617 20349 caffe.cpp:314] Batch 57, loss = 3.56461
I0111 17:56:48.150955 20349 caffe.cpp:314] Batch 58, accuracy = 0.36
I0111 17:56:48.151033 20349 caffe.cpp:314] Batch 58, accuracy_5 = 0.585
I0111 17:56:48.151042 20349 caffe.cpp:314] Batch 58, loss = 3.424
I0111 17:56:50.287487 20349 caffe.cpp:314] Batch 59, accuracy = 0.32
I0111 17:56:50.287564 20349 caffe.cpp:314] Batch 59, accuracy_5 = 0.51
I0111 17:56:50.287572 20349 caffe.cpp:314] Batch 59, loss = 3.80056
I0111 17:56:52.522517 20349 caffe.cpp:314] Batch 60, accuracy = 0.39
I0111 17:56:52.522598 20349 caffe.cpp:314] Batch 60, accuracy_5 = 0.6
I0111 17:56:52.522608 20349 caffe.cpp:314] Batch 60, loss = 3.16927
I0111 17:56:55.069532 20349 caffe.cpp:314] Batch 61, accuracy = 0.365
I0111 17:56:55.069610 20349 caffe.cpp:314] Batch 61, accuracy_5 = 0.555
I0111 17:56:55.069620 20349 caffe.cpp:314] Batch 61, loss = 3.41309
I0111 17:56:57.590354 20349 caffe.cpp:314] Batch 62, accuracy = 0.26
I0111 17:56:57.590427 20349 caffe.cpp:314] Batch 62, accuracy_5 = 0.455
I0111 17:56:57.590437 20349 caffe.cpp:314] Batch 62, loss = 4.07613
I0111 17:57:00.100986 20349 caffe.cpp:314] Batch 63, accuracy = 0.355
I0111 17:57:00.101364 20349 caffe.cpp:314] Batch 63, accuracy_5 = 0.555
I0111 17:57:00.101423 20349 caffe.cpp:314] Batch 63, loss = 3.59907
I0111 17:57:02.479410 20349 caffe.cpp:314] Batch 64, accuracy = 0.375
I0111 17:57:02.479496 20349 caffe.cpp:314] Batch 64, accuracy_5 = 0.575
I0111 17:57:02.479511 20349 caffe.cpp:314] Batch 64, loss = 3.28201
I0111 17:57:04.654228 20349 caffe.cpp:314] Batch 65, accuracy = 0.38
I0111 17:57:04.654295 20349 caffe.cpp:314] Batch 65, accuracy_5 = 0.555
I0111 17:57:04.654304 20349 caffe.cpp:314] Batch 65, loss = 3.32233
I0111 17:57:06.805156 20349 caffe.cpp:314] Batch 66, accuracy = 0.34
I0111 17:57:06.805234 20349 caffe.cpp:314] Batch 66, accuracy_5 = 0.595
I0111 17:57:06.805244 20349 caffe.cpp:314] Batch 66, loss = 3.34125
I0111 17:57:09.399174 20349 caffe.cpp:314] Batch 67, accuracy = 0.31
I0111 17:57:09.399256 20349 caffe.cpp:314] Batch 67, accuracy_5 = 0.53
I0111 17:57:09.399272 20349 caffe.cpp:314] Batch 67, loss = 3.69878
I0111 17:57:11.867277 20349 caffe.cpp:314] Batch 68, accuracy = 0.345
I0111 17:57:11.867341 20349 caffe.cpp:314] Batch 68, accuracy_5 = 0.55
I0111 17:57:11.867348 20349 caffe.cpp:314] Batch 68, loss = 3.7226
I0111 17:57:14.507902 20349 caffe.cpp:314] Batch 69, accuracy = 0.325
I0111 17:57:14.507977 20349 caffe.cpp:314] Batch 69, accuracy_5 = 0.49
I0111 17:57:14.507987 20349 caffe.cpp:314] Batch 69, loss = 3.67372
I0111 17:57:17.057832 20349 caffe.cpp:314] Batch 70, accuracy = 0.265
I0111 17:57:17.057905 20349 caffe.cpp:314] Batch 70, accuracy_5 = 0.56
I0111 17:57:17.057914 20349 caffe.cpp:314] Batch 70, loss = 3.49897
I0111 17:57:19.330857 20349 caffe.cpp:314] Batch 71, accuracy = 0.36
I0111 17:57:19.330942 20349 caffe.cpp:314] Batch 71, accuracy_5 = 0.565
I0111 17:57:19.330952 20349 caffe.cpp:314] Batch 71, loss = 3.2969
I0111 17:57:21.392873 20349 caffe.cpp:314] Batch 72, accuracy = 0.335
I0111 17:57:21.392940 20349 caffe.cpp:314] Batch 72, accuracy_5 = 0.53
I0111 17:57:21.392956 20349 caffe.cpp:314] Batch 72, loss = 3.68169
I0111 17:57:23.686337 20349 caffe.cpp:314] Batch 73, accuracy = 0.34
I0111 17:57:23.686405 20349 caffe.cpp:314] Batch 73, accuracy_5 = 0.535
I0111 17:57:23.686414 20349 caffe.cpp:314] Batch 73, loss = 3.45572
I0111 17:57:26.159809 20349 caffe.cpp:314] Batch 74, accuracy = 0.265
I0111 17:57:26.159869 20349 caffe.cpp:314] Batch 74, accuracy_5 = 0.495
I0111 17:57:26.159884 20349 caffe.cpp:314] Batch 74, loss = 3.75414
I0111 17:57:28.635720 20349 caffe.cpp:314] Batch 75, accuracy = 0.315
I0111 17:57:28.635790 20349 caffe.cpp:314] Batch 75, accuracy_5 = 0.5
I0111 17:57:28.635807 20349 caffe.cpp:314] Batch 75, loss = 3.90606
I0111 17:57:30.846355 20349 caffe.cpp:314] Batch 76, accuracy = 0.325
I0111 17:57:30.846616 20349 caffe.cpp:314] Batch 76, accuracy_5 = 0.545
I0111 17:57:30.846644 20349 caffe.cpp:314] Batch 76, loss = 3.5216
I0111 17:57:32.858619 20349 caffe.cpp:314] Batch 77, accuracy = 0.315
I0111 17:57:32.858690 20349 caffe.cpp:314] Batch 77, accuracy_5 = 0.59
I0111 17:57:32.858705 20349 caffe.cpp:314] Batch 77, loss = 3.37771
I0111 17:57:35.226213 20349 caffe.cpp:314] Batch 78, accuracy = 0.355
I0111 17:57:35.226292 20349 caffe.cpp:314] Batch 78, accuracy_5 = 0.57
I0111 17:57:35.226302 20349 caffe.cpp:314] Batch 78, loss = 3.44036
I0111 17:57:37.866046 20349 caffe.cpp:314] Batch 79, accuracy = 0.315
I0111 17:57:37.866143 20349 caffe.cpp:314] Batch 79, accuracy_5 = 0.59
I0111 17:57:37.866154 20349 caffe.cpp:314] Batch 79, loss = 3.42056
I0111 17:57:40.413888 20349 caffe.cpp:314] Batch 80, accuracy = 0.29
I0111 17:57:40.413975 20349 caffe.cpp:314] Batch 80, accuracy_5 = 0.465
I0111 17:57:40.413986 20349 caffe.cpp:314] Batch 80, loss = 3.83852
I0111 17:57:42.588634 20349 caffe.cpp:314] Batch 81, accuracy = 0.345
I0111 17:57:42.588702 20349 caffe.cpp:314] Batch 81, accuracy_5 = 0.5
I0111 17:57:42.588719 20349 caffe.cpp:314] Batch 81, loss = 3.6599
I0111 17:57:44.595151 20349 caffe.cpp:314] Batch 82, accuracy = 0.31
I0111 17:57:44.595224 20349 caffe.cpp:314] Batch 82, accuracy_5 = 0.545
I0111 17:57:44.595233 20349 caffe.cpp:314] Batch 82, loss = 3.697
I0111 17:57:46.710096 20349 caffe.cpp:314] Batch 83, accuracy = 0.315
I0111 17:57:46.710156 20349 caffe.cpp:314] Batch 83, accuracy_5 = 0.52
I0111 17:57:46.710172 20349 caffe.cpp:314] Batch 83, loss = 3.81837
I0111 17:57:49.280710 20349 caffe.cpp:314] Batch 84, accuracy = 0.305
I0111 17:57:49.280777 20349 caffe.cpp:314] Batch 84, accuracy_5 = 0.505
I0111 17:57:49.280793 20349 caffe.cpp:314] Batch 84, loss = 3.72142
I0111 17:57:51.771970 20349 caffe.cpp:314] Batch 85, accuracy = 0.345
I0111 17:57:51.772045 20349 caffe.cpp:314] Batch 85, accuracy_5 = 0.535
I0111 17:57:51.772060 20349 caffe.cpp:314] Batch 85, loss = 3.62861
I0111 17:57:53.890523 20349 caffe.cpp:314] Batch 86, accuracy = 0.355
I0111 17:57:53.890594 20349 caffe.cpp:314] Batch 86, accuracy_5 = 0.6
I0111 17:57:53.890610 20349 caffe.cpp:314] Batch 86, loss = 3.16856
I0111 17:57:55.877558 20349 caffe.cpp:314] Batch 87, accuracy = 0.375
I0111 17:57:55.877642 20349 caffe.cpp:314] Batch 87, accuracy_5 = 0.585
I0111 17:57:55.877655 20349 caffe.cpp:314] Batch 87, loss = 3.43402
I0111 17:57:58.214176 20349 caffe.cpp:314] Batch 88, accuracy = 0.33
I0111 17:57:58.214256 20349 caffe.cpp:314] Batch 88, accuracy_5 = 0.55
I0111 17:57:58.214265 20349 caffe.cpp:314] Batch 88, loss = 3.53833
I0111 17:58:00.691588 20349 caffe.cpp:314] Batch 89, accuracy = 0.3
I0111 17:58:00.691665 20349 caffe.cpp:314] Batch 89, accuracy_5 = 0.505
I0111 17:58:00.691675 20349 caffe.cpp:314] Batch 89, loss = 3.67238
I0111 17:58:03.277082 20349 caffe.cpp:314] Batch 90, accuracy = 0.335
I0111 17:58:03.277295 20349 caffe.cpp:314] Batch 90, accuracy_5 = 0.545
I0111 17:58:03.277344 20349 caffe.cpp:314] Batch 90, loss = 3.63225
I0111 17:58:05.448309 20349 caffe.cpp:314] Batch 91, accuracy = 0.31
I0111 17:58:05.448385 20349 caffe.cpp:314] Batch 91, accuracy_5 = 0.55
I0111 17:58:05.448395 20349 caffe.cpp:314] Batch 91, loss = 3.68804
I0111 17:58:07.540040 20349 caffe.cpp:314] Batch 92, accuracy = 0.32
I0111 17:58:07.540109 20349 caffe.cpp:314] Batch 92, accuracy_5 = 0.505
I0111 17:58:07.540130 20349 caffe.cpp:314] Batch 92, loss = 3.85254
I0111 17:58:09.709295 20349 caffe.cpp:314] Batch 93, accuracy = 0.34
I0111 17:58:09.709374 20349 caffe.cpp:314] Batch 93, accuracy_5 = 0.59
I0111 17:58:09.709384 20349 caffe.cpp:314] Batch 93, loss = 3.28238
I0111 17:58:12.164073 20349 caffe.cpp:314] Batch 94, accuracy = 0.34
I0111 17:58:12.164151 20349 caffe.cpp:314] Batch 94, accuracy_5 = 0.535
I0111 17:58:12.164161 20349 caffe.cpp:314] Batch 94, loss = 3.49714
I0111 17:58:14.582237 20349 caffe.cpp:314] Batch 95, accuracy = 0.32
I0111 17:58:14.582315 20349 caffe.cpp:314] Batch 95, accuracy_5 = 0.545
I0111 17:58:14.582325 20349 caffe.cpp:314] Batch 95, loss = 3.79943
I0111 17:58:17.208493 20349 caffe.cpp:314] Batch 96, accuracy = 0.31
I0111 17:58:17.208578 20349 caffe.cpp:314] Batch 96, accuracy_5 = 0.565
I0111 17:58:17.208596 20349 caffe.cpp:314] Batch 96, loss = 3.34403
I0111 17:58:19.357131 20349 caffe.cpp:314] Batch 97, accuracy = 0.38
I0111 17:58:19.357210 20349 caffe.cpp:314] Batch 97, accuracy_5 = 0.595
I0111 17:58:19.357219 20349 caffe.cpp:314] Batch 97, loss = 3.54441
I0111 17:58:21.442751 20349 caffe.cpp:314] Batch 98, accuracy = 0.395
I0111 17:58:21.442822 20349 caffe.cpp:314] Batch 98, accuracy_5 = 0.565
I0111 17:58:21.442839 20349 caffe.cpp:314] Batch 98, loss = 3.19165
I0111 17:58:23.586482 20349 caffe.cpp:314] Batch 99, accuracy = 0.39
I0111 17:58:23.586549 20349 caffe.cpp:314] Batch 99, accuracy_5 = 0.655
I0111 17:58:23.586565 20349 caffe.cpp:314] Batch 99, loss = 3.07411
I0111 17:58:26.017848 20349 caffe.cpp:314] Batch 100, accuracy = 0.32
I0111 17:58:26.017911 20349 caffe.cpp:314] Batch 100, accuracy_5 = 0.6
I0111 17:58:26.017936 20349 caffe.cpp:314] Batch 100, loss = 3.23488
I0111 17:58:28.430618 20349 caffe.cpp:314] Batch 101, accuracy = 0.34
I0111 17:58:28.430690 20349 caffe.cpp:314] Batch 101, accuracy_5 = 0.54
I0111 17:58:28.430704 20349 caffe.cpp:314] Batch 101, loss = 3.72672
I0111 17:58:31.055841 20349 caffe.cpp:314] Batch 102, accuracy = 0.385
I0111 17:58:31.055929 20349 caffe.cpp:314] Batch 102, accuracy_5 = 0.56
I0111 17:58:31.055939 20349 caffe.cpp:314] Batch 102, loss = 3.33755
I0111 17:58:33.177698 20349 caffe.cpp:314] Batch 103, accuracy = 0.31
I0111 17:58:33.177778 20349 caffe.cpp:314] Batch 103, accuracy_5 = 0.515
I0111 17:58:33.177788 20349 caffe.cpp:314] Batch 103, loss = 3.7219
I0111 17:58:35.207945 20349 caffe.cpp:314] Batch 104, accuracy = 0.39
I0111 17:58:35.208421 20349 caffe.cpp:314] Batch 104, accuracy_5 = 0.58
I0111 17:58:35.208449 20349 caffe.cpp:314] Batch 104, loss = 3.28968
I0111 17:58:37.388072 20349 caffe.cpp:314] Batch 105, accuracy = 0.3
I0111 17:58:37.388137 20349 caffe.cpp:314] Batch 105, accuracy_5 = 0.56
I0111 17:58:37.388159 20349 caffe.cpp:314] Batch 105, loss = 3.72352
I0111 17:58:39.820523 20349 caffe.cpp:314] Batch 106, accuracy = 0.28
I0111 17:58:39.820601 20349 caffe.cpp:314] Batch 106, accuracy_5 = 0.505
I0111 17:58:39.820611 20349 caffe.cpp:314] Batch 106, loss = 3.76005
I0111 17:58:42.248100 20349 caffe.cpp:314] Batch 107, accuracy = 0.385
I0111 17:58:42.248167 20349 caffe.cpp:314] Batch 107, accuracy_5 = 0.58
I0111 17:58:42.248193 20349 caffe.cpp:314] Batch 107, loss = 3.35511
I0111 17:58:44.813289 20349 caffe.cpp:314] Batch 108, accuracy = 0.36
I0111 17:58:44.813367 20349 caffe.cpp:314] Batch 108, accuracy_5 = 0.575
I0111 17:58:44.813377 20349 caffe.cpp:314] Batch 108, loss = 3.48788
I0111 17:58:46.905045 20349 caffe.cpp:314] Batch 109, accuracy = 0.29
I0111 17:58:46.905113 20349 caffe.cpp:314] Batch 109, accuracy_5 = 0.51
I0111 17:58:46.905135 20349 caffe.cpp:314] Batch 109, loss = 3.69694
I0111 17:58:48.894901 20349 caffe.cpp:314] Batch 110, accuracy = 0.375
I0111 17:58:48.894969 20349 caffe.cpp:314] Batch 110, accuracy_5 = 0.57
I0111 17:58:48.894979 20349 caffe.cpp:314] Batch 110, loss = 3.42713
I0111 17:58:51.004106 20349 caffe.cpp:314] Batch 111, accuracy = 0.29
I0111 17:58:51.004174 20349 caffe.cpp:314] Batch 111, accuracy_5 = 0.52
I0111 17:58:51.004195 20349 caffe.cpp:314] Batch 111, loss = 3.60433
I0111 17:58:53.114203 20349 caffe.cpp:314] Batch 112, accuracy = 0.335
I0111 17:58:53.114282 20349 caffe.cpp:314] Batch 112, accuracy_5 = 0.56
I0111 17:58:53.114291 20349 caffe.cpp:314] Batch 112, loss = 3.47848
I0111 17:58:55.417681 20349 caffe.cpp:314] Batch 113, accuracy = 0.345
I0111 17:58:55.417749 20349 caffe.cpp:314] Batch 113, accuracy_5 = 0.55
I0111 17:58:55.417757 20349 caffe.cpp:314] Batch 113, loss = 3.52801
I0111 17:58:57.858541 20349 caffe.cpp:314] Batch 114, accuracy = 0.32
I0111 17:58:57.858620 20349 caffe.cpp:314] Batch 114, accuracy_5 = 0.51
I0111 17:58:57.858630 20349 caffe.cpp:314] Batch 114, loss = 3.6367
I0111 17:59:00.175230 20349 caffe.cpp:314] Batch 115, accuracy = 0.285
I0111 17:59:00.175303 20349 caffe.cpp:314] Batch 115, accuracy_5 = 0.49
I0111 17:59:00.175325 20349 caffe.cpp:314] Batch 115, loss = 3.69395
I0111 17:59:02.307719 20349 caffe.cpp:314] Batch 116, accuracy = 0.32
I0111 17:59:02.307798 20349 caffe.cpp:314] Batch 116, accuracy_5 = 0.505
I0111 17:59:02.307808 20349 caffe.cpp:314] Batch 116, loss = 3.76944
I0111 17:59:04.503262 20349 caffe.cpp:314] Batch 117, accuracy = 0.265
I0111 17:59:04.503347 20349 caffe.cpp:314] Batch 117, accuracy_5 = 0.485
I0111 17:59:04.503357 20349 caffe.cpp:314] Batch 117, loss = 3.9502
I0111 17:59:06.649178 20349 caffe.cpp:314] Batch 118, accuracy = 0.355
I0111 17:59:06.650194 20349 caffe.cpp:314] Batch 118, accuracy_5 = 0.535
I0111 17:59:06.650215 20349 caffe.cpp:314] Batch 118, loss = 3.52486
I0111 17:59:08.887598 20349 caffe.cpp:314] Batch 119, accuracy = 0.345
I0111 17:59:08.887672 20349 caffe.cpp:314] Batch 119, accuracy_5 = 0.555
I0111 17:59:08.887682 20349 caffe.cpp:314] Batch 119, loss = 3.42566
I0111 17:59:11.086598 20349 caffe.cpp:314] Batch 120, accuracy = 0.355
I0111 17:59:11.086665 20349 caffe.cpp:314] Batch 120, accuracy_5 = 0.545
I0111 17:59:11.086681 20349 caffe.cpp:314] Batch 120, loss = 3.42495
I0111 17:59:13.287376 20349 caffe.cpp:314] Batch 121, accuracy = 0.355
I0111 17:59:13.287464 20349 caffe.cpp:314] Batch 121, accuracy_5 = 0.58
I0111 17:59:13.287474 20349 caffe.cpp:314] Batch 121, loss = 3.32085
I0111 17:59:15.403479 20349 caffe.cpp:314] Batch 122, accuracy = 0.375
I0111 17:59:15.403554 20349 caffe.cpp:314] Batch 122, accuracy_5 = 0.67
I0111 17:59:15.403564 20349 caffe.cpp:314] Batch 122, loss = 2.94067
I0111 17:59:17.542748 20349 caffe.cpp:314] Batch 123, accuracy = 0.33
I0111 17:59:17.542815 20349 caffe.cpp:314] Batch 123, accuracy_5 = 0.53
I0111 17:59:17.542841 20349 caffe.cpp:314] Batch 123, loss = 3.4955
I0111 17:59:19.643162 20349 caffe.cpp:314] Batch 124, accuracy = 0.315
I0111 17:59:19.643241 20349 caffe.cpp:314] Batch 124, accuracy_5 = 0.53
I0111 17:59:19.643250 20349 caffe.cpp:314] Batch 124, loss = 3.73499
I0111 17:59:21.744779 20349 caffe.cpp:314] Batch 125, accuracy = 0.315
I0111 17:59:21.744858 20349 caffe.cpp:314] Batch 125, accuracy_5 = 0.55
I0111 17:59:21.744868 20349 caffe.cpp:314] Batch 125, loss = 3.65793
I0111 17:59:23.838021 20349 caffe.cpp:314] Batch 126, accuracy = 0.42
I0111 17:59:23.838109 20349 caffe.cpp:314] Batch 126, accuracy_5 = 0.6
I0111 17:59:23.838124 20349 caffe.cpp:314] Batch 126, loss = 3.04082
I0111 17:59:25.941494 20349 caffe.cpp:314] Batch 127, accuracy = 0.325
I0111 17:59:25.941573 20349 caffe.cpp:314] Batch 127, accuracy_5 = 0.545
I0111 17:59:25.941583 20349 caffe.cpp:314] Batch 127, loss = 3.82916
I0111 17:59:28.040746 20349 caffe.cpp:314] Batch 128, accuracy = 0.32
I0111 17:59:28.040824 20349 caffe.cpp:314] Batch 128, accuracy_5 = 0.515
I0111 17:59:28.040833 20349 caffe.cpp:314] Batch 128, loss = 3.67279
I0111 17:59:30.142531 20349 caffe.cpp:314] Batch 129, accuracy = 0.32
I0111 17:59:30.142627 20349 caffe.cpp:314] Batch 129, accuracy_5 = 0.485
I0111 17:59:30.142639 20349 caffe.cpp:314] Batch 129, loss = 3.84072
I0111 17:59:32.241196 20349 caffe.cpp:314] Batch 130, accuracy = 0.33
I0111 17:59:32.241276 20349 caffe.cpp:314] Batch 130, accuracy_5 = 0.5
I0111 17:59:32.241286 20349 caffe.cpp:314] Batch 130, loss = 3.87418
I0111 17:59:34.334338 20349 caffe.cpp:314] Batch 131, accuracy = 0.3
I0111 17:59:34.334417 20349 caffe.cpp:314] Batch 131, accuracy_5 = 0.5
I0111 17:59:34.334426 20349 caffe.cpp:314] Batch 131, loss = 3.68184
I0111 17:59:36.447233 20349 caffe.cpp:314] Batch 132, accuracy = 0.33
I0111 17:59:36.447314 20349 caffe.cpp:314] Batch 132, accuracy_5 = 0.52
I0111 17:59:36.447324 20349 caffe.cpp:314] Batch 132, loss = 3.76354
I0111 17:59:38.542016 20349 caffe.cpp:314] Batch 133, accuracy = 0.305
I0111 17:59:38.542299 20349 caffe.cpp:314] Batch 133, accuracy_5 = 0.515
I0111 17:59:38.542347 20349 caffe.cpp:314] Batch 133, loss = 3.63663
I0111 17:59:40.637589 20349 caffe.cpp:314] Batch 134, accuracy = 0.37
I0111 17:59:40.637656 20349 caffe.cpp:314] Batch 134, accuracy_5 = 0.555
I0111 17:59:40.637672 20349 caffe.cpp:314] Batch 134, loss = 3.55133
I0111 17:59:42.741484 20349 caffe.cpp:314] Batch 135, accuracy = 0.335
I0111 17:59:42.741569 20349 caffe.cpp:314] Batch 135, accuracy_5 = 0.495
I0111 17:59:42.741578 20349 caffe.cpp:314] Batch 135, loss = 3.61164
I0111 17:59:44.895179 20349 caffe.cpp:314] Batch 136, accuracy = 0.33
I0111 17:59:44.895246 20349 caffe.cpp:314] Batch 136, accuracy_5 = 0.53
I0111 17:59:44.895267 20349 caffe.cpp:314] Batch 136, loss = 3.5318
I0111 17:59:47.025179 20349 caffe.cpp:314] Batch 137, accuracy = 0.325
I0111 17:59:47.025261 20349 caffe.cpp:314] Batch 137, accuracy_5 = 0.56
I0111 17:59:47.025274 20349 caffe.cpp:314] Batch 137, loss = 3.34961
I0111 17:59:49.151245 20349 caffe.cpp:314] Batch 138, accuracy = 0.32
I0111 17:59:49.151324 20349 caffe.cpp:314] Batch 138, accuracy_5 = 0.55
I0111 17:59:49.151335 20349 caffe.cpp:314] Batch 138, loss = 3.42922
I0111 17:59:51.280153 20349 caffe.cpp:314] Batch 139, accuracy = 0.37
I0111 17:59:51.280230 20349 caffe.cpp:314] Batch 139, accuracy_5 = 0.55
I0111 17:59:51.280241 20349 caffe.cpp:314] Batch 139, loss = 3.58002
I0111 17:59:53.402475 20349 caffe.cpp:314] Batch 140, accuracy = 0.34
I0111 17:59:53.402554 20349 caffe.cpp:314] Batch 140, accuracy_5 = 0.52
I0111 17:59:53.402565 20349 caffe.cpp:314] Batch 140, loss = 3.75892
I0111 17:59:55.526433 20349 caffe.cpp:314] Batch 141, accuracy = 0.355
I0111 17:59:55.526512 20349 caffe.cpp:314] Batch 141, accuracy_5 = 0.545
I0111 17:59:55.526522 20349 caffe.cpp:314] Batch 141, loss = 3.66969
I0111 17:59:57.648370 20349 caffe.cpp:314] Batch 142, accuracy = 0.325
I0111 17:59:57.648454 20349 caffe.cpp:314] Batch 142, accuracy_5 = 0.495
I0111 17:59:57.648468 20349 caffe.cpp:314] Batch 142, loss = 3.79551
I0111 17:59:59.772722 20349 caffe.cpp:314] Batch 143, accuracy = 0.325
I0111 17:59:59.772799 20349 caffe.cpp:314] Batch 143, accuracy_5 = 0.51
I0111 17:59:59.772809 20349 caffe.cpp:314] Batch 143, loss = 3.71748
I0111 18:00:01.899996 20349 caffe.cpp:314] Batch 144, accuracy = 0.31
I0111 18:00:01.900074 20349 caffe.cpp:314] Batch 144, accuracy_5 = 0.535
I0111 18:00:01.900084 20349 caffe.cpp:314] Batch 144, loss = 3.89631
I0111 18:00:04.029271 20349 caffe.cpp:314] Batch 145, accuracy = 0.28
I0111 18:00:04.029350 20349 caffe.cpp:314] Batch 145, accuracy_5 = 0.51
I0111 18:00:04.029359 20349 caffe.cpp:314] Batch 145, loss = 3.64944
I0111 18:00:06.153041 20349 caffe.cpp:314] Batch 146, accuracy = 0.285
I0111 18:00:06.153121 20349 caffe.cpp:314] Batch 146, accuracy_5 = 0.5
I0111 18:00:06.153131 20349 caffe.cpp:314] Batch 146, loss = 4.02705
I0111 18:00:08.280421 20349 caffe.cpp:314] Batch 147, accuracy = 0.315
I0111 18:00:08.280499 20349 caffe.cpp:314] Batch 147, accuracy_5 = 0.6
I0111 18:00:08.280508 20349 caffe.cpp:314] Batch 147, loss = 3.40081
I0111 18:00:10.438176 20349 caffe.cpp:314] Batch 148, accuracy = 0.35
I0111 18:00:10.438638 20349 caffe.cpp:314] Batch 148, accuracy_5 = 0.545
I0111 18:00:10.438695 20349 caffe.cpp:314] Batch 148, loss = 3.54031
I0111 18:00:12.535864 20349 caffe.cpp:314] Batch 149, accuracy = 0.355
I0111 18:00:12.535940 20349 caffe.cpp:314] Batch 149, accuracy_5 = 0.53
I0111 18:00:12.535950 20349 caffe.cpp:314] Batch 149, loss = 3.61976
I0111 18:00:14.635644 20349 caffe.cpp:314] Batch 150, accuracy = 0.35
I0111 18:00:14.635723 20349 caffe.cpp:314] Batch 150, accuracy_5 = 0.525
I0111 18:00:14.635732 20349 caffe.cpp:314] Batch 150, loss = 3.81099
I0111 18:00:16.733510 20349 caffe.cpp:314] Batch 151, accuracy = 0.39
I0111 18:00:16.733584 20349 caffe.cpp:314] Batch 151, accuracy_5 = 0.61
I0111 18:00:16.733594 20349 caffe.cpp:314] Batch 151, loss = 3.19575
I0111 18:00:18.831773 20349 caffe.cpp:314] Batch 152, accuracy = 0.36
I0111 18:00:18.831847 20349 caffe.cpp:314] Batch 152, accuracy_5 = 0.58
I0111 18:00:18.831858 20349 caffe.cpp:314] Batch 152, loss = 3.41528
I0111 18:00:20.925251 20349 caffe.cpp:314] Batch 153, accuracy = 0.3
I0111 18:00:20.925328 20349 caffe.cpp:314] Batch 153, accuracy_5 = 0.56
I0111 18:00:20.925338 20349 caffe.cpp:314] Batch 153, loss = 3.47123
I0111 18:00:23.026116 20349 caffe.cpp:314] Batch 154, accuracy = 0.365
I0111 18:00:23.026193 20349 caffe.cpp:314] Batch 154, accuracy_5 = 0.525
I0111 18:00:23.026203 20349 caffe.cpp:314] Batch 154, loss = 3.49825
I0111 18:00:25.129951 20349 caffe.cpp:314] Batch 155, accuracy = 0.375
I0111 18:00:25.130031 20349 caffe.cpp:314] Batch 155, accuracy_5 = 0.55
I0111 18:00:25.130043 20349 caffe.cpp:314] Batch 155, loss = 3.39703
I0111 18:00:27.230247 20349 caffe.cpp:314] Batch 156, accuracy = 0.29
I0111 18:00:27.230319 20349 caffe.cpp:314] Batch 156, accuracy_5 = 0.47
I0111 18:00:27.230326 20349 caffe.cpp:314] Batch 156, loss = 4.02149
I0111 18:00:29.356545 20349 caffe.cpp:314] Batch 157, accuracy = 0.325
I0111 18:00:29.356613 20349 caffe.cpp:314] Batch 157, accuracy_5 = 0.53
I0111 18:00:29.356622 20349 caffe.cpp:314] Batch 157, loss = 3.42331
I0111 18:00:31.445610 20349 caffe.cpp:314] Batch 158, accuracy = 0.27
I0111 18:00:31.445679 20349 caffe.cpp:314] Batch 158, accuracy_5 = 0.45
I0111 18:00:31.445688 20349 caffe.cpp:314] Batch 158, loss = 3.9939
I0111 18:00:33.559646 20349 caffe.cpp:314] Batch 159, accuracy = 0.33
I0111 18:00:33.559715 20349 caffe.cpp:314] Batch 159, accuracy_5 = 0.515
I0111 18:00:33.559722 20349 caffe.cpp:314] Batch 159, loss = 3.53346
I0111 18:00:35.657232 20349 caffe.cpp:314] Batch 160, accuracy = 0.36
I0111 18:00:35.657299 20349 caffe.cpp:314] Batch 160, accuracy_5 = 0.575
I0111 18:00:35.657307 20349 caffe.cpp:314] Batch 160, loss = 3.27733
I0111 18:00:37.741595 20349 caffe.cpp:314] Batch 161, accuracy = 0.31
I0111 18:00:37.741670 20349 caffe.cpp:314] Batch 161, accuracy_5 = 0.535
I0111 18:00:37.741680 20349 caffe.cpp:314] Batch 161, loss = 3.50941
I0111 18:00:39.890543 20349 caffe.cpp:314] Batch 162, accuracy = 0.405
I0111 18:00:39.890612 20349 caffe.cpp:314] Batch 162, accuracy_5 = 0.595
I0111 18:00:39.890627 20349 caffe.cpp:314] Batch 162, loss = 3.23424
I0111 18:00:41.988654 20349 caffe.cpp:314] Batch 163, accuracy = 0.325
I0111 18:00:41.989138 20349 caffe.cpp:314] Batch 163, accuracy_5 = 0.52
I0111 18:00:41.989187 20349 caffe.cpp:314] Batch 163, loss = 3.5974
I0111 18:00:44.161006 20349 caffe.cpp:314] Batch 164, accuracy = 0.295
I0111 18:00:44.161085 20349 caffe.cpp:314] Batch 164, accuracy_5 = 0.585
I0111 18:00:44.161095 20349 caffe.cpp:314] Batch 164, loss = 3.57173
I0111 18:00:46.272330 20349 caffe.cpp:314] Batch 165, accuracy = 0.31
I0111 18:00:46.272408 20349 caffe.cpp:314] Batch 165, accuracy_5 = 0.57
I0111 18:00:46.272418 20349 caffe.cpp:314] Batch 165, loss = 3.43427
I0111 18:00:48.443707 20349 caffe.cpp:314] Batch 166, accuracy = 0.26
I0111 18:00:48.443786 20349 caffe.cpp:314] Batch 166, accuracy_5 = 0.485
I0111 18:00:48.443795 20349 caffe.cpp:314] Batch 166, loss = 4.12237
I0111 18:00:50.696365 20349 caffe.cpp:314] Batch 167, accuracy = 0.35
I0111 18:00:50.696434 20349 caffe.cpp:314] Batch 167, accuracy_5 = 0.58
I0111 18:00:50.696450 20349 caffe.cpp:314] Batch 167, loss = 3.44138
I0111 18:00:52.804317 20349 caffe.cpp:314] Batch 168, accuracy = 0.34
I0111 18:00:52.804396 20349 caffe.cpp:314] Batch 168, accuracy_5 = 0.6
I0111 18:00:52.804406 20349 caffe.cpp:314] Batch 168, loss = 3.36206
I0111 18:00:54.938974 20349 caffe.cpp:314] Batch 169, accuracy = 0.29
I0111 18:00:54.939051 20349 caffe.cpp:314] Batch 169, accuracy_5 = 0.515
I0111 18:00:54.939060 20349 caffe.cpp:314] Batch 169, loss = 3.77529
I0111 18:00:57.047121 20349 caffe.cpp:314] Batch 170, accuracy = 0.335
I0111 18:00:57.047199 20349 caffe.cpp:314] Batch 170, accuracy_5 = 0.565
I0111 18:00:57.047209 20349 caffe.cpp:314] Batch 170, loss = 3.57923
I0111 18:00:59.245543 20349 caffe.cpp:314] Batch 171, accuracy = 0.305
I0111 18:00:59.245604 20349 caffe.cpp:314] Batch 171, accuracy_5 = 0.52
I0111 18:00:59.245625 20349 caffe.cpp:314] Batch 171, loss = 3.72254
I0111 18:01:01.437049 20349 caffe.cpp:314] Batch 172, accuracy = 0.345
I0111 18:01:01.437129 20349 caffe.cpp:314] Batch 172, accuracy_5 = 0.545
I0111 18:01:01.437137 20349 caffe.cpp:314] Batch 172, loss = 3.6925
I0111 18:01:03.693496 20349 caffe.cpp:314] Batch 173, accuracy = 0.38
I0111 18:01:03.693575 20349 caffe.cpp:314] Batch 173, accuracy_5 = 0.575
I0111 18:01:03.693586 20349 caffe.cpp:314] Batch 173, loss = 3.22658
I0111 18:01:06.110276 20349 caffe.cpp:314] Batch 174, accuracy = 0.37
I0111 18:01:06.110359 20349 caffe.cpp:314] Batch 174, accuracy_5 = 0.58
I0111 18:01:06.110369 20349 caffe.cpp:314] Batch 174, loss = 3.43352
I0111 18:01:08.509424 20349 caffe.cpp:314] Batch 175, accuracy = 0.35
I0111 18:01:08.509500 20349 caffe.cpp:314] Batch 175, accuracy_5 = 0.585
I0111 18:01:08.509510 20349 caffe.cpp:314] Batch 175, loss = 3.34113
I0111 18:01:10.577123 20349 caffe.cpp:314] Batch 176, accuracy = 0.27
I0111 18:01:10.577200 20349 caffe.cpp:314] Batch 176, accuracy_5 = 0.495
I0111 18:01:10.577209 20349 caffe.cpp:314] Batch 176, loss = 3.8618
I0111 18:01:12.688467 20349 caffe.cpp:314] Batch 177, accuracy = 0.29
I0111 18:01:12.688897 20349 caffe.cpp:314] Batch 177, accuracy_5 = 0.51
I0111 18:01:12.688932 20349 caffe.cpp:314] Batch 177, loss = 3.94488
I0111 18:01:14.792557 20349 caffe.cpp:314] Batch 178, accuracy = 0.295
I0111 18:01:14.792634 20349 caffe.cpp:314] Batch 178, accuracy_5 = 0.535
I0111 18:01:14.792644 20349 caffe.cpp:314] Batch 178, loss = 3.64585
I0111 18:01:16.894413 20349 caffe.cpp:314] Batch 179, accuracy = 0.345
I0111 18:01:16.894496 20349 caffe.cpp:314] Batch 179, accuracy_5 = 0.555
I0111 18:01:16.894510 20349 caffe.cpp:314] Batch 179, loss = 3.46629
I0111 18:01:18.995865 20349 caffe.cpp:314] Batch 180, accuracy = 0.32
I0111 18:01:18.995936 20349 caffe.cpp:314] Batch 180, accuracy_5 = 0.575
I0111 18:01:18.995945 20349 caffe.cpp:314] Batch 180, loss = 3.5182
I0111 18:01:21.088672 20349 caffe.cpp:314] Batch 181, accuracy = 0.275
I0111 18:01:21.088757 20349 caffe.cpp:314] Batch 181, accuracy_5 = 0.54
I0111 18:01:21.088766 20349 caffe.cpp:314] Batch 181, loss = 3.71311
I0111 18:01:23.259255 20349 caffe.cpp:314] Batch 182, accuracy = 0.32
I0111 18:01:23.259328 20349 caffe.cpp:314] Batch 182, accuracy_5 = 0.535
I0111 18:01:23.259340 20349 caffe.cpp:314] Batch 182, loss = 3.48966
I0111 18:01:25.398713 20349 caffe.cpp:314] Batch 183, accuracy = 0.33
I0111 18:01:25.398803 20349 caffe.cpp:314] Batch 183, accuracy_5 = 0.555
I0111 18:01:25.398813 20349 caffe.cpp:314] Batch 183, loss = 3.51399
I0111 18:01:27.520345 20349 caffe.cpp:314] Batch 184, accuracy = 0.39
I0111 18:01:27.520424 20349 caffe.cpp:314] Batch 184, accuracy_5 = 0.605
I0111 18:01:27.520434 20349 caffe.cpp:314] Batch 184, loss = 3.15458
I0111 18:01:29.653854 20349 caffe.cpp:314] Batch 185, accuracy = 0.35
I0111 18:01:29.653923 20349 caffe.cpp:314] Batch 185, accuracy_5 = 0.59
I0111 18:01:29.653933 20349 caffe.cpp:314] Batch 185, loss = 3.4771
I0111 18:01:31.791213 20349 caffe.cpp:314] Batch 186, accuracy = 0.31
I0111 18:01:31.791277 20349 caffe.cpp:314] Batch 186, accuracy_5 = 0.56
I0111 18:01:31.791298 20349 caffe.cpp:314] Batch 186, loss = 3.62766
I0111 18:01:33.903829 20349 caffe.cpp:314] Batch 187, accuracy = 0.32
I0111 18:01:33.903908 20349 caffe.cpp:314] Batch 187, accuracy_5 = 0.56
I0111 18:01:33.903918 20349 caffe.cpp:314] Batch 187, loss = 3.57733
I0111 18:01:35.975215 20349 caffe.cpp:314] Batch 188, accuracy = 0.33
I0111 18:01:35.975291 20349 caffe.cpp:314] Batch 188, accuracy_5 = 0.535
I0111 18:01:35.975301 20349 caffe.cpp:314] Batch 188, loss = 3.47887
I0111 18:01:38.065919 20349 caffe.cpp:314] Batch 189, accuracy = 0.35
I0111 18:01:38.065994 20349 caffe.cpp:314] Batch 189, accuracy_5 = 0.615
I0111 18:01:38.066015 20349 caffe.cpp:314] Batch 189, loss = 3.08623
I0111 18:01:40.155537 20349 caffe.cpp:314] Batch 190, accuracy = 0.32
I0111 18:01:40.155616 20349 caffe.cpp:314] Batch 190, accuracy_5 = 0.535
I0111 18:01:40.155625 20349 caffe.cpp:314] Batch 190, loss = 3.47041
I0111 18:01:42.251324 20349 caffe.cpp:314] Batch 191, accuracy = 0.345
I0111 18:01:42.251405 20349 caffe.cpp:314] Batch 191, accuracy_5 = 0.53
I0111 18:01:42.251415 20349 caffe.cpp:314] Batch 191, loss = 3.54655
I0111 18:01:44.327971 20349 caffe.cpp:314] Batch 192, accuracy = 0.295
I0111 18:01:44.328373 20349 caffe.cpp:314] Batch 192, accuracy_5 = 0.515
I0111 18:01:44.328393 20349 caffe.cpp:314] Batch 192, loss = 3.76333
I0111 18:01:46.460785 20349 caffe.cpp:314] Batch 193, accuracy = 0.33
I0111 18:01:46.460849 20349 caffe.cpp:314] Batch 193, accuracy_5 = 0.535
I0111 18:01:46.460862 20349 caffe.cpp:314] Batch 193, loss = 3.588
I0111 18:01:48.489398 20349 caffe.cpp:314] Batch 194, accuracy = 0.355
I0111 18:01:48.489470 20349 caffe.cpp:314] Batch 194, accuracy_5 = 0.515
I0111 18:01:48.489485 20349 caffe.cpp:314] Batch 194, loss = 3.63639
I0111 18:01:50.556771 20349 caffe.cpp:314] Batch 195, accuracy = 0.31
I0111 18:01:50.556839 20349 caffe.cpp:314] Batch 195, accuracy_5 = 0.495
I0111 18:01:50.556856 20349 caffe.cpp:314] Batch 195, loss = 3.92354
I0111 18:01:52.621937 20349 caffe.cpp:314] Batch 196, accuracy = 0.365
I0111 18:01:52.621997 20349 caffe.cpp:314] Batch 196, accuracy_5 = 0.595
I0111 18:01:52.622012 20349 caffe.cpp:314] Batch 196, loss = 3.39283
I0111 18:01:54.684876 20349 caffe.cpp:314] Batch 197, accuracy = 0.29
I0111 18:01:54.684932 20349 caffe.cpp:314] Batch 197, accuracy_5 = 0.535
I0111 18:01:54.684947 20349 caffe.cpp:314] Batch 197, loss = 3.62736
I0111 18:01:56.726056 20349 caffe.cpp:314] Batch 198, accuracy = 0.325
I0111 18:01:56.726143 20349 caffe.cpp:314] Batch 198, accuracy_5 = 0.52
I0111 18:01:56.726153 20349 caffe.cpp:314] Batch 198, loss = 3.56273
I0111 18:01:58.804095 20349 caffe.cpp:314] Batch 199, accuracy = 0.265
I0111 18:01:58.804172 20349 caffe.cpp:314] Batch 199, accuracy_5 = 0.525
I0111 18:01:58.804183 20349 caffe.cpp:314] Batch 199, loss = 3.8047
I0111 18:02:00.903185 20349 caffe.cpp:314] Batch 200, accuracy = 0.37
I0111 18:02:00.903264 20349 caffe.cpp:314] Batch 200, accuracy_5 = 0.565
I0111 18:02:00.903273 20349 caffe.cpp:314] Batch 200, loss = 3.50733
I0111 18:02:02.968776 20349 caffe.cpp:314] Batch 201, accuracy = 0.34
I0111 18:02:02.968855 20349 caffe.cpp:314] Batch 201, accuracy_5 = 0.54
I0111 18:02:02.968864 20349 caffe.cpp:314] Batch 201, loss = 3.62976
I0111 18:02:05.061462 20349 caffe.cpp:314] Batch 202, accuracy = 0.36
I0111 18:02:05.061558 20349 caffe.cpp:314] Batch 202, accuracy_5 = 0.55
I0111 18:02:05.061568 20349 caffe.cpp:314] Batch 202, loss = 3.28471
I0111 18:02:07.127482 20349 caffe.cpp:314] Batch 203, accuracy = 0.27
I0111 18:02:07.127549 20349 caffe.cpp:314] Batch 203, accuracy_5 = 0.49
I0111 18:02:07.127565 20349 caffe.cpp:314] Batch 203, loss = 3.85129
I0111 18:02:09.161391 20349 caffe.cpp:314] Batch 204, accuracy = 0.325
I0111 18:02:09.161473 20349 caffe.cpp:314] Batch 204, accuracy_5 = 0.575
I0111 18:02:09.161481 20349 caffe.cpp:314] Batch 204, loss = 3.39885
I0111 18:02:11.218222 20349 caffe.cpp:314] Batch 205, accuracy = 0.275
I0111 18:02:11.218305 20349 caffe.cpp:314] Batch 205, accuracy_5 = 0.485
I0111 18:02:11.218315 20349 caffe.cpp:314] Batch 205, loss = 3.9169
I0111 18:02:13.255599 20349 caffe.cpp:314] Batch 206, accuracy = 0.33
I0111 18:02:13.255679 20349 caffe.cpp:314] Batch 206, accuracy_5 = 0.535
I0111 18:02:13.255689 20349 caffe.cpp:314] Batch 206, loss = 3.5402
I0111 18:02:15.327340 20349 caffe.cpp:314] Batch 207, accuracy = 0.395
I0111 18:02:15.327630 20349 caffe.cpp:314] Batch 207, accuracy_5 = 0.58
I0111 18:02:15.327648 20349 caffe.cpp:314] Batch 207, loss = 3.37143
I0111 18:02:17.352475 20349 caffe.cpp:314] Batch 208, accuracy = 0.375
I0111 18:02:17.352552 20349 caffe.cpp:314] Batch 208, accuracy_5 = 0.565
I0111 18:02:17.352562 20349 caffe.cpp:314] Batch 208, loss = 3.43706
I0111 18:02:19.346668 20349 caffe.cpp:314] Batch 209, accuracy = 0.335
I0111 18:02:19.346745 20349 caffe.cpp:314] Batch 209, accuracy_5 = 0.51
I0111 18:02:19.346755 20349 caffe.cpp:314] Batch 209, loss = 3.76667
I0111 18:02:21.431114 20349 caffe.cpp:314] Batch 210, accuracy = 0.275
I0111 18:02:21.431191 20349 caffe.cpp:314] Batch 210, accuracy_5 = 0.505
I0111 18:02:21.431201 20349 caffe.cpp:314] Batch 210, loss = 3.81183
I0111 18:02:23.479761 20349 caffe.cpp:314] Batch 211, accuracy = 0.3
I0111 18:02:23.479835 20349 caffe.cpp:314] Batch 211, accuracy_5 = 0.535
I0111 18:02:23.479843 20349 caffe.cpp:314] Batch 211, loss = 3.86149
I0111 18:02:25.528687 20349 caffe.cpp:314] Batch 212, accuracy = 0.32
I0111 18:02:25.528764 20349 caffe.cpp:314] Batch 212, accuracy_5 = 0.53
I0111 18:02:25.528772 20349 caffe.cpp:314] Batch 212, loss = 3.52695
I0111 18:02:27.575153 20349 caffe.cpp:314] Batch 213, accuracy = 0.385
I0111 18:02:27.575232 20349 caffe.cpp:314] Batch 213, accuracy_5 = 0.615
I0111 18:02:27.575242 20349 caffe.cpp:314] Batch 213, loss = 3.18956
I0111 18:02:29.614537 20349 caffe.cpp:314] Batch 214, accuracy = 0.31
I0111 18:02:29.614634 20349 caffe.cpp:314] Batch 214, accuracy_5 = 0.515
I0111 18:02:29.614645 20349 caffe.cpp:314] Batch 214, loss = 3.8082
I0111 18:02:31.700748 20349 caffe.cpp:314] Batch 215, accuracy = 0.335
I0111 18:02:31.700825 20349 caffe.cpp:314] Batch 215, accuracy_5 = 0.545
I0111 18:02:31.700835 20349 caffe.cpp:314] Batch 215, loss = 3.52641
I0111 18:02:33.759771 20349 caffe.cpp:314] Batch 216, accuracy = 0.335
I0111 18:02:33.759860 20349 caffe.cpp:314] Batch 216, accuracy_5 = 0.53
I0111 18:02:33.759871 20349 caffe.cpp:314] Batch 216, loss = 3.75434
I0111 18:02:35.778131 20349 caffe.cpp:314] Batch 217, accuracy = 0.345
I0111 18:02:35.778205 20349 caffe.cpp:314] Batch 217, accuracy_5 = 0.53
I0111 18:02:35.778215 20349 caffe.cpp:314] Batch 217, loss = 3.66504
I0111 18:02:37.800593 20349 caffe.cpp:314] Batch 218, accuracy = 0.315
I0111 18:02:37.800668 20349 caffe.cpp:314] Batch 218, accuracy_5 = 0.525
I0111 18:02:37.800676 20349 caffe.cpp:314] Batch 218, loss = 3.66632
I0111 18:02:39.783429 20349 caffe.cpp:314] Batch 219, accuracy = 0.325
I0111 18:02:39.783498 20349 caffe.cpp:314] Batch 219, accuracy_5 = 0.545
I0111 18:02:39.783512 20349 caffe.cpp:314] Batch 219, loss = 3.67769
I0111 18:02:41.770449 20349 caffe.cpp:314] Batch 220, accuracy = 0.295
I0111 18:02:41.770542 20349 caffe.cpp:314] Batch 220, accuracy_5 = 0.475
I0111 18:02:41.770552 20349 caffe.cpp:314] Batch 220, loss = 3.59331
I0111 18:02:43.829891 20349 caffe.cpp:314] Batch 221, accuracy = 0.33
I0111 18:02:43.829972 20349 caffe.cpp:314] Batch 221, accuracy_5 = 0.56
I0111 18:02:43.829982 20349 caffe.cpp:314] Batch 221, loss = 3.46017
I0111 18:02:45.833031 20349 caffe.cpp:314] Batch 222, accuracy = 0.28
I0111 18:02:45.833509 20349 caffe.cpp:314] Batch 222, accuracy_5 = 0.495
I0111 18:02:45.833539 20349 caffe.cpp:314] Batch 222, loss = 4.06343
I0111 18:02:47.929478 20349 caffe.cpp:314] Batch 223, accuracy = 0.335
I0111 18:02:47.929548 20349 caffe.cpp:314] Batch 223, accuracy_5 = 0.545
I0111 18:02:47.929564 20349 caffe.cpp:314] Batch 223, loss = 3.71441
I0111 18:02:49.905838 20349 caffe.cpp:314] Batch 224, accuracy = 0.335
I0111 18:02:49.905910 20349 caffe.cpp:314] Batch 224, accuracy_5 = 0.59
I0111 18:02:49.905920 20349 caffe.cpp:314] Batch 224, loss = 3.41114
I0111 18:02:51.927703 20349 caffe.cpp:314] Batch 225, accuracy = 0.345
I0111 18:02:51.927788 20349 caffe.cpp:314] Batch 225, accuracy_5 = 0.555
I0111 18:02:51.927798 20349 caffe.cpp:314] Batch 225, loss = 3.3502
I0111 18:02:53.965044 20349 caffe.cpp:314] Batch 226, accuracy = 0.33
I0111 18:02:53.965114 20349 caffe.cpp:314] Batch 226, accuracy_5 = 0.56
I0111 18:02:53.965129 20349 caffe.cpp:314] Batch 226, loss = 3.60445
I0111 18:02:55.950482 20349 caffe.cpp:314] Batch 227, accuracy = 0.36
I0111 18:02:55.950548 20349 caffe.cpp:314] Batch 227, accuracy_5 = 0.52
I0111 18:02:55.950570 20349 caffe.cpp:314] Batch 227, loss = 3.59028
I0111 18:02:57.984560 20349 caffe.cpp:314] Batch 228, accuracy = 0.34
I0111 18:02:57.984640 20349 caffe.cpp:314] Batch 228, accuracy_5 = 0.55
I0111 18:02:57.984649 20349 caffe.cpp:314] Batch 228, loss = 3.51481
I0111 18:02:59.948326 20349 caffe.cpp:314] Batch 229, accuracy = 0.27
I0111 18:02:59.948403 20349 caffe.cpp:314] Batch 229, accuracy_5 = 0.495
I0111 18:02:59.948413 20349 caffe.cpp:314] Batch 229, loss = 3.90607
I0111 18:03:01.949559 20349 caffe.cpp:314] Batch 230, accuracy = 0.325
I0111 18:03:01.949640 20349 caffe.cpp:314] Batch 230, accuracy_5 = 0.56
I0111 18:03:01.949651 20349 caffe.cpp:314] Batch 230, loss = 3.68149
I0111 18:03:03.928236 20349 caffe.cpp:314] Batch 231, accuracy = 0.305
I0111 18:03:03.928318 20349 caffe.cpp:314] Batch 231, accuracy_5 = 0.545
I0111 18:03:03.928328 20349 caffe.cpp:314] Batch 231, loss = 3.54502
I0111 18:03:06.574348 20349 caffe.cpp:314] Batch 232, accuracy = 0.365
I0111 18:03:06.574434 20349 caffe.cpp:314] Batch 232, accuracy_5 = 0.535
I0111 18:03:06.574445 20349 caffe.cpp:314] Batch 232, loss = 3.51685
I0111 18:03:09.262472 20349 caffe.cpp:314] Batch 233, accuracy = 0.32
I0111 18:03:09.262650 20349 caffe.cpp:314] Batch 233, accuracy_5 = 0.49
I0111 18:03:09.262684 20349 caffe.cpp:314] Batch 233, loss = 3.8033
I0111 18:03:12.309715 20349 caffe.cpp:314] Batch 234, accuracy = 0.31
I0111 18:03:12.309789 20349 caffe.cpp:314] Batch 234, accuracy_5 = 0.545
I0111 18:03:12.309798 20349 caffe.cpp:314] Batch 234, loss = 3.61636
I0111 18:03:14.420374 20349 caffe.cpp:314] Batch 235, accuracy = 0.375
I0111 18:03:14.420439 20349 caffe.cpp:314] Batch 235, accuracy_5 = 0.525
I0111 18:03:14.420452 20349 caffe.cpp:314] Batch 235, loss = 3.53497
I0111 18:03:16.447885 20349 caffe.cpp:314] Batch 236, accuracy = 0.27
I0111 18:03:16.448266 20349 caffe.cpp:314] Batch 236, accuracy_5 = 0.515
I0111 18:03:16.448297 20349 caffe.cpp:314] Batch 236, loss = 4.01175
I0111 18:03:18.496294 20349 caffe.cpp:314] Batch 237, accuracy = 0.325
I0111 18:03:18.496361 20349 caffe.cpp:314] Batch 237, accuracy_5 = 0.525
I0111 18:03:18.496376 20349 caffe.cpp:314] Batch 237, loss = 3.6489
I0111 18:03:20.516217 20349 caffe.cpp:314] Batch 238, accuracy = 0.34
I0111 18:03:20.516286 20349 caffe.cpp:314] Batch 238, accuracy_5 = 0.57
I0111 18:03:20.516302 20349 caffe.cpp:314] Batch 238, loss = 3.40871
I0111 18:03:22.555254 20349 caffe.cpp:314] Batch 239, accuracy = 0.325
I0111 18:03:22.555321 20349 caffe.cpp:314] Batch 239, accuracy_5 = 0.52
I0111 18:03:22.555335 20349 caffe.cpp:314] Batch 239, loss = 3.63991
I0111 18:03:24.636636 20349 caffe.cpp:314] Batch 240, accuracy = 0.33
I0111 18:03:24.636709 20349 caffe.cpp:314] Batch 240, accuracy_5 = 0.52
I0111 18:03:24.636726 20349 caffe.cpp:314] Batch 240, loss = 3.56759
I0111 18:03:26.615705 20349 caffe.cpp:314] Batch 241, accuracy = 0.335
I0111 18:03:26.615773 20349 caffe.cpp:314] Batch 241, accuracy_5 = 0.47
I0111 18:03:26.615789 20349 caffe.cpp:314] Batch 241, loss = 3.94984
I0111 18:03:28.550132 20349 caffe.cpp:314] Batch 242, accuracy = 0.335
I0111 18:03:28.550199 20349 caffe.cpp:314] Batch 242, accuracy_5 = 0.535
I0111 18:03:28.550212 20349 caffe.cpp:314] Batch 242, loss = 3.73979
I0111 18:03:30.491108 20349 caffe.cpp:314] Batch 243, accuracy = 0.34
I0111 18:03:30.491178 20349 caffe.cpp:314] Batch 243, accuracy_5 = 0.525
I0111 18:03:30.491194 20349 caffe.cpp:314] Batch 243, loss = 3.73527
I0111 18:03:32.451081 20349 caffe.cpp:314] Batch 244, accuracy = 0.33
I0111 18:03:32.451148 20349 caffe.cpp:314] Batch 244, accuracy_5 = 0.55
I0111 18:03:32.451164 20349 caffe.cpp:314] Batch 244, loss = 3.53951
I0111 18:03:34.404325 20349 caffe.cpp:314] Batch 245, accuracy = 0.29
I0111 18:03:34.404408 20349 caffe.cpp:314] Batch 245, accuracy_5 = 0.485
I0111 18:03:34.404417 20349 caffe.cpp:314] Batch 245, loss = 4.1171
I0111 18:03:34.642535 20353 data_layer.cpp:73] Restarting data prefetching from start.
I0111 18:03:36.376703 20349 caffe.cpp:314] Batch 246, accuracy = 0.34
I0111 18:03:36.376780 20349 caffe.cpp:314] Batch 246, accuracy_5 = 0.53
I0111 18:03:36.376788 20349 caffe.cpp:314] Batch 246, loss = 3.59256
I0111 18:03:38.793134 20349 caffe.cpp:314] Batch 247, accuracy = 0.3
I0111 18:03:38.793203 20349 caffe.cpp:314] Batch 247, accuracy_5 = 0.52
I0111 18:03:38.793218 20349 caffe.cpp:314] Batch 247, loss = 3.89431
I0111 18:03:40.783326 20349 caffe.cpp:314] Batch 248, accuracy = 0.315
I0111 18:03:40.783407 20349 caffe.cpp:314] Batch 248, accuracy_5 = 0.475
I0111 18:03:40.783424 20349 caffe.cpp:314] Batch 248, loss = 3.78632
I0111 18:03:43.844137 20349 caffe.cpp:314] Batch 249, accuracy = 0.35
I0111 18:03:43.844203 20349 caffe.cpp:314] Batch 249, accuracy_5 = 0.6
I0111 18:03:43.844213 20349 caffe.cpp:314] Batch 249, loss = 3.23392
I0111 18:03:43.844220 20349 caffe.cpp:319] Loss: 3.58722
I0111 18:03:43.844243 20349 caffe.cpp:332] accuracy = 0.33116
I0111 18:03:43.844269 20349 caffe.cpp:332] accuracy_5 = 0.54112
I0111 18:03:43.844285 20349 caffe.cpp:332] loss = 3.58722 (* 1 = 3.58722 loss)
ydwu@ai-atd-srv:~/work/QNN-Caffe/caffe-gerrit/qnn_alexnet-BN/other_wa$ ./train_w_alexnet.sh 
I0111 18:04:56.332566 20429 caffe.cpp:218] Using GPUs 3
I0111 18:04:56.374406 20429 caffe.cpp:223] GPU 3: GeForce GTX 1080 Ti
I0111 18:04:57.825129 20429 solver.cpp:44] Initializing solver from parameters: 
test_iter: 250
test_interval: 1000
base_lr: 1e-07
display: 100
max_iter: 160000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 5e-05
snapshot: 2000
snapshot_prefix: "../other_model/alexnet_a4w8"
solver_mode: GPU
device_id: 3
net: "quan_aw_train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 84000
I0111 18:04:57.826968 20429 solver.cpp:87] Creating training net from net file: quan_aw_train_val.prototxt
I0111 18:04:57.827719 20429 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0111 18:04:57.827764 20429 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0111 18:04:57.827775 20429 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_5
I0111 18:04:57.828138 20429 net.cpp:51] Initializing net from parameters: 
name: "AlexNet-BN"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 224
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.68
  }
  data_param {
    source: "/home/DATA/imagenet_resize/lmdb_resize/ilsvrc12_train_lmdb"
    batch_size: 200
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "QuanConvolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 96
    pad: 2
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.872089
    range_high: 0.927025
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "pool1"
  top: "pool1"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 13.965383
  }
}
layer {
  name: "conv2"
  type: "QuanConvolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.806018
    range_high: 1.5182
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "pool2"
  top: "pool2"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 6.74956
  }
}
layer {
  name: "conv3"
  type: "QuanConvolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.621858
    range_high: 0.673858
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv3"
  top: "conv3"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 8.48454
  }
}
layer {
  name: "conv4"
  type: "QuanConvolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.376634
    range_high: 0.341829
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv4"
  top: "conv4"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 7.3369722
  }
}
layer {
  name: "conv5"
  type: "QuanConvolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.347658
    range_high: 0.330042
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "scale5"
  type: "Scale"
  bottom: "conv5"
  top: "conv5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "pool5"
  top: "pool5"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 8.7577133
  }
}
layer {
  name: "fc6"
  type: "QuanInnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.160061
    range_high: 0.148064
  }
}
layer {
  name: "bn6"
  type: "BatchNorm"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "scale6"
  type: "Scale"
  bottom: "fc6"
  top: "fc6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "fc6"
  top: "fc6"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 1.659358
  }
}
layer {
  name: "fc7"
  type: "QuanInnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.162636
    range_high: 0.197684
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "scale7"
  type: "Scale"
  bottom: "fc7"
  top: "fc7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "fc7"
  top: "fc7"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 7.5598521
  }
}
layer {
  name: "fc8"
  type: "QuanInnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.141857
    range_high: 0.274249
  }
}
layer {
  name: "accuracy_5_TRAIN"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy_5_TRAIN"
  include {
    phase: TRAIN
  }
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0111 18:04:57.828439 20429 layer_factory.hpp:77] Creating layer data
I0111 18:04:57.860201 20429 db_lmdb.cpp:35] Opened lmdb /home/DATA/imagenet_resize/lmdb_resize/ilsvrc12_train_lmdb
I0111 18:04:57.870749 20429 net.cpp:84] Creating Layer data
I0111 18:04:57.870791 20429 net.cpp:380] data -> data
I0111 18:04:57.870826 20429 net.cpp:380] data -> label
I0111 18:04:57.880707 20429 data_layer.cpp:45] output data size: 200,3,224,224
I0111 18:04:58.248072 20429 net.cpp:122] Setting up data
I0111 18:04:58.248148 20429 net.cpp:129] Top shape: 200 3 224 224 (30105600)
I0111 18:04:58.248160 20429 net.cpp:129] Top shape: 200 (200)
I0111 18:04:58.248167 20429 net.cpp:137] Memory required for data: 120423200
I0111 18:04:58.248201 20429 layer_factory.hpp:77] Creating layer label_data_1_split
I0111 18:04:58.248227 20429 net.cpp:84] Creating Layer label_data_1_split
I0111 18:04:58.248240 20429 net.cpp:406] label_data_1_split <- label
I0111 18:04:58.248265 20429 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0111 18:04:58.248301 20429 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0111 18:04:58.248419 20429 net.cpp:122] Setting up label_data_1_split
I0111 18:04:58.248431 20429 net.cpp:129] Top shape: 200 (200)
I0111 18:04:58.248438 20429 net.cpp:129] Top shape: 200 (200)
I0111 18:04:58.248443 20429 net.cpp:137] Memory required for data: 120424800
I0111 18:04:58.248448 20429 layer_factory.hpp:77] Creating layer conv1
I0111 18:04:58.248491 20429 net.cpp:84] Creating Layer conv1
I0111 18:04:58.248499 20429 net.cpp:406] conv1 <- data
I0111 18:04:58.248512 20429 net.cpp:380] conv1 -> conv1
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.872089;  range_high=0.927025
I0111 18:04:58.270277 20429 net.cpp:122] Setting up conv1
I0111 18:04:58.270314 20429 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 18:04:58.270323 20429 net.cpp:137] Memory required for data: 352744800
I0111 18:04:58.270354 20429 layer_factory.hpp:77] Creating layer bn1
I0111 18:04:58.270380 20429 net.cpp:84] Creating Layer bn1
I0111 18:04:58.270386 20429 net.cpp:406] bn1 <- conv1
I0111 18:04:58.270395 20429 net.cpp:367] bn1 -> conv1 (in-place)
I0111 18:04:58.270617 20429 net.cpp:122] Setting up bn1
I0111 18:04:58.270630 20429 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 18:04:58.270648 20429 net.cpp:137] Memory required for data: 585064800
I0111 18:04:58.270663 20429 layer_factory.hpp:77] Creating layer scale1
I0111 18:04:58.270676 20429 net.cpp:84] Creating Layer scale1
I0111 18:04:58.270684 20429 net.cpp:406] scale1 <- conv1
I0111 18:04:58.270742 20429 net.cpp:367] scale1 -> conv1 (in-place)
I0111 18:04:58.270813 20429 layer_factory.hpp:77] Creating layer scale1
I0111 18:04:58.270962 20429 net.cpp:122] Setting up scale1
I0111 18:04:58.270975 20429 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 18:04:58.270993 20429 net.cpp:137] Memory required for data: 817384800
I0111 18:04:58.271003 20429 layer_factory.hpp:77] Creating layer relu1
I0111 18:04:58.271013 20429 net.cpp:84] Creating Layer relu1
I0111 18:04:58.271019 20429 net.cpp:406] relu1 <- conv1
I0111 18:04:58.271029 20429 net.cpp:367] relu1 -> conv1 (in-place)
I0111 18:04:58.271051 20429 net.cpp:122] Setting up relu1
I0111 18:04:58.271060 20429 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 18:04:58.271067 20429 net.cpp:137] Memory required for data: 1049704800
I0111 18:04:58.271075 20429 layer_factory.hpp:77] Creating layer pool1
I0111 18:04:58.271087 20429 net.cpp:84] Creating Layer pool1
I0111 18:04:58.271095 20429 net.cpp:406] pool1 <- conv1
I0111 18:04:58.271104 20429 net.cpp:380] pool1 -> pool1
I0111 18:04:58.271178 20429 net.cpp:122] Setting up pool1
I0111 18:04:58.271190 20429 net.cpp:129] Top shape: 200 96 27 27 (13996800)
I0111 18:04:58.271198 20429 net.cpp:137] Memory required for data: 1105692000
I0111 18:04:58.271203 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:58.271215 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:58.271235 20429 net.cpp:406] quantized_conv1 <- pool1
I0111 18:04:58.271245 20429 net.cpp:367] quantized_conv1 -> pool1 (in-place)
I0111 18:04:58.271258 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:58.271268 20429 net.cpp:129] Top shape: 200 96 27 27 (13996800)
I0111 18:04:58.271275 20429 net.cpp:137] Memory required for data: 1161679200
I0111 18:04:58.271281 20429 layer_factory.hpp:77] Creating layer conv2
I0111 18:04:58.271298 20429 net.cpp:84] Creating Layer conv2
I0111 18:04:58.271306 20429 net.cpp:406] conv2 <- pool1
I0111 18:04:58.271317 20429 net.cpp:380] conv2 -> conv2
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.806018;  range_high=1.5182
I0111 18:04:58.282549 20429 net.cpp:122] Setting up conv2
I0111 18:04:58.282572 20429 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 18:04:58.282593 20429 net.cpp:137] Memory required for data: 1310978400
I0111 18:04:58.282610 20429 layer_factory.hpp:77] Creating layer bn2
I0111 18:04:58.282624 20429 net.cpp:84] Creating Layer bn2
I0111 18:04:58.282632 20429 net.cpp:406] bn2 <- conv2
I0111 18:04:58.282644 20429 net.cpp:367] bn2 -> conv2 (in-place)
I0111 18:04:58.282841 20429 net.cpp:122] Setting up bn2
I0111 18:04:58.282856 20429 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 18:04:58.282862 20429 net.cpp:137] Memory required for data: 1460277600
I0111 18:04:58.282887 20429 layer_factory.hpp:77] Creating layer scale2
I0111 18:04:58.282899 20429 net.cpp:84] Creating Layer scale2
I0111 18:04:58.282907 20429 net.cpp:406] scale2 <- conv2
I0111 18:04:58.282917 20429 net.cpp:367] scale2 -> conv2 (in-place)
I0111 18:04:58.282977 20429 layer_factory.hpp:77] Creating layer scale2
I0111 18:04:58.283092 20429 net.cpp:122] Setting up scale2
I0111 18:04:58.283104 20429 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 18:04:58.283110 20429 net.cpp:137] Memory required for data: 1609576800
I0111 18:04:58.283133 20429 layer_factory.hpp:77] Creating layer relu2
I0111 18:04:58.283144 20429 net.cpp:84] Creating Layer relu2
I0111 18:04:58.283151 20429 net.cpp:406] relu2 <- conv2
I0111 18:04:58.283161 20429 net.cpp:367] relu2 -> conv2 (in-place)
I0111 18:04:58.283171 20429 net.cpp:122] Setting up relu2
I0111 18:04:58.283180 20429 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 18:04:58.283187 20429 net.cpp:137] Memory required for data: 1758876000
I0111 18:04:58.283193 20429 layer_factory.hpp:77] Creating layer pool2
I0111 18:04:58.283216 20429 net.cpp:84] Creating Layer pool2
I0111 18:04:58.283224 20429 net.cpp:406] pool2 <- conv2
I0111 18:04:58.283234 20429 net.cpp:380] pool2 -> pool2
I0111 18:04:58.283290 20429 net.cpp:122] Setting up pool2
I0111 18:04:58.283303 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:58.283339 20429 net.cpp:137] Memory required for data: 1793487200
I0111 18:04:58.283347 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:58.283358 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:58.283365 20429 net.cpp:406] quantized_conv1 <- pool2
I0111 18:04:58.283375 20429 net.cpp:367] quantized_conv1 -> pool2 (in-place)
I0111 18:04:58.283385 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:58.283393 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:58.283399 20429 net.cpp:137] Memory required for data: 1828098400
I0111 18:04:58.283406 20429 layer_factory.hpp:77] Creating layer conv3
I0111 18:04:58.283418 20429 net.cpp:84] Creating Layer conv3
I0111 18:04:58.283426 20429 net.cpp:406] conv3 <- pool2
I0111 18:04:58.283437 20429 net.cpp:380] conv3 -> conv3
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.621858;  range_high=0.673858
I0111 18:04:58.297771 20429 net.cpp:122] Setting up conv3
I0111 18:04:58.297804 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.297811 20429 net.cpp:137] Memory required for data: 1880015200
I0111 18:04:58.297844 20429 layer_factory.hpp:77] Creating layer bn3
I0111 18:04:58.297860 20429 net.cpp:84] Creating Layer bn3
I0111 18:04:58.297868 20429 net.cpp:406] bn3 <- conv3
I0111 18:04:58.297878 20429 net.cpp:367] bn3 -> conv3 (in-place)
I0111 18:04:58.298089 20429 net.cpp:122] Setting up bn3
I0111 18:04:58.298115 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.298121 20429 net.cpp:137] Memory required for data: 1931932000
I0111 18:04:58.298140 20429 layer_factory.hpp:77] Creating layer scale3
I0111 18:04:58.298154 20429 net.cpp:84] Creating Layer scale3
I0111 18:04:58.298162 20429 net.cpp:406] scale3 <- conv3
I0111 18:04:58.298172 20429 net.cpp:367] scale3 -> conv3 (in-place)
I0111 18:04:58.298221 20429 layer_factory.hpp:77] Creating layer scale3
I0111 18:04:58.298348 20429 net.cpp:122] Setting up scale3
I0111 18:04:58.298362 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.298369 20429 net.cpp:137] Memory required for data: 1983848800
I0111 18:04:58.298380 20429 layer_factory.hpp:77] Creating layer relu3
I0111 18:04:58.298391 20429 net.cpp:84] Creating Layer relu3
I0111 18:04:58.298398 20429 net.cpp:406] relu3 <- conv3
I0111 18:04:58.298409 20429 net.cpp:367] relu3 -> conv3 (in-place)
I0111 18:04:58.298418 20429 net.cpp:122] Setting up relu3
I0111 18:04:58.298427 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.298434 20429 net.cpp:137] Memory required for data: 2035765600
I0111 18:04:58.298441 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:58.298452 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:58.298460 20429 net.cpp:406] quantized_conv1 <- conv3
I0111 18:04:58.298468 20429 net.cpp:367] quantized_conv1 -> conv3 (in-place)
I0111 18:04:58.298481 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:58.298491 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.298497 20429 net.cpp:137] Memory required for data: 2087682400
I0111 18:04:58.298504 20429 layer_factory.hpp:77] Creating layer conv4
I0111 18:04:58.298518 20429 net.cpp:84] Creating Layer conv4
I0111 18:04:58.298526 20429 net.cpp:406] conv4 <- conv3
I0111 18:04:58.298537 20429 net.cpp:380] conv4 -> conv4
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.376634;  range_high=0.341829
I0111 18:04:58.320158 20429 net.cpp:122] Setting up conv4
I0111 18:04:58.320181 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.320188 20429 net.cpp:137] Memory required for data: 2139599200
I0111 18:04:58.320199 20429 layer_factory.hpp:77] Creating layer bn4
I0111 18:04:58.320211 20429 net.cpp:84] Creating Layer bn4
I0111 18:04:58.320219 20429 net.cpp:406] bn4 <- conv4
I0111 18:04:58.320231 20429 net.cpp:367] bn4 -> conv4 (in-place)
I0111 18:04:58.320436 20429 net.cpp:122] Setting up bn4
I0111 18:04:58.320451 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.320461 20429 net.cpp:137] Memory required for data: 2191516000
I0111 18:04:58.320472 20429 layer_factory.hpp:77] Creating layer scale4
I0111 18:04:58.320483 20429 net.cpp:84] Creating Layer scale4
I0111 18:04:58.320523 20429 net.cpp:406] scale4 <- conv4
I0111 18:04:58.320531 20429 net.cpp:367] scale4 -> conv4 (in-place)
I0111 18:04:58.320580 20429 layer_factory.hpp:77] Creating layer scale4
I0111 18:04:58.320709 20429 net.cpp:122] Setting up scale4
I0111 18:04:58.320722 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.320729 20429 net.cpp:137] Memory required for data: 2243432800
I0111 18:04:58.320739 20429 layer_factory.hpp:77] Creating layer relu4
I0111 18:04:58.320749 20429 net.cpp:84] Creating Layer relu4
I0111 18:04:58.320755 20429 net.cpp:406] relu4 <- conv4
I0111 18:04:58.320766 20429 net.cpp:367] relu4 -> conv4 (in-place)
I0111 18:04:58.320776 20429 net.cpp:122] Setting up relu4
I0111 18:04:58.320785 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.320791 20429 net.cpp:137] Memory required for data: 2295349600
I0111 18:04:58.320796 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:58.320818 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:58.320827 20429 net.cpp:406] quantized_conv1 <- conv4
I0111 18:04:58.320835 20429 net.cpp:367] quantized_conv1 -> conv4 (in-place)
I0111 18:04:58.320845 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:58.320854 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:58.320860 20429 net.cpp:137] Memory required for data: 2347266400
I0111 18:04:58.320868 20429 layer_factory.hpp:77] Creating layer conv5
I0111 18:04:58.320883 20429 net.cpp:84] Creating Layer conv5
I0111 18:04:58.320891 20429 net.cpp:406] conv5 <- conv4
I0111 18:04:58.320904 20429 net.cpp:380] conv5 -> conv5
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.347658;  range_high=0.330042
I0111 18:04:58.336361 20429 net.cpp:122] Setting up conv5
I0111 18:04:58.336380 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:58.336387 20429 net.cpp:137] Memory required for data: 2381877600
I0111 18:04:58.336400 20429 layer_factory.hpp:77] Creating layer bn5
I0111 18:04:58.336424 20429 net.cpp:84] Creating Layer bn5
I0111 18:04:58.336432 20429 net.cpp:406] bn5 <- conv5
I0111 18:04:58.336446 20429 net.cpp:367] bn5 -> conv5 (in-place)
I0111 18:04:58.336660 20429 net.cpp:122] Setting up bn5
I0111 18:04:58.336675 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:58.336683 20429 net.cpp:137] Memory required for data: 2416488800
I0111 18:04:58.336707 20429 layer_factory.hpp:77] Creating layer scale5
I0111 18:04:58.336720 20429 net.cpp:84] Creating Layer scale5
I0111 18:04:58.336727 20429 net.cpp:406] scale5 <- conv5
I0111 18:04:58.336737 20429 net.cpp:367] scale5 -> conv5 (in-place)
I0111 18:04:58.336802 20429 layer_factory.hpp:77] Creating layer scale5
I0111 18:04:58.336925 20429 net.cpp:122] Setting up scale5
I0111 18:04:58.336940 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:58.336946 20429 net.cpp:137] Memory required for data: 2451100000
I0111 18:04:58.336956 20429 layer_factory.hpp:77] Creating layer relu5
I0111 18:04:58.336985 20429 net.cpp:84] Creating Layer relu5
I0111 18:04:58.336993 20429 net.cpp:406] relu5 <- conv5
I0111 18:04:58.337002 20429 net.cpp:367] relu5 -> conv5 (in-place)
I0111 18:04:58.337013 20429 net.cpp:122] Setting up relu5
I0111 18:04:58.337021 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:58.337028 20429 net.cpp:137] Memory required for data: 2485711200
I0111 18:04:58.337034 20429 layer_factory.hpp:77] Creating layer pool5
I0111 18:04:58.337049 20429 net.cpp:84] Creating Layer pool5
I0111 18:04:58.337056 20429 net.cpp:406] pool5 <- conv5
I0111 18:04:58.337065 20429 net.cpp:380] pool5 -> pool5
I0111 18:04:58.337117 20429 net.cpp:122] Setting up pool5
I0111 18:04:58.337131 20429 net.cpp:129] Top shape: 200 256 6 6 (1843200)
I0111 18:04:58.337137 20429 net.cpp:137] Memory required for data: 2493084000
I0111 18:04:58.337146 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:58.337155 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:58.337162 20429 net.cpp:406] quantized_conv1 <- pool5
I0111 18:04:58.337186 20429 net.cpp:367] quantized_conv1 -> pool5 (in-place)
I0111 18:04:58.337198 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:58.337239 20429 net.cpp:129] Top shape: 200 256 6 6 (1843200)
I0111 18:04:58.337249 20429 net.cpp:137] Memory required for data: 2500456800
I0111 18:04:58.337263 20429 layer_factory.hpp:77] Creating layer fc6
I0111 18:04:58.337276 20429 net.cpp:84] Creating Layer fc6
I0111 18:04:58.337283 20429 net.cpp:406] fc6 <- pool5
I0111 18:04:58.337294 20429 net.cpp:380] fc6 -> fc6
I0111 18:04:58.337312 20429 quan_inner_product_layer.cpp:21] 4096   4096
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.160061;  range_high=0.148064
I0111 18:04:58.950455 20429 net.cpp:122] Setting up fc6
I0111 18:04:58.950515 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:58.950522 20429 net.cpp:137] Memory required for data: 2503733600
I0111 18:04:58.950549 20429 layer_factory.hpp:77] Creating layer bn6
I0111 18:04:58.950572 20429 net.cpp:84] Creating Layer bn6
I0111 18:04:58.950584 20429 net.cpp:406] bn6 <- fc6
I0111 18:04:58.950598 20429 net.cpp:367] bn6 -> fc6 (in-place)
I0111 18:04:58.950837 20429 net.cpp:122] Setting up bn6
I0111 18:04:58.950851 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:58.950870 20429 net.cpp:137] Memory required for data: 2507010400
I0111 18:04:58.950883 20429 layer_factory.hpp:77] Creating layer scale6
I0111 18:04:58.950908 20429 net.cpp:84] Creating Layer scale6
I0111 18:04:58.950917 20429 net.cpp:406] scale6 <- fc6
I0111 18:04:58.950927 20429 net.cpp:367] scale6 -> fc6 (in-place)
I0111 18:04:58.950987 20429 layer_factory.hpp:77] Creating layer scale6
I0111 18:04:58.951135 20429 net.cpp:122] Setting up scale6
I0111 18:04:58.951149 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:58.951156 20429 net.cpp:137] Memory required for data: 2510287200
I0111 18:04:58.951167 20429 layer_factory.hpp:77] Creating layer relu6
I0111 18:04:58.951179 20429 net.cpp:84] Creating Layer relu6
I0111 18:04:58.951192 20429 net.cpp:406] relu6 <- fc6
I0111 18:04:58.951205 20429 net.cpp:367] relu6 -> fc6 (in-place)
I0111 18:04:58.951216 20429 net.cpp:122] Setting up relu6
I0111 18:04:58.951225 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:58.951232 20429 net.cpp:137] Memory required for data: 2513564000
I0111 18:04:58.951239 20429 layer_factory.hpp:77] Creating layer drop6
I0111 18:04:58.951251 20429 net.cpp:84] Creating Layer drop6
I0111 18:04:58.951257 20429 net.cpp:406] drop6 <- fc6
I0111 18:04:58.951267 20429 net.cpp:367] drop6 -> fc6 (in-place)
I0111 18:04:58.951313 20429 net.cpp:122] Setting up drop6
I0111 18:04:58.951325 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:58.951333 20429 net.cpp:137] Memory required for data: 2516840800
I0111 18:04:58.951339 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:58.951351 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:58.951359 20429 net.cpp:406] quantized_conv1 <- fc6
I0111 18:04:58.951367 20429 net.cpp:367] quantized_conv1 -> fc6 (in-place)
I0111 18:04:58.951380 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:58.951388 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:58.951395 20429 net.cpp:137] Memory required for data: 2520117600
I0111 18:04:58.951401 20429 layer_factory.hpp:77] Creating layer fc7
I0111 18:04:58.951414 20429 net.cpp:84] Creating Layer fc7
I0111 18:04:58.951421 20429 net.cpp:406] fc7 <- fc6
I0111 18:04:58.951434 20429 net.cpp:380] fc7 -> fc7
I0111 18:04:58.951448 20429 quan_inner_product_layer.cpp:21] 4096   4096
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.162636;  range_high=0.197684
I0111 18:04:59.219722 20429 net.cpp:122] Setting up fc7
I0111 18:04:59.219779 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:59.219785 20429 net.cpp:137] Memory required for data: 2523394400
I0111 18:04:59.219805 20429 layer_factory.hpp:77] Creating layer bn7
I0111 18:04:59.219841 20429 net.cpp:84] Creating Layer bn7
I0111 18:04:59.219849 20429 net.cpp:406] bn7 <- fc7
I0111 18:04:59.219861 20429 net.cpp:367] bn7 -> fc7 (in-place)
I0111 18:04:59.220124 20429 net.cpp:122] Setting up bn7
I0111 18:04:59.220149 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:59.220156 20429 net.cpp:137] Memory required for data: 2526671200
I0111 18:04:59.220168 20429 layer_factory.hpp:77] Creating layer scale7
I0111 18:04:59.220240 20429 net.cpp:84] Creating Layer scale7
I0111 18:04:59.220247 20429 net.cpp:406] scale7 <- fc7
I0111 18:04:59.220254 20429 net.cpp:367] scale7 -> fc7 (in-place)
I0111 18:04:59.220338 20429 layer_factory.hpp:77] Creating layer scale7
I0111 18:04:59.220510 20429 net.cpp:122] Setting up scale7
I0111 18:04:59.220523 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:59.220541 20429 net.cpp:137] Memory required for data: 2529948000
I0111 18:04:59.220552 20429 layer_factory.hpp:77] Creating layer relu7
I0111 18:04:59.220566 20429 net.cpp:84] Creating Layer relu7
I0111 18:04:59.220573 20429 net.cpp:406] relu7 <- fc7
I0111 18:04:59.220582 20429 net.cpp:367] relu7 -> fc7 (in-place)
I0111 18:04:59.220597 20429 net.cpp:122] Setting up relu7
I0111 18:04:59.220604 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:59.220612 20429 net.cpp:137] Memory required for data: 2533224800
I0111 18:04:59.220618 20429 layer_factory.hpp:77] Creating layer drop7
I0111 18:04:59.220630 20429 net.cpp:84] Creating Layer drop7
I0111 18:04:59.220638 20429 net.cpp:406] drop7 <- fc7
I0111 18:04:59.220646 20429 net.cpp:367] drop7 -> fc7 (in-place)
I0111 18:04:59.220681 20429 net.cpp:122] Setting up drop7
I0111 18:04:59.220695 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:59.220701 20429 net.cpp:137] Memory required for data: 2536501600
I0111 18:04:59.220708 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:59.220721 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:59.220727 20429 net.cpp:406] quantized_conv1 <- fc7
I0111 18:04:59.220736 20429 net.cpp:367] quantized_conv1 -> fc7 (in-place)
I0111 18:04:59.220748 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:59.220757 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:04:59.220763 20429 net.cpp:137] Memory required for data: 2539778400
I0111 18:04:59.220769 20429 layer_factory.hpp:77] Creating layer fc8
I0111 18:04:59.220782 20429 net.cpp:84] Creating Layer fc8
I0111 18:04:59.220789 20429 net.cpp:406] fc8 <- fc7
I0111 18:04:59.220801 20429 net.cpp:380] fc8 -> fc8
I0111 18:04:59.220815 20429 quan_inner_product_layer.cpp:21] 1000   1000
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.141857;  range_high=0.274249
I0111 18:04:59.287291 20429 net.cpp:122] Setting up fc8
I0111 18:04:59.287331 20429 net.cpp:129] Top shape: 200 1000 (200000)
I0111 18:04:59.287338 20429 net.cpp:137] Memory required for data: 2540578400
I0111 18:04:59.287355 20429 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0111 18:04:59.287384 20429 net.cpp:84] Creating Layer fc8_fc8_0_split
I0111 18:04:59.287392 20429 net.cpp:406] fc8_fc8_0_split <- fc8
I0111 18:04:59.287405 20429 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0111 18:04:59.287420 20429 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0111 18:04:59.287492 20429 net.cpp:122] Setting up fc8_fc8_0_split
I0111 18:04:59.287518 20429 net.cpp:129] Top shape: 200 1000 (200000)
I0111 18:04:59.287525 20429 net.cpp:129] Top shape: 200 1000 (200000)
I0111 18:04:59.287530 20429 net.cpp:137] Memory required for data: 2542178400
I0111 18:04:59.287536 20429 layer_factory.hpp:77] Creating layer accuracy_5_TRAIN
I0111 18:04:59.287554 20429 net.cpp:84] Creating Layer accuracy_5_TRAIN
I0111 18:04:59.287572 20429 net.cpp:406] accuracy_5_TRAIN <- fc8_fc8_0_split_0
I0111 18:04:59.287580 20429 net.cpp:406] accuracy_5_TRAIN <- label_data_1_split_0
I0111 18:04:59.287593 20429 net.cpp:380] accuracy_5_TRAIN -> accuracy_5_TRAIN
I0111 18:04:59.287616 20429 net.cpp:122] Setting up accuracy_5_TRAIN
I0111 18:04:59.287627 20429 net.cpp:129] Top shape: (1)
I0111 18:04:59.287632 20429 net.cpp:137] Memory required for data: 2542178404
I0111 18:04:59.287639 20429 layer_factory.hpp:77] Creating layer loss
I0111 18:04:59.287650 20429 net.cpp:84] Creating Layer loss
I0111 18:04:59.287657 20429 net.cpp:406] loss <- fc8_fc8_0_split_1
I0111 18:04:59.287664 20429 net.cpp:406] loss <- label_data_1_split_1
I0111 18:04:59.287675 20429 net.cpp:380] loss -> loss
I0111 18:04:59.287693 20429 layer_factory.hpp:77] Creating layer loss
I0111 18:04:59.290004 20429 net.cpp:122] Setting up loss
I0111 18:04:59.290069 20429 net.cpp:129] Top shape: (1)
I0111 18:04:59.290078 20429 net.cpp:132]     with loss weight 1
I0111 18:04:59.290089 20429 net.cpp:137] Memory required for data: 2542178408
I0111 18:04:59.290098 20429 net.cpp:198] loss needs backward computation.
I0111 18:04:59.290114 20429 net.cpp:200] accuracy_5_TRAIN does not need backward computation.
I0111 18:04:59.290123 20429 net.cpp:198] fc8_fc8_0_split needs backward computation.
I0111 18:04:59.290130 20429 net.cpp:198] fc8 needs backward computation.
I0111 18:04:59.290138 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:04:59.290144 20429 net.cpp:198] drop7 needs backward computation.
I0111 18:04:59.290151 20429 net.cpp:198] relu7 needs backward computation.
I0111 18:04:59.290158 20429 net.cpp:198] scale7 needs backward computation.
I0111 18:04:59.290164 20429 net.cpp:198] bn7 needs backward computation.
I0111 18:04:59.290171 20429 net.cpp:198] fc7 needs backward computation.
I0111 18:04:59.290179 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:04:59.290185 20429 net.cpp:198] drop6 needs backward computation.
I0111 18:04:59.290191 20429 net.cpp:198] relu6 needs backward computation.
I0111 18:04:59.290199 20429 net.cpp:198] scale6 needs backward computation.
I0111 18:04:59.290205 20429 net.cpp:198] bn6 needs backward computation.
I0111 18:04:59.290211 20429 net.cpp:198] fc6 needs backward computation.
I0111 18:04:59.290218 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:04:59.290225 20429 net.cpp:198] pool5 needs backward computation.
I0111 18:04:59.290232 20429 net.cpp:198] relu5 needs backward computation.
I0111 18:04:59.290240 20429 net.cpp:198] scale5 needs backward computation.
I0111 18:04:59.290246 20429 net.cpp:198] bn5 needs backward computation.
I0111 18:04:59.290252 20429 net.cpp:198] conv5 needs backward computation.
I0111 18:04:59.290259 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:04:59.290266 20429 net.cpp:198] relu4 needs backward computation.
I0111 18:04:59.290272 20429 net.cpp:198] scale4 needs backward computation.
I0111 18:04:59.290279 20429 net.cpp:198] bn4 needs backward computation.
I0111 18:04:59.290285 20429 net.cpp:198] conv4 needs backward computation.
I0111 18:04:59.290292 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:04:59.290299 20429 net.cpp:198] relu3 needs backward computation.
I0111 18:04:59.290307 20429 net.cpp:198] scale3 needs backward computation.
I0111 18:04:59.290313 20429 net.cpp:198] bn3 needs backward computation.
I0111 18:04:59.290319 20429 net.cpp:198] conv3 needs backward computation.
I0111 18:04:59.290326 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:04:59.290333 20429 net.cpp:198] pool2 needs backward computation.
I0111 18:04:59.290340 20429 net.cpp:198] relu2 needs backward computation.
I0111 18:04:59.290347 20429 net.cpp:198] scale2 needs backward computation.
I0111 18:04:59.290354 20429 net.cpp:198] bn2 needs backward computation.
I0111 18:04:59.290360 20429 net.cpp:198] conv2 needs backward computation.
I0111 18:04:59.290366 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:04:59.290374 20429 net.cpp:198] pool1 needs backward computation.
I0111 18:04:59.290380 20429 net.cpp:198] relu1 needs backward computation.
I0111 18:04:59.290387 20429 net.cpp:198] scale1 needs backward computation.
I0111 18:04:59.290393 20429 net.cpp:198] bn1 needs backward computation.
I0111 18:04:59.290400 20429 net.cpp:198] conv1 needs backward computation.
I0111 18:04:59.290407 20429 net.cpp:200] label_data_1_split does not need backward computation.
I0111 18:04:59.290416 20429 net.cpp:200] data does not need backward computation.
I0111 18:04:59.290422 20429 net.cpp:242] This network produces output accuracy_5_TRAIN
I0111 18:04:59.290431 20429 net.cpp:242] This network produces output loss
I0111 18:04:59.290462 20429 net.cpp:255] Network initialization done.
I0111 18:04:59.291096 20429 solver.cpp:172] Creating test net (#0) specified by net file: quan_aw_train_val.prototxt
I0111 18:04:59.291213 20429 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0111 18:04:59.291244 20429 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_5_TRAIN
I0111 18:04:59.291589 20429 net.cpp:51] Initializing net from parameters: 
name: "AlexNet-BN"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.68
  }
  data_param {
    source: "/home/DATA/imagenet_resize/lmdb_resize/ilsvrc12_val_lmdb"
    batch_size: 200
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "QuanConvolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 96
    pad: 2
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.872089
    range_high: 0.927025
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "pool1"
  top: "pool1"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 13.965383
  }
}
layer {
  name: "conv2"
  type: "QuanConvolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.806018
    range_high: 1.5182
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "pool2"
  top: "pool2"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 6.74956
  }
}
layer {
  name: "conv3"
  type: "QuanConvolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.621858
    range_high: 0.673858
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv3"
  top: "conv3"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 8.48454
  }
}
layer {
  name: "conv4"
  type: "QuanConvolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.376634
    range_high: 0.341829
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv4"
  top: "conv4"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 7.3369722
  }
}
layer {
  name: "conv5"
  type: "QuanConvolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.347658
    range_high: 0.330042
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "scale5"
  type: "Scale"
  bottom: "conv5"
  top: "conv5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "pool5"
  top: "pool5"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 8.7577133
  }
}
layer {
  name: "fc6"
  type: "QuanInnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.160061
    range_high: 0.148064
  }
}
layer {
  name: "bn6"
  type: "BatchNorm"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "scale6"
  type: "Scale"
  bottom: "fc6"
  top: "fc6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "fc6"
  top: "fc6"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 1.659358
  }
}
layer {
  name: "fc7"
  type: "QuanInnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.162636
    range_high: 0.197684
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "scale7"
  type: "Scale"
  bottom: "fc7"
  top: "fc7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "fc7"
  top: "fc7"
  quantization_param {
    round_method: ROUND
    round_strategy: AGGRESSIVE
    bit_width: 4
    range: 0
    range: 7.5598521
  }
}
layer {
  name: "fc8"
  type: "QuanInnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  quan_inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    round_method: ROUND
    round_strategy: NEUTRAL
    bit_width: 8
    range_low: -0.141857
    range_high: 0.274249
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy_5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0111 18:04:59.291834 20429 layer_factory.hpp:77] Creating layer data
I0111 18:04:59.291975 20429 db_lmdb.cpp:35] Opened lmdb /home/DATA/imagenet_resize/lmdb_resize/ilsvrc12_val_lmdb
I0111 18:04:59.292019 20429 net.cpp:84] Creating Layer data
I0111 18:04:59.292034 20429 net.cpp:380] data -> data
I0111 18:04:59.292049 20429 net.cpp:380] data -> label
I0111 18:04:59.292425 20429 data_layer.cpp:45] output data size: 200,3,224,224
I0111 18:04:59.790794 20429 net.cpp:122] Setting up data
I0111 18:04:59.790869 20429 net.cpp:129] Top shape: 200 3 224 224 (30105600)
I0111 18:04:59.790882 20429 net.cpp:129] Top shape: 200 (200)
I0111 18:04:59.790896 20429 net.cpp:137] Memory required for data: 120423200
I0111 18:04:59.790920 20429 layer_factory.hpp:77] Creating layer label_data_1_split
I0111 18:04:59.790944 20429 net.cpp:84] Creating Layer label_data_1_split
I0111 18:04:59.790953 20429 net.cpp:406] label_data_1_split <- label
I0111 18:04:59.790969 20429 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0111 18:04:59.790990 20429 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0111 18:04:59.791002 20429 net.cpp:380] label_data_1_split -> label_data_1_split_2
I0111 18:04:59.791205 20429 net.cpp:122] Setting up label_data_1_split
I0111 18:04:59.791260 20429 net.cpp:129] Top shape: 200 (200)
I0111 18:04:59.791268 20429 net.cpp:129] Top shape: 200 (200)
I0111 18:04:59.791275 20429 net.cpp:129] Top shape: 200 (200)
I0111 18:04:59.791283 20429 net.cpp:137] Memory required for data: 120425600
I0111 18:04:59.791293 20429 layer_factory.hpp:77] Creating layer conv1
I0111 18:04:59.791326 20429 net.cpp:84] Creating Layer conv1
I0111 18:04:59.791335 20429 net.cpp:406] conv1 <- data
I0111 18:04:59.791350 20429 net.cpp:380] conv1 -> conv1
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.872089;  range_high=0.927025
I0111 18:04:59.792421 20429 net.cpp:122] Setting up conv1
I0111 18:04:59.792445 20429 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 18:04:59.792453 20429 net.cpp:137] Memory required for data: 352745600
I0111 18:04:59.792471 20429 layer_factory.hpp:77] Creating layer bn1
I0111 18:04:59.792495 20429 net.cpp:84] Creating Layer bn1
I0111 18:04:59.792502 20429 net.cpp:406] bn1 <- conv1
I0111 18:04:59.792512 20429 net.cpp:367] bn1 -> conv1 (in-place)
I0111 18:04:59.811642 20429 net.cpp:122] Setting up bn1
I0111 18:04:59.811722 20429 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 18:04:59.811728 20429 net.cpp:137] Memory required for data: 585065600
I0111 18:04:59.811784 20429 layer_factory.hpp:77] Creating layer scale1
I0111 18:04:59.811812 20429 net.cpp:84] Creating Layer scale1
I0111 18:04:59.811821 20429 net.cpp:406] scale1 <- conv1
I0111 18:04:59.811837 20429 net.cpp:367] scale1 -> conv1 (in-place)
I0111 18:04:59.811944 20429 layer_factory.hpp:77] Creating layer scale1
I0111 18:04:59.812115 20429 net.cpp:122] Setting up scale1
I0111 18:04:59.812130 20429 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 18:04:59.812136 20429 net.cpp:137] Memory required for data: 817385600
I0111 18:04:59.812147 20429 layer_factory.hpp:77] Creating layer relu1
I0111 18:04:59.812223 20429 net.cpp:84] Creating Layer relu1
I0111 18:04:59.812230 20429 net.cpp:406] relu1 <- conv1
I0111 18:04:59.812240 20429 net.cpp:367] relu1 -> conv1 (in-place)
I0111 18:04:59.812252 20429 net.cpp:122] Setting up relu1
I0111 18:04:59.812261 20429 net.cpp:129] Top shape: 200 96 55 55 (58080000)
I0111 18:04:59.812268 20429 net.cpp:137] Memory required for data: 1049705600
I0111 18:04:59.812274 20429 layer_factory.hpp:77] Creating layer pool1
I0111 18:04:59.812288 20429 net.cpp:84] Creating Layer pool1
I0111 18:04:59.812294 20429 net.cpp:406] pool1 <- conv1
I0111 18:04:59.812304 20429 net.cpp:380] pool1 -> pool1
I0111 18:04:59.812362 20429 net.cpp:122] Setting up pool1
I0111 18:04:59.812376 20429 net.cpp:129] Top shape: 200 96 27 27 (13996800)
I0111 18:04:59.812383 20429 net.cpp:137] Memory required for data: 1105692800
I0111 18:04:59.812391 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:59.812405 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:59.812412 20429 net.cpp:406] quantized_conv1 <- pool1
I0111 18:04:59.812422 20429 net.cpp:367] quantized_conv1 -> pool1 (in-place)
I0111 18:04:59.812435 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:59.812444 20429 net.cpp:129] Top shape: 200 96 27 27 (13996800)
I0111 18:04:59.812451 20429 net.cpp:137] Memory required for data: 1161680000
I0111 18:04:59.812458 20429 layer_factory.hpp:77] Creating layer conv2
I0111 18:04:59.812481 20429 net.cpp:84] Creating Layer conv2
I0111 18:04:59.812489 20429 net.cpp:406] conv2 <- pool1
I0111 18:04:59.812500 20429 net.cpp:380] conv2 -> conv2
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.806018;  range_high=1.5182
I0111 18:04:59.823848 20429 net.cpp:122] Setting up conv2
I0111 18:04:59.823881 20429 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 18:04:59.823889 20429 net.cpp:137] Memory required for data: 1310979200
I0111 18:04:59.823911 20429 layer_factory.hpp:77] Creating layer bn2
I0111 18:04:59.823931 20429 net.cpp:84] Creating Layer bn2
I0111 18:04:59.823940 20429 net.cpp:406] bn2 <- conv2
I0111 18:04:59.823963 20429 net.cpp:367] bn2 -> conv2 (in-place)
I0111 18:04:59.824198 20429 net.cpp:122] Setting up bn2
I0111 18:04:59.824213 20429 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 18:04:59.824218 20429 net.cpp:137] Memory required for data: 1460278400
I0111 18:04:59.824229 20429 layer_factory.hpp:77] Creating layer scale2
I0111 18:04:59.824244 20429 net.cpp:84] Creating Layer scale2
I0111 18:04:59.824251 20429 net.cpp:406] scale2 <- conv2
I0111 18:04:59.824260 20429 net.cpp:367] scale2 -> conv2 (in-place)
I0111 18:04:59.824326 20429 layer_factory.hpp:77] Creating layer scale2
I0111 18:04:59.824466 20429 net.cpp:122] Setting up scale2
I0111 18:04:59.824481 20429 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 18:04:59.824486 20429 net.cpp:137] Memory required for data: 1609577600
I0111 18:04:59.824496 20429 layer_factory.hpp:77] Creating layer relu2
I0111 18:04:59.824507 20429 net.cpp:84] Creating Layer relu2
I0111 18:04:59.824515 20429 net.cpp:406] relu2 <- conv2
I0111 18:04:59.824525 20429 net.cpp:367] relu2 -> conv2 (in-place)
I0111 18:04:59.824537 20429 net.cpp:122] Setting up relu2
I0111 18:04:59.824544 20429 net.cpp:129] Top shape: 200 256 27 27 (37324800)
I0111 18:04:59.824551 20429 net.cpp:137] Memory required for data: 1758876800
I0111 18:04:59.824558 20429 layer_factory.hpp:77] Creating layer pool2
I0111 18:04:59.824573 20429 net.cpp:84] Creating Layer pool2
I0111 18:04:59.824580 20429 net.cpp:406] pool2 <- conv2
I0111 18:04:59.824591 20429 net.cpp:380] pool2 -> pool2
I0111 18:04:59.824651 20429 net.cpp:122] Setting up pool2
I0111 18:04:59.824664 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:59.824671 20429 net.cpp:137] Memory required for data: 1793488000
I0111 18:04:59.824678 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:59.824692 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:59.824699 20429 net.cpp:406] quantized_conv1 <- pool2
I0111 18:04:59.824710 20429 net.cpp:367] quantized_conv1 -> pool2 (in-place)
I0111 18:04:59.824723 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:59.824779 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:59.824785 20429 net.cpp:137] Memory required for data: 1828099200
I0111 18:04:59.824792 20429 layer_factory.hpp:77] Creating layer conv3
I0111 18:04:59.824810 20429 net.cpp:84] Creating Layer conv3
I0111 18:04:59.824816 20429 net.cpp:406] conv3 <- pool2
I0111 18:04:59.824828 20429 net.cpp:380] conv3 -> conv3
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.621858;  range_high=0.673858
I0111 18:04:59.842250 20429 net.cpp:122] Setting up conv3
I0111 18:04:59.842283 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.842290 20429 net.cpp:137] Memory required for data: 1880016000
I0111 18:04:59.842306 20429 layer_factory.hpp:77] Creating layer bn3
I0111 18:04:59.842325 20429 net.cpp:84] Creating Layer bn3
I0111 18:04:59.842334 20429 net.cpp:406] bn3 <- conv3
I0111 18:04:59.842346 20429 net.cpp:367] bn3 -> conv3 (in-place)
I0111 18:04:59.842573 20429 net.cpp:122] Setting up bn3
I0111 18:04:59.842587 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.842594 20429 net.cpp:137] Memory required for data: 1931932800
I0111 18:04:59.842617 20429 layer_factory.hpp:77] Creating layer scale3
I0111 18:04:59.842634 20429 net.cpp:84] Creating Layer scale3
I0111 18:04:59.842641 20429 net.cpp:406] scale3 <- conv3
I0111 18:04:59.842649 20429 net.cpp:367] scale3 -> conv3 (in-place)
I0111 18:04:59.842712 20429 layer_factory.hpp:77] Creating layer scale3
I0111 18:04:59.842856 20429 net.cpp:122] Setting up scale3
I0111 18:04:59.842870 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.842876 20429 net.cpp:137] Memory required for data: 1983849600
I0111 18:04:59.842887 20429 layer_factory.hpp:77] Creating layer relu3
I0111 18:04:59.842908 20429 net.cpp:84] Creating Layer relu3
I0111 18:04:59.842914 20429 net.cpp:406] relu3 <- conv3
I0111 18:04:59.842927 20429 net.cpp:367] relu3 -> conv3 (in-place)
I0111 18:04:59.842939 20429 net.cpp:122] Setting up relu3
I0111 18:04:59.842948 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.842954 20429 net.cpp:137] Memory required for data: 2035766400
I0111 18:04:59.842962 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:59.842983 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:59.842996 20429 net.cpp:406] quantized_conv1 <- conv3
I0111 18:04:59.843006 20429 net.cpp:367] quantized_conv1 -> conv3 (in-place)
I0111 18:04:59.843017 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:59.843041 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.843062 20429 net.cpp:137] Memory required for data: 2087683200
I0111 18:04:59.843080 20429 layer_factory.hpp:77] Creating layer conv4
I0111 18:04:59.843124 20429 net.cpp:84] Creating Layer conv4
I0111 18:04:59.843143 20429 net.cpp:406] conv4 <- conv3
I0111 18:04:59.843180 20429 net.cpp:380] conv4 -> conv4
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.376634;  range_high=0.341829
I0111 18:04:59.867979 20429 net.cpp:122] Setting up conv4
I0111 18:04:59.868060 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.868082 20429 net.cpp:137] Memory required for data: 2139600000
I0111 18:04:59.868126 20429 layer_factory.hpp:77] Creating layer bn4
I0111 18:04:59.868180 20429 net.cpp:84] Creating Layer bn4
I0111 18:04:59.868211 20429 net.cpp:406] bn4 <- conv4
I0111 18:04:59.868237 20429 net.cpp:367] bn4 -> conv4 (in-place)
I0111 18:04:59.869040 20429 net.cpp:122] Setting up bn4
I0111 18:04:59.869069 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.869088 20429 net.cpp:137] Memory required for data: 2191516800
I0111 18:04:59.869137 20429 layer_factory.hpp:77] Creating layer scale4
I0111 18:04:59.869163 20429 net.cpp:84] Creating Layer scale4
I0111 18:04:59.869176 20429 net.cpp:406] scale4 <- conv4
I0111 18:04:59.869207 20429 net.cpp:367] scale4 -> conv4 (in-place)
I0111 18:04:59.869375 20429 layer_factory.hpp:77] Creating layer scale4
I0111 18:04:59.869843 20429 net.cpp:122] Setting up scale4
I0111 18:04:59.869869 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.869880 20429 net.cpp:137] Memory required for data: 2243433600
I0111 18:04:59.869910 20429 layer_factory.hpp:77] Creating layer relu4
I0111 18:04:59.870005 20429 net.cpp:84] Creating Layer relu4
I0111 18:04:59.870028 20429 net.cpp:406] relu4 <- conv4
I0111 18:04:59.870054 20429 net.cpp:367] relu4 -> conv4 (in-place)
I0111 18:04:59.870084 20429 net.cpp:122] Setting up relu4
I0111 18:04:59.870112 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.870126 20429 net.cpp:137] Memory required for data: 2295350400
I0111 18:04:59.870134 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:59.870146 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:59.870153 20429 net.cpp:406] quantized_conv1 <- conv4
I0111 18:04:59.870167 20429 net.cpp:367] quantized_conv1 -> conv4 (in-place)
I0111 18:04:59.870178 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:59.870188 20429 net.cpp:129] Top shape: 200 384 13 13 (12979200)
I0111 18:04:59.870195 20429 net.cpp:137] Memory required for data: 2347267200
I0111 18:04:59.870203 20429 layer_factory.hpp:77] Creating layer conv5
I0111 18:04:59.870221 20429 net.cpp:84] Creating Layer conv5
I0111 18:04:59.870229 20429 net.cpp:406] conv5 <- conv4
I0111 18:04:59.870244 20429 net.cpp:380] conv5 -> conv5
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.347658;  range_high=0.330042
I0111 18:04:59.892895 20429 net.cpp:122] Setting up conv5
I0111 18:04:59.892964 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:59.892983 20429 net.cpp:137] Memory required for data: 2381878400
I0111 18:04:59.893026 20429 layer_factory.hpp:77] Creating layer bn5
I0111 18:04:59.893069 20429 net.cpp:84] Creating Layer bn5
I0111 18:04:59.893088 20429 net.cpp:406] bn5 <- conv5
I0111 18:04:59.893126 20429 net.cpp:367] bn5 -> conv5 (in-place)
I0111 18:04:59.893892 20429 net.cpp:122] Setting up bn5
I0111 18:04:59.893925 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:59.893945 20429 net.cpp:137] Memory required for data: 2416489600
I0111 18:04:59.894039 20429 layer_factory.hpp:77] Creating layer scale5
I0111 18:04:59.894078 20429 net.cpp:84] Creating Layer scale5
I0111 18:04:59.894098 20429 net.cpp:406] scale5 <- conv5
I0111 18:04:59.894126 20429 net.cpp:367] scale5 -> conv5 (in-place)
I0111 18:04:59.894323 20429 layer_factory.hpp:77] Creating layer scale5
I0111 18:04:59.894760 20429 net.cpp:122] Setting up scale5
I0111 18:04:59.894793 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:59.894811 20429 net.cpp:137] Memory required for data: 2451100800
I0111 18:04:59.894845 20429 layer_factory.hpp:77] Creating layer relu5
I0111 18:04:59.894888 20429 net.cpp:84] Creating Layer relu5
I0111 18:04:59.894909 20429 net.cpp:406] relu5 <- conv5
I0111 18:04:59.894934 20429 net.cpp:367] relu5 -> conv5 (in-place)
I0111 18:04:59.894963 20429 net.cpp:122] Setting up relu5
I0111 18:04:59.894986 20429 net.cpp:129] Top shape: 200 256 13 13 (8652800)
I0111 18:04:59.895005 20429 net.cpp:137] Memory required for data: 2485712000
I0111 18:04:59.895026 20429 layer_factory.hpp:77] Creating layer pool5
I0111 18:04:59.895057 20429 net.cpp:84] Creating Layer pool5
I0111 18:04:59.895077 20429 net.cpp:406] pool5 <- conv5
I0111 18:04:59.895110 20429 net.cpp:380] pool5 -> pool5
I0111 18:04:59.895218 20429 net.cpp:122] Setting up pool5
I0111 18:04:59.895246 20429 net.cpp:129] Top shape: 200 256 6 6 (1843200)
I0111 18:04:59.895258 20429 net.cpp:137] Memory required for data: 2493084800
I0111 18:04:59.895273 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:04:59.895305 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:04:59.895323 20429 net.cpp:406] quantized_conv1 <- pool5
I0111 18:04:59.895350 20429 net.cpp:367] quantized_conv1 -> pool5 (in-place)
I0111 18:04:59.895381 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:04:59.895408 20429 net.cpp:129] Top shape: 200 256 6 6 (1843200)
I0111 18:04:59.895426 20429 net.cpp:137] Memory required for data: 2500457600
I0111 18:04:59.895447 20429 layer_factory.hpp:77] Creating layer fc6
I0111 18:04:59.895484 20429 net.cpp:84] Creating Layer fc6
I0111 18:04:59.895504 20429 net.cpp:406] fc6 <- pool5
I0111 18:04:59.895534 20429 net.cpp:380] fc6 -> fc6
I0111 18:04:59.895623 20429 quan_inner_product_layer.cpp:21] 4096   4096
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.160061;  range_high=0.148064
I0111 18:05:01.039456 20429 net.cpp:122] Setting up fc6
I0111 18:05:01.039499 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.039505 20429 net.cpp:137] Memory required for data: 2503734400
I0111 18:05:01.039522 20429 layer_factory.hpp:77] Creating layer bn6
I0111 18:05:01.039544 20429 net.cpp:84] Creating Layer bn6
I0111 18:05:01.039553 20429 net.cpp:406] bn6 <- fc6
I0111 18:05:01.039566 20429 net.cpp:367] bn6 -> fc6 (in-place)
I0111 18:05:01.039816 20429 net.cpp:122] Setting up bn6
I0111 18:05:01.039829 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.039834 20429 net.cpp:137] Memory required for data: 2507011200
I0111 18:05:01.039845 20429 layer_factory.hpp:77] Creating layer scale6
I0111 18:05:01.039865 20429 net.cpp:84] Creating Layer scale6
I0111 18:05:01.039873 20429 net.cpp:406] scale6 <- fc6
I0111 18:05:01.039881 20429 net.cpp:367] scale6 -> fc6 (in-place)
I0111 18:05:01.039942 20429 layer_factory.hpp:77] Creating layer scale6
I0111 18:05:01.040091 20429 net.cpp:122] Setting up scale6
I0111 18:05:01.040107 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.040118 20429 net.cpp:137] Memory required for data: 2510288000
I0111 18:05:01.040127 20429 layer_factory.hpp:77] Creating layer relu6
I0111 18:05:01.040136 20429 net.cpp:84] Creating Layer relu6
I0111 18:05:01.040143 20429 net.cpp:406] relu6 <- fc6
I0111 18:05:01.040153 20429 net.cpp:367] relu6 -> fc6 (in-place)
I0111 18:05:01.040163 20429 net.cpp:122] Setting up relu6
I0111 18:05:01.040169 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.040176 20429 net.cpp:137] Memory required for data: 2513564800
I0111 18:05:01.040181 20429 layer_factory.hpp:77] Creating layer drop6
I0111 18:05:01.040191 20429 net.cpp:84] Creating Layer drop6
I0111 18:05:01.040197 20429 net.cpp:406] drop6 <- fc6
I0111 18:05:01.040206 20429 net.cpp:367] drop6 -> fc6 (in-place)
I0111 18:05:01.040238 20429 net.cpp:122] Setting up drop6
I0111 18:05:01.040249 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.040256 20429 net.cpp:137] Memory required for data: 2516841600
I0111 18:05:01.040263 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:05:01.040276 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:05:01.040282 20429 net.cpp:406] quantized_conv1 <- fc6
I0111 18:05:01.040290 20429 net.cpp:367] quantized_conv1 -> fc6 (in-place)
I0111 18:05:01.040302 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:05:01.040310 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.040316 20429 net.cpp:137] Memory required for data: 2520118400
I0111 18:05:01.040323 20429 layer_factory.hpp:77] Creating layer fc7
I0111 18:05:01.040338 20429 net.cpp:84] Creating Layer fc7
I0111 18:05:01.040351 20429 net.cpp:406] fc7 <- fc6
I0111 18:05:01.040361 20429 net.cpp:380] fc7 -> fc7
I0111 18:05:01.040374 20429 quan_inner_product_layer.cpp:21] 4096   4096
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.162636;  range_high=0.197684
I0111 18:05:01.592205 20429 net.cpp:122] Setting up fc7
I0111 18:05:01.592321 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.592342 20429 net.cpp:137] Memory required for data: 2523395200
I0111 18:05:01.592401 20429 layer_factory.hpp:77] Creating layer bn7
I0111 18:05:01.592454 20429 net.cpp:84] Creating Layer bn7
I0111 18:05:01.592489 20429 net.cpp:406] bn7 <- fc7
I0111 18:05:01.592527 20429 net.cpp:367] bn7 -> fc7 (in-place)
I0111 18:05:01.593405 20429 net.cpp:122] Setting up bn7
I0111 18:05:01.593437 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.593451 20429 net.cpp:137] Memory required for data: 2526672000
I0111 18:05:01.593493 20429 layer_factory.hpp:77] Creating layer scale7
I0111 18:05:01.593528 20429 net.cpp:84] Creating Layer scale7
I0111 18:05:01.593549 20429 net.cpp:406] scale7 <- fc7
I0111 18:05:01.593585 20429 net.cpp:367] scale7 -> fc7 (in-place)
I0111 18:05:01.593747 20429 layer_factory.hpp:77] Creating layer scale7
I0111 18:05:01.594252 20429 net.cpp:122] Setting up scale7
I0111 18:05:01.594281 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.594377 20429 net.cpp:137] Memory required for data: 2529948800
I0111 18:05:01.594413 20429 layer_factory.hpp:77] Creating layer relu7
I0111 18:05:01.594449 20429 net.cpp:84] Creating Layer relu7
I0111 18:05:01.594470 20429 net.cpp:406] relu7 <- fc7
I0111 18:05:01.594499 20429 net.cpp:367] relu7 -> fc7 (in-place)
I0111 18:05:01.594529 20429 net.cpp:122] Setting up relu7
I0111 18:05:01.594555 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.594573 20429 net.cpp:137] Memory required for data: 2533225600
I0111 18:05:01.594588 20429 layer_factory.hpp:77] Creating layer drop7
I0111 18:05:01.594616 20429 net.cpp:84] Creating Layer drop7
I0111 18:05:01.594636 20429 net.cpp:406] drop7 <- fc7
I0111 18:05:01.594667 20429 net.cpp:367] drop7 -> fc7 (in-place)
I0111 18:05:01.594772 20429 net.cpp:122] Setting up drop7
I0111 18:05:01.594808 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.594825 20429 net.cpp:137] Memory required for data: 2536502400
I0111 18:05:01.594846 20429 layer_factory.hpp:77] Creating layer quantized_conv1
I0111 18:05:01.594880 20429 net.cpp:84] Creating Layer quantized_conv1
I0111 18:05:01.594900 20429 net.cpp:406] quantized_conv1 <- fc7
I0111 18:05:01.594926 20429 net.cpp:367] quantized_conv1 -> fc7 (in-place)
I0111 18:05:01.594959 20429 net.cpp:122] Setting up quantized_conv1
I0111 18:05:01.594985 20429 net.cpp:129] Top shape: 200 4096 (819200)
I0111 18:05:01.595003 20429 net.cpp:137] Memory required for data: 2539779200
I0111 18:05:01.595023 20429 layer_factory.hpp:77] Creating layer fc8
I0111 18:05:01.595068 20429 net.cpp:84] Creating Layer fc8
I0111 18:05:01.595088 20429 net.cpp:406] fc8 <- fc7
I0111 18:05:01.595118 20429 net.cpp:380] fc8 -> fc8
I0111 18:05:01.595155 20429 quan_inner_product_layer.cpp:21] 1000   1000
ydwu=======get:
bit_width=8;  round_method=0;  round_strategy=1;  is_runtime=0;  range_low=-0.141857;  range_high=0.274249
I0111 18:05:01.697742 20429 net.cpp:122] Setting up fc8
I0111 18:05:01.697830 20429 net.cpp:129] Top shape: 200 1000 (200000)
I0111 18:05:01.697854 20429 net.cpp:137] Memory required for data: 2540579200
I0111 18:05:01.697902 20429 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0111 18:05:01.697945 20429 net.cpp:84] Creating Layer fc8_fc8_0_split
I0111 18:05:01.697966 20429 net.cpp:406] fc8_fc8_0_split <- fc8
I0111 18:05:01.698011 20429 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0111 18:05:01.698086 20429 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0111 18:05:01.698123 20429 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_2
I0111 18:05:01.698335 20429 net.cpp:122] Setting up fc8_fc8_0_split
I0111 18:05:01.698361 20429 net.cpp:129] Top shape: 200 1000 (200000)
I0111 18:05:01.698386 20429 net.cpp:129] Top shape: 200 1000 (200000)
I0111 18:05:01.698412 20429 net.cpp:129] Top shape: 200 1000 (200000)
I0111 18:05:01.698431 20429 net.cpp:137] Memory required for data: 2542979200
I0111 18:05:01.698453 20429 layer_factory.hpp:77] Creating layer accuracy
I0111 18:05:01.698498 20429 net.cpp:84] Creating Layer accuracy
I0111 18:05:01.698520 20429 net.cpp:406] accuracy <- fc8_fc8_0_split_0
I0111 18:05:01.698547 20429 net.cpp:406] accuracy <- label_data_1_split_0
I0111 18:05:01.698580 20429 net.cpp:380] accuracy -> accuracy
I0111 18:05:01.698626 20429 net.cpp:122] Setting up accuracy
I0111 18:05:01.698652 20429 net.cpp:129] Top shape: (1)
I0111 18:05:01.698674 20429 net.cpp:137] Memory required for data: 2542979204
I0111 18:05:01.698695 20429 layer_factory.hpp:77] Creating layer accuracy_5
I0111 18:05:01.698732 20429 net.cpp:84] Creating Layer accuracy_5
I0111 18:05:01.698753 20429 net.cpp:406] accuracy_5 <- fc8_fc8_0_split_1
I0111 18:05:01.698779 20429 net.cpp:406] accuracy_5 <- label_data_1_split_1
I0111 18:05:01.698810 20429 net.cpp:380] accuracy_5 -> accuracy_5
I0111 18:05:01.698851 20429 net.cpp:122] Setting up accuracy_5
I0111 18:05:01.698878 20429 net.cpp:129] Top shape: (1)
I0111 18:05:01.698897 20429 net.cpp:137] Memory required for data: 2542979208
I0111 18:05:01.698918 20429 layer_factory.hpp:77] Creating layer loss
I0111 18:05:01.698952 20429 net.cpp:84] Creating Layer loss
I0111 18:05:01.698973 20429 net.cpp:406] loss <- fc8_fc8_0_split_2
I0111 18:05:01.699059 20429 net.cpp:406] loss <- label_data_1_split_2
I0111 18:05:01.699082 20429 net.cpp:380] loss -> loss
I0111 18:05:01.699118 20429 layer_factory.hpp:77] Creating layer loss
I0111 18:05:01.700042 20429 net.cpp:122] Setting up loss
I0111 18:05:01.700076 20429 net.cpp:129] Top shape: (1)
I0111 18:05:01.700096 20429 net.cpp:132]     with loss weight 1
I0111 18:05:01.700124 20429 net.cpp:137] Memory required for data: 2542979212
I0111 18:05:01.700147 20429 net.cpp:198] loss needs backward computation.
I0111 18:05:01.700175 20429 net.cpp:200] accuracy_5 does not need backward computation.
I0111 18:05:01.700198 20429 net.cpp:200] accuracy does not need backward computation.
I0111 18:05:01.700222 20429 net.cpp:198] fc8_fc8_0_split needs backward computation.
I0111 18:05:01.700244 20429 net.cpp:198] fc8 needs backward computation.
I0111 18:05:01.700265 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:05:01.700287 20429 net.cpp:198] drop7 needs backward computation.
I0111 18:05:01.700306 20429 net.cpp:198] relu7 needs backward computation.
I0111 18:05:01.700327 20429 net.cpp:198] scale7 needs backward computation.
I0111 18:05:01.700348 20429 net.cpp:198] bn7 needs backward computation.
I0111 18:05:01.700369 20429 net.cpp:198] fc7 needs backward computation.
I0111 18:05:01.700392 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:05:01.700410 20429 net.cpp:198] drop6 needs backward computation.
I0111 18:05:01.700430 20429 net.cpp:198] relu6 needs backward computation.
I0111 18:05:01.700451 20429 net.cpp:198] scale6 needs backward computation.
I0111 18:05:01.700470 20429 net.cpp:198] bn6 needs backward computation.
I0111 18:05:01.700489 20429 net.cpp:198] fc6 needs backward computation.
I0111 18:05:01.700510 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:05:01.700531 20429 net.cpp:198] pool5 needs backward computation.
I0111 18:05:01.700556 20429 net.cpp:198] relu5 needs backward computation.
I0111 18:05:01.700577 20429 net.cpp:198] scale5 needs backward computation.
I0111 18:05:01.700592 20429 net.cpp:198] bn5 needs backward computation.
I0111 18:05:01.700610 20429 net.cpp:198] conv5 needs backward computation.
I0111 18:05:01.700631 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:05:01.700650 20429 net.cpp:198] relu4 needs backward computation.
I0111 18:05:01.700670 20429 net.cpp:198] scale4 needs backward computation.
I0111 18:05:01.700690 20429 net.cpp:198] bn4 needs backward computation.
I0111 18:05:01.700711 20429 net.cpp:198] conv4 needs backward computation.
I0111 18:05:01.700732 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:05:01.700753 20429 net.cpp:198] relu3 needs backward computation.
I0111 18:05:01.700773 20429 net.cpp:198] scale3 needs backward computation.
I0111 18:05:01.700794 20429 net.cpp:198] bn3 needs backward computation.
I0111 18:05:01.700814 20429 net.cpp:198] conv3 needs backward computation.
I0111 18:05:01.700836 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:05:01.700856 20429 net.cpp:198] pool2 needs backward computation.
I0111 18:05:01.700879 20429 net.cpp:198] relu2 needs backward computation.
I0111 18:05:01.700898 20429 net.cpp:198] scale2 needs backward computation.
I0111 18:05:01.700917 20429 net.cpp:198] bn2 needs backward computation.
I0111 18:05:01.700937 20429 net.cpp:198] conv2 needs backward computation.
I0111 18:05:01.700960 20429 net.cpp:198] quantized_conv1 needs backward computation.
I0111 18:05:01.700980 20429 net.cpp:198] pool1 needs backward computation.
I0111 18:05:01.701001 20429 net.cpp:198] relu1 needs backward computation.
I0111 18:05:01.701020 20429 net.cpp:198] scale1 needs backward computation.
I0111 18:05:01.701040 20429 net.cpp:198] bn1 needs backward computation.
I0111 18:05:01.701061 20429 net.cpp:198] conv1 needs backward computation.
I0111 18:05:01.701084 20429 net.cpp:200] label_data_1_split does not need backward computation.
I0111 18:05:01.701107 20429 net.cpp:200] data does not need backward computation.
I0111 18:05:01.701154 20429 net.cpp:242] This network produces output accuracy
I0111 18:05:01.701179 20429 net.cpp:242] This network produces output accuracy_5
I0111 18:05:01.701200 20429 net.cpp:242] This network produces output loss
I0111 18:05:01.701297 20429 net.cpp:255] Network initialization done.
I0111 18:05:01.701771 20429 solver.cpp:56] Solver scaffolding done.
I0111 18:05:01.710054 20429 caffe.cpp:155] Finetuning from alexnet_origine.caffemodel
I0111 18:05:05.112716 20429 caffe.cpp:248] Starting Optimization
I0111 18:05:05.112784 20429 solver.cpp:273] Solving AlexNet-BN
I0111 18:05:05.112790 20429 solver.cpp:274] Learning Rate Policy: multistep
I0111 18:05:05.116951 20429 solver.cpp:331] Iteration 0, Testing net (#0)
I0111 18:05:05.155135 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0111 18:14:28.815778 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0111 18:14:37.445605 20429 solver.cpp:400]     Test net output #0: accuracy = 0.33116
I0111 18:14:37.445668 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.54112
I0111 18:14:37.445691 20429 solver.cpp:400]     Test net output #2: loss = 3.58722 (* 1 = 3.58722 loss)
I0111 18:14:39.889438 20429 solver.cpp:218] Iteration 0 (0 iter/s, 574.749s/100 iters), loss = 2.321
I0111 18:14:39.889919 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.71
I0111 18:14:39.890054 20429 solver.cpp:238]     Train net output #1: loss = 2.321 (* 1 = 2.321 loss)
I0111 18:14:39.890122 20429 sgd_solver.cpp:105] Iteration 0, lr = 1e-07
I0111 18:19:05.234783 20429 solver.cpp:218] Iteration 100 (0.376886 iter/s, 265.332s/100 iters), loss = 2.4107
I0111 18:19:05.235164 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.71
I0111 18:19:05.235218 20429 solver.cpp:238]     Train net output #1: loss = 2.4107 (* 1 = 2.4107 loss)
I0111 18:19:05.235230 20429 sgd_solver.cpp:105] Iteration 100, lr = 1e-07
I0111 18:23:24.886262 20429 solver.cpp:218] Iteration 200 (0.38515 iter/s, 259.639s/100 iters), loss = 2.21657
I0111 18:23:24.886739 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.75
I0111 18:23:24.886767 20429 solver.cpp:238]     Train net output #1: loss = 2.21657 (* 1 = 2.21657 loss)
I0111 18:23:24.886795 20429 sgd_solver.cpp:105] Iteration 200, lr = 1e-07
I0111 18:27:53.285950 20429 solver.cpp:218] Iteration 300 (0.372597 iter/s, 268.387s/100 iters), loss = 2.39553
I0111 18:27:53.286319 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0111 18:27:53.286348 20429 solver.cpp:238]     Train net output #1: loss = 2.39553 (* 1 = 2.39553 loss)
I0111 18:27:53.286370 20429 sgd_solver.cpp:105] Iteration 300, lr = 1e-07
I0111 18:32:28.955004 20429 solver.cpp:218] Iteration 400 (0.362771 iter/s, 275.656s/100 iters), loss = 2.14592
I0111 18:32:28.955375 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.75
I0111 18:32:28.955435 20429 solver.cpp:238]     Train net output #1: loss = 2.14592 (* 1 = 2.14592 loss)
I0111 18:32:28.955447 20429 sgd_solver.cpp:105] Iteration 400, lr = 1e-07
I0111 18:36:53.802459 20429 solver.cpp:218] Iteration 500 (0.377594 iter/s, 264.835s/100 iters), loss = 2.42122
I0111 18:36:53.802811 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.72
I0111 18:36:53.802875 20429 solver.cpp:238]     Train net output #1: loss = 2.42122 (* 1 = 2.42122 loss)
I0111 18:36:53.802888 20429 sgd_solver.cpp:105] Iteration 500, lr = 1e-07
I0111 18:41:31.182687 20429 solver.cpp:218] Iteration 600 (0.360533 iter/s, 277.367s/100 iters), loss = 2.18288
I0111 18:41:31.183095 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0111 18:41:31.183154 20429 solver.cpp:238]     Train net output #1: loss = 2.18288 (* 1 = 2.18288 loss)
I0111 18:41:31.183169 20429 sgd_solver.cpp:105] Iteration 600, lr = 1e-07
I0111 18:45:57.257359 20429 solver.cpp:218] Iteration 700 (0.375847 iter/s, 266.066s/100 iters), loss = 2.20993
I0111 18:45:57.257846 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0111 18:45:57.257882 20429 solver.cpp:238]     Train net output #1: loss = 2.20993 (* 1 = 2.20993 loss)
I0111 18:45:57.257895 20429 sgd_solver.cpp:105] Iteration 700, lr = 1e-07
I0111 18:50:34.999868 20429 solver.cpp:218] Iteration 800 (0.360059 iter/s, 277.732s/100 iters), loss = 2.42816
I0111 18:50:35.018447 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0111 18:50:35.018496 20429 solver.cpp:238]     Train net output #1: loss = 2.42816 (* 1 = 2.42816 loss)
I0111 18:50:35.018508 20429 sgd_solver.cpp:105] Iteration 800, lr = 1e-07
I0111 18:55:03.163923 20429 solver.cpp:218] Iteration 900 (0.372946 iter/s, 268.135s/100 iters), loss = 2.19733
I0111 18:55:03.164326 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.735
I0111 18:55:03.164356 20429 solver.cpp:238]     Train net output #1: loss = 2.19733 (* 1 = 2.19733 loss)
I0111 18:55:03.164377 20429 sgd_solver.cpp:105] Iteration 900, lr = 1e-07
I0111 18:59:22.871754 20429 solver.cpp:331] Iteration 1000, Testing net (#0)
I0111 18:59:22.872099 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0111 19:08:34.308929 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0111 19:08:42.622022 20429 solver.cpp:400]     Test net output #0: accuracy = 0.45824
I0111 19:08:42.622108 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.6964
I0111 19:08:42.622128 20429 solver.cpp:400]     Test net output #2: loss = 2.50957 (* 1 = 2.50957 loss)
I0111 19:08:45.332777 20429 solver.cpp:218] Iteration 1000 (0.121635 iter/s, 822.135s/100 iters), loss = 2.14784
I0111 19:08:45.332870 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.74
I0111 19:08:45.332904 20429 solver.cpp:238]     Train net output #1: loss = 2.14784 (* 1 = 2.14784 loss)
I0111 19:08:45.332918 20429 sgd_solver.cpp:105] Iteration 1000, lr = 1e-07
I0111 19:13:16.527242 20429 solver.cpp:218] Iteration 1100 (0.368755 iter/s, 271.183s/100 iters), loss = 2.19374
I0111 19:13:16.527572 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.735
I0111 19:13:16.527601 20429 solver.cpp:238]     Train net output #1: loss = 2.19374 (* 1 = 2.19374 loss)
I0111 19:13:16.527614 20429 sgd_solver.cpp:105] Iteration 1100, lr = 1e-07
I0111 19:17:38.085844 20429 solver.cpp:218] Iteration 1200 (0.38234 iter/s, 261.547s/100 iters), loss = 2.34555
I0111 19:17:38.086200 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0111 19:17:38.086251 20429 solver.cpp:238]     Train net output #1: loss = 2.34555 (* 1 = 2.34555 loss)
I0111 19:17:38.086264 20429 sgd_solver.cpp:105] Iteration 1200, lr = 1e-07
I0111 19:21:55.006990 20429 solver.cpp:218] Iteration 1300 (0.389241 iter/s, 256.91s/100 iters), loss = 2.21812
I0111 19:21:55.007288 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.75
I0111 19:21:55.007326 20429 solver.cpp:238]     Train net output #1: loss = 2.21812 (* 1 = 2.21812 loss)
I0111 19:21:55.007339 20429 sgd_solver.cpp:105] Iteration 1300, lr = 1e-07
I0111 19:26:19.923671 20429 solver.cpp:218] Iteration 1400 (0.377494 iter/s, 264.905s/100 iters), loss = 2.46466
I0111 19:26:19.924078 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.665
I0111 19:26:19.924127 20429 solver.cpp:238]     Train net output #1: loss = 2.46466 (* 1 = 2.46466 loss)
I0111 19:26:19.924140 20429 sgd_solver.cpp:105] Iteration 1400, lr = 1e-07
I0111 19:30:44.060850 20429 solver.cpp:218] Iteration 1500 (0.378608 iter/s, 264.126s/100 iters), loss = 2.29797
I0111 19:30:44.061218 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.705
I0111 19:30:44.061277 20429 solver.cpp:238]     Train net output #1: loss = 2.29797 (* 1 = 2.29797 loss)
I0111 19:30:44.061296 20429 sgd_solver.cpp:105] Iteration 1500, lr = 1e-07
I0111 19:35:09.511055 20429 solver.cpp:218] Iteration 1600 (0.376735 iter/s, 265.438s/100 iters), loss = 2.33549
I0111 19:35:09.511484 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0111 19:35:09.511556 20429 solver.cpp:238]     Train net output #1: loss = 2.33549 (* 1 = 2.33549 loss)
I0111 19:35:09.511572 20429 sgd_solver.cpp:105] Iteration 1600, lr = 1e-07
I0111 19:39:36.635637 20429 solver.cpp:218] Iteration 1700 (0.374374 iter/s, 267.113s/100 iters), loss = 2.25922
I0111 19:39:36.636234 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0111 19:39:36.636263 20429 solver.cpp:238]     Train net output #1: loss = 2.25922 (* 1 = 2.25922 loss)
I0111 19:39:36.636276 20429 sgd_solver.cpp:105] Iteration 1700, lr = 1e-07
I0111 19:44:16.298298 20429 solver.cpp:218] Iteration 1800 (0.35759 iter/s, 279.65s/100 iters), loss = 2.36368
I0111 19:44:16.313356 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.75
I0111 19:44:16.313398 20429 solver.cpp:238]     Train net output #1: loss = 2.36368 (* 1 = 2.36368 loss)
I0111 19:44:16.313412 20429 sgd_solver.cpp:105] Iteration 1800, lr = 1e-07
I0111 19:48:39.218945 20429 solver.cpp:218] Iteration 1900 (0.380381 iter/s, 262.894s/100 iters), loss = 2.08385
I0111 19:48:39.219319 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.775
I0111 19:48:39.219347 20429 solver.cpp:238]     Train net output #1: loss = 2.08385 (* 1 = 2.08385 loss)
I0111 19:48:39.219362 20429 sgd_solver.cpp:105] Iteration 1900, lr = 1e-07
I0111 19:53:12.591653 20429 solver.cpp:450] Snapshotting to binary proto file ../other_model/alexnet_a4w8_iter_2000.caffemodel
I0111 19:53:20.322767 20429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../other_model/alexnet_a4w8_iter_2000.solverstate
I0111 19:53:25.818042 20429 solver.cpp:331] Iteration 2000, Testing net (#0)
I0111 19:53:25.818130 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0111 20:02:58.539686 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0111 20:03:08.375305 20429 solver.cpp:400]     Test net output #0: accuracy = 0.47876
I0111 20:03:08.375370 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.72266
I0111 20:03:08.375385 20429 solver.cpp:400]     Test net output #2: loss = 2.35366 (* 1 = 2.35366 loss)
I0111 20:03:10.906827 20429 solver.cpp:218] Iteration 2000 (0.114725 iter/s, 871.648s/100 iters), loss = 2.53398
I0111 20:03:10.906927 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.695
I0111 20:03:10.906949 20429 solver.cpp:238]     Train net output #1: loss = 2.53398 (* 1 = 2.53398 loss)
I0111 20:03:10.906963 20429 sgd_solver.cpp:105] Iteration 2000, lr = 1e-07
I0111 20:07:40.251206 20429 solver.cpp:218] Iteration 2100 (0.371288 iter/s, 269.332s/100 iters), loss = 2.39254
I0111 20:07:40.251624 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.67
I0111 20:07:40.251659 20429 solver.cpp:238]     Train net output #1: loss = 2.39254 (* 1 = 2.39254 loss)
I0111 20:07:40.251672 20429 sgd_solver.cpp:105] Iteration 2100, lr = 1e-07
I0111 20:12:16.512691 20429 solver.cpp:218] Iteration 2200 (0.361992 iter/s, 276.249s/100 iters), loss = 2.37754
I0111 20:12:16.512914 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0111 20:12:16.512948 20429 solver.cpp:238]     Train net output #1: loss = 2.37754 (* 1 = 2.37754 loss)
I0111 20:12:16.512960 20429 sgd_solver.cpp:105] Iteration 2200, lr = 1e-07
I0111 20:16:37.490257 20429 solver.cpp:218] Iteration 2300 (0.383192 iter/s, 260.966s/100 iters), loss = 2.15504
I0111 20:16:37.490609 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.78
I0111 20:16:37.490659 20429 solver.cpp:238]     Train net output #1: loss = 2.15504 (* 1 = 2.15504 loss)
I0111 20:16:37.490672 20429 sgd_solver.cpp:105] Iteration 2300, lr = 1e-07
I0111 20:21:17.485649 20429 solver.cpp:218] Iteration 2400 (0.357165 iter/s, 279.983s/100 iters), loss = 2.41548
I0111 20:21:17.486184 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.685
I0111 20:21:17.486222 20429 solver.cpp:238]     Train net output #1: loss = 2.41548 (* 1 = 2.41548 loss)
I0111 20:21:17.486233 20429 sgd_solver.cpp:105] Iteration 2400, lr = 1e-07
I0111 20:25:48.567612 20429 solver.cpp:218] Iteration 2500 (0.36891 iter/s, 271.069s/100 iters), loss = 2.31703
I0111 20:25:48.568092 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.745
I0111 20:25:48.568121 20429 solver.cpp:238]     Train net output #1: loss = 2.31703 (* 1 = 2.31703 loss)
I0111 20:25:48.568135 20429 sgd_solver.cpp:105] Iteration 2500, lr = 1e-07
I0111 20:30:25.454030 20429 solver.cpp:218] Iteration 2600 (0.361178 iter/s, 276.872s/100 iters), loss = 2.16465
I0111 20:30:25.477327 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.745
I0111 20:30:25.477388 20429 solver.cpp:238]     Train net output #1: loss = 2.16465 (* 1 = 2.16465 loss)
I0111 20:30:25.477406 20429 sgd_solver.cpp:105] Iteration 2600, lr = 1e-07
I0111 20:35:02.495170 20429 solver.cpp:218] Iteration 2700 (0.361005 iter/s, 277.004s/100 iters), loss = 2.0909
I0111 20:35:02.495616 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.77
I0111 20:35:02.495668 20429 solver.cpp:238]     Train net output #1: loss = 2.0909 (* 1 = 2.0909 loss)
I0111 20:35:02.495679 20429 sgd_solver.cpp:105] Iteration 2700, lr = 1e-07
I0111 20:39:45.834776 20429 solver.cpp:218] Iteration 2800 (0.352951 iter/s, 283.326s/100 iters), loss = 2.23171
I0111 20:39:45.835197 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.76
I0111 20:39:45.835237 20429 solver.cpp:238]     Train net output #1: loss = 2.23171 (* 1 = 2.23171 loss)
I0111 20:39:45.835249 20429 sgd_solver.cpp:105] Iteration 2800, lr = 1e-07
I0111 20:44:38.059201 20429 solver.cpp:218] Iteration 2900 (0.342219 iter/s, 292.21s/100 iters), loss = 2.25293
I0111 20:44:38.059538 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.735
I0111 20:44:38.059589 20429 solver.cpp:238]     Train net output #1: loss = 2.25293 (* 1 = 2.25293 loss)
I0111 20:44:38.059602 20429 sgd_solver.cpp:105] Iteration 2900, lr = 1e-07
I0111 20:49:02.915184 20429 solver.cpp:331] Iteration 3000, Testing net (#0)
I0111 20:49:02.915494 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0111 20:58:14.215909 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0111 20:58:23.306789 20429 solver.cpp:400]     Test net output #0: accuracy = 0.482
I0111 20:58:23.306880 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.72366
I0111 20:58:23.306907 20429 solver.cpp:400]     Test net output #2: loss = 2.33565 (* 1 = 2.33565 loss)
I0111 20:58:25.824169 20429 solver.cpp:218] Iteration 3000 (0.120813 iter/s, 827.727s/100 iters), loss = 2.14303
I0111 20:58:25.824266 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.76
I0111 20:58:25.824288 20429 solver.cpp:238]     Train net output #1: loss = 2.14303 (* 1 = 2.14303 loss)
I0111 20:58:25.824301 20429 sgd_solver.cpp:105] Iteration 3000, lr = 1e-07
I0111 21:02:55.063408 20429 solver.cpp:218] Iteration 3100 (0.371433 iter/s, 269.227s/100 iters), loss = 2.52335
I0111 21:02:55.063750 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.72
I0111 21:02:55.063782 20429 solver.cpp:238]     Train net output #1: loss = 2.52335 (* 1 = 2.52335 loss)
I0111 21:02:55.063793 20429 sgd_solver.cpp:105] Iteration 3100, lr = 1e-07
I0111 21:07:36.764135 20429 solver.cpp:218] Iteration 3200 (0.355003 iter/s, 281.688s/100 iters), loss = 2.42755
I0111 21:07:36.764513 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.75
I0111 21:07:36.764554 20429 solver.cpp:238]     Train net output #1: loss = 2.42755 (* 1 = 2.42755 loss)
I0111 21:07:36.764570 20429 sgd_solver.cpp:105] Iteration 3200, lr = 1e-07
I0111 21:12:10.608883 20429 solver.cpp:218] Iteration 3300 (0.365187 iter/s, 273.832s/100 iters), loss = 2.33866
I0111 21:12:10.609233 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.71
I0111 21:12:10.609292 20429 solver.cpp:238]     Train net output #1: loss = 2.33866 (* 1 = 2.33866 loss)
I0111 21:12:10.609304 20429 sgd_solver.cpp:105] Iteration 3300, lr = 1e-07
I0111 21:16:41.409617 20429 solver.cpp:218] Iteration 3400 (0.369292 iter/s, 270.788s/100 iters), loss = 2.43515
I0111 21:16:41.410149 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0111 21:16:41.410214 20429 solver.cpp:238]     Train net output #1: loss = 2.43515 (* 1 = 2.43515 loss)
I0111 21:16:41.410228 20429 sgd_solver.cpp:105] Iteration 3400, lr = 1e-07
I0111 21:21:20.308776 20429 solver.cpp:218] Iteration 3500 (0.358569 iter/s, 278.886s/100 iters), loss = 2.48567
I0111 21:21:20.327335 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.68
I0111 21:21:20.327386 20429 solver.cpp:238]     Train net output #1: loss = 2.48567 (* 1 = 2.48567 loss)
I0111 21:21:20.327399 20429 sgd_solver.cpp:105] Iteration 3500, lr = 1e-07
I0111 21:26:28.158234 20429 solver.cpp:218] Iteration 3600 (0.324868 iter/s, 307.817s/100 iters), loss = 2.37463
I0111 21:26:28.158818 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0111 21:26:28.158875 20429 solver.cpp:238]     Train net output #1: loss = 2.37463 (* 1 = 2.37463 loss)
I0111 21:26:28.158888 20429 sgd_solver.cpp:105] Iteration 3600, lr = 1e-07
I0111 21:31:16.313786 20429 solver.cpp:218] Iteration 3700 (0.347051 iter/s, 288.142s/100 iters), loss = 2.13489
I0111 21:31:16.314303 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.76
I0111 21:31:16.314355 20429 solver.cpp:238]     Train net output #1: loss = 2.13489 (* 1 = 2.13489 loss)
I0111 21:31:16.314368 20429 sgd_solver.cpp:105] Iteration 3700, lr = 1e-07
I0111 21:35:54.159935 20429 solver.cpp:218] Iteration 3800 (0.359928 iter/s, 277.834s/100 iters), loss = 2.37986
I0111 21:35:54.160274 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0111 21:35:54.160326 20429 solver.cpp:238]     Train net output #1: loss = 2.37986 (* 1 = 2.37986 loss)
I0111 21:35:54.160339 20429 sgd_solver.cpp:105] Iteration 3800, lr = 1e-07
I0111 21:40:41.753063 20429 solver.cpp:218] Iteration 3900 (0.347729 iter/s, 287.58s/100 iters), loss = 2.30661
I0111 21:40:41.753439 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.74
I0111 21:40:41.753495 20429 solver.cpp:238]     Train net output #1: loss = 2.30661 (* 1 = 2.30661 loss)
I0111 21:40:41.753507 20429 sgd_solver.cpp:105] Iteration 3900, lr = 1e-07
I0111 21:45:18.248733 20429 solver.cpp:450] Snapshotting to binary proto file ../other_model/alexnet_a4w8_iter_4000.caffemodel
I0111 21:45:25.207770 20429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../other_model/alexnet_a4w8_iter_4000.solverstate
I0111 21:45:30.662201 20429 solver.cpp:331] Iteration 4000, Testing net (#0)
I0111 21:45:30.662312 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0111 21:55:05.508330 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0111 21:55:14.526957 20429 solver.cpp:400]     Test net output #0: accuracy = 0.48204
I0111 21:55:14.527041 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.72366
I0111 21:55:14.527060 20429 solver.cpp:400]     Test net output #2: loss = 2.33486 (* 1 = 2.33486 loss)
I0111 21:55:17.047049 20429 solver.cpp:218] Iteration 4000 (0.114252 iter/s, 875.256s/100 iters), loss = 2.15879
I0111 21:55:17.047149 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.75
I0111 21:55:17.047169 20429 solver.cpp:238]     Train net output #1: loss = 2.15879 (* 1 = 2.15879 loss)
I0111 21:55:17.047183 20429 sgd_solver.cpp:105] Iteration 4000, lr = 1e-07
I0111 21:59:52.226974 20429 solver.cpp:218] Iteration 4100 (0.363415 iter/s, 275.168s/100 iters), loss = 2.09154
I0111 21:59:52.227263 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.765
I0111 21:59:52.227326 20429 solver.cpp:238]     Train net output #1: loss = 2.09154 (* 1 = 2.09154 loss)
I0111 21:59:52.227339 20429 sgd_solver.cpp:105] Iteration 4100, lr = 1e-07
I0111 22:04:43.104996 20429 solver.cpp:218] Iteration 4200 (0.343802 iter/s, 290.865s/100 iters), loss = 2.67177
I0111 22:04:43.105350 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.655
I0111 22:04:43.105401 20429 solver.cpp:238]     Train net output #1: loss = 2.67177 (* 1 = 2.67177 loss)
I0111 22:04:43.105414 20429 sgd_solver.cpp:105] Iteration 4200, lr = 1e-07
I0111 22:09:26.853180 20429 solver.cpp:218] Iteration 4300 (0.352442 iter/s, 283.735s/100 iters), loss = 2.22511
I0111 22:09:26.853768 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.76
I0111 22:09:26.853847 20429 solver.cpp:238]     Train net output #1: loss = 2.22511 (* 1 = 2.22511 loss)
I0111 22:09:26.853866 20429 sgd_solver.cpp:105] Iteration 4300, lr = 1e-07
I0111 22:13:56.872089 20429 solver.cpp:218] Iteration 4400 (0.370363 iter/s, 270.006s/100 iters), loss = 2.35219
I0111 22:13:56.872459 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0111 22:13:56.872511 20429 solver.cpp:238]     Train net output #1: loss = 2.35219 (* 1 = 2.35219 loss)
I0111 22:13:56.872524 20429 sgd_solver.cpp:105] Iteration 4400, lr = 1e-07
I0111 22:18:51.674335 20429 solver.cpp:218] Iteration 4500 (0.339226 iter/s, 294.788s/100 iters), loss = 2.16847
I0111 22:18:51.674789 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.775
I0111 22:18:51.674836 20429 solver.cpp:238]     Train net output #1: loss = 2.16847 (* 1 = 2.16847 loss)
I0111 22:18:51.674852 20429 sgd_solver.cpp:105] Iteration 4500, lr = 1e-07
I0111 22:23:34.204942 20429 solver.cpp:218] Iteration 4600 (0.353961 iter/s, 282.517s/100 iters), loss = 2.24051
I0111 22:23:34.205279 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0111 22:23:34.205332 20429 solver.cpp:238]     Train net output #1: loss = 2.24051 (* 1 = 2.24051 loss)
I0111 22:23:34.205345 20429 sgd_solver.cpp:105] Iteration 4600, lr = 1e-07
I0111 22:28:27.069052 20429 solver.cpp:218] Iteration 4700 (0.341471 iter/s, 292.851s/100 iters), loss = 2.51685
I0111 22:28:27.069386 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.66
I0111 22:28:27.069416 20429 solver.cpp:238]     Train net output #1: loss = 2.51685 (* 1 = 2.51685 loss)
I0111 22:28:27.069427 20429 sgd_solver.cpp:105] Iteration 4700, lr = 1e-07
I0111 22:33:10.324995 20429 solver.cpp:218] Iteration 4800 (0.353054 iter/s, 283.243s/100 iters), loss = 2.71614
I0111 22:33:10.325419 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.66
I0111 22:33:10.325469 20429 solver.cpp:238]     Train net output #1: loss = 2.71614 (* 1 = 2.71614 loss)
I0111 22:33:10.325481 20429 sgd_solver.cpp:105] Iteration 4800, lr = 1e-07
I0111 22:38:08.520334 20429 solver.cpp:218] Iteration 4900 (0.335366 iter/s, 298.182s/100 iters), loss = 2.26104
I0111 22:38:08.520685 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0111 22:38:08.520721 20429 solver.cpp:238]     Train net output #1: loss = 2.26104 (* 1 = 2.26104 loss)
I0111 22:38:08.520745 20429 sgd_solver.cpp:105] Iteration 4900, lr = 1e-07
I0111 22:42:40.949892 20429 solver.cpp:331] Iteration 5000, Testing net (#0)
I0111 22:42:40.950229 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0111 22:52:31.294525 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0111 22:52:40.454898 20429 solver.cpp:400]     Test net output #0: accuracy = 0.4791
I0111 22:52:40.454982 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.72316
I0111 22:52:40.454998 20429 solver.cpp:400]     Test net output #2: loss = 2.33955 (* 1 = 2.33955 loss)
I0111 22:52:43.149138 20429 solver.cpp:218] Iteration 5000 (0.11434 iter/s, 874.587s/100 iters), loss = 2.36088
I0111 22:52:43.149247 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.71
I0111 22:52:43.149271 20429 solver.cpp:238]     Train net output #1: loss = 2.36088 (* 1 = 2.36088 loss)
I0111 22:52:43.149296 20429 sgd_solver.cpp:105] Iteration 5000, lr = 1e-07
I0111 22:57:23.237084 20429 solver.cpp:218] Iteration 5100 (0.357047 iter/s, 280.075s/100 iters), loss = 2.26636
I0111 22:57:23.237385 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.74
I0111 22:57:23.237423 20429 solver.cpp:238]     Train net output #1: loss = 2.26636 (* 1 = 2.26636 loss)
I0111 22:57:23.237437 20429 sgd_solver.cpp:105] Iteration 5100, lr = 1e-07
I0111 23:02:15.181443 20429 solver.cpp:218] Iteration 5200 (0.342547 iter/s, 291.931s/100 iters), loss = 2.40309
I0111 23:02:15.181867 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0111 23:02:15.181922 20429 solver.cpp:238]     Train net output #1: loss = 2.40309 (* 1 = 2.40309 loss)
I0111 23:02:15.181937 20429 sgd_solver.cpp:105] Iteration 5200, lr = 1e-07
I0111 23:07:11.847951 20429 solver.cpp:218] Iteration 5300 (0.337095 iter/s, 296.653s/100 iters), loss = 2.48124
I0111 23:07:11.864853 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.705
I0111 23:07:11.864903 20429 solver.cpp:238]     Train net output #1: loss = 2.48124 (* 1 = 2.48124 loss)
I0111 23:07:11.864917 20429 sgd_solver.cpp:105] Iteration 5300, lr = 1e-07
I0111 23:12:02.831714 20429 solver.cpp:218] Iteration 5400 (0.343697 iter/s, 290.954s/100 iters), loss = 2.43757
I0111 23:12:02.832094 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.72
I0111 23:12:02.832132 20429 solver.cpp:238]     Train net output #1: loss = 2.43757 (* 1 = 2.43757 loss)
I0111 23:12:02.832145 20429 sgd_solver.cpp:105] Iteration 5400, lr = 1e-07
I0111 23:16:51.554023 20429 solver.cpp:218] Iteration 5500 (0.34637 iter/s, 288.709s/100 iters), loss = 2.47646
I0111 23:16:51.554391 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.705
I0111 23:16:51.554420 20429 solver.cpp:238]     Train net output #1: loss = 2.47646 (* 1 = 2.47646 loss)
I0111 23:16:51.554432 20429 sgd_solver.cpp:105] Iteration 5500, lr = 1e-07
I0111 23:21:55.099544 20429 solver.cpp:218] Iteration 5600 (0.329455 iter/s, 303.531s/100 iters), loss = 1.81938
I0111 23:21:55.099897 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.825
I0111 23:21:55.099936 20429 solver.cpp:238]     Train net output #1: loss = 1.81938 (* 1 = 1.81938 loss)
I0111 23:21:55.099949 20429 sgd_solver.cpp:105] Iteration 5600, lr = 1e-07
I0111 23:26:58.103250 20429 solver.cpp:218] Iteration 5700 (0.330044 iter/s, 302.99s/100 iters), loss = 1.90377
I0111 23:26:58.103667 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.795
I0111 23:26:58.103720 20429 solver.cpp:238]     Train net output #1: loss = 1.90377 (* 1 = 1.90377 loss)
I0111 23:26:58.103735 20429 sgd_solver.cpp:105] Iteration 5700, lr = 1e-07
I0111 23:31:38.347203 20429 solver.cpp:218] Iteration 5800 (0.356849 iter/s, 280.231s/100 iters), loss = 2.21814
I0111 23:31:38.347429 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.74
I0111 23:31:38.347451 20429 solver.cpp:238]     Train net output #1: loss = 2.21814 (* 1 = 2.21814 loss)
I0111 23:31:38.347465 20429 sgd_solver.cpp:105] Iteration 5800, lr = 1e-07
I0111 23:36:44.472779 20429 solver.cpp:218] Iteration 5900 (0.326676 iter/s, 306.114s/100 iters), loss = 2.29578
I0111 23:36:44.478443 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.705
I0111 23:36:44.478493 20429 solver.cpp:238]     Train net output #1: loss = 2.29578 (* 1 = 2.29578 loss)
I0111 23:36:44.478507 20429 sgd_solver.cpp:105] Iteration 5900, lr = 1e-07
I0111 23:41:23.616780 20429 solver.cpp:450] Snapshotting to binary proto file ../other_model/alexnet_a4w8_iter_6000.caffemodel
I0111 23:41:32.817281 20429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../other_model/alexnet_a4w8_iter_6000.solverstate
I0111 23:41:38.484297 20429 solver.cpp:331] Iteration 6000, Testing net (#0)
I0111 23:41:38.484408 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0111 23:51:32.727505 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0111 23:51:45.667562 20429 solver.cpp:400]     Test net output #0: accuracy = 0.48064
I0111 23:51:45.667632 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.7239
I0111 23:51:45.667649 20429 solver.cpp:400]     Test net output #2: loss = 2.34109 (* 1 = 2.34109 loss)
I0111 23:51:48.173928 20429 solver.cpp:218] Iteration 6000 (0.110661 iter/s, 903.661s/100 iters), loss = 2.36752
I0111 23:51:48.174042 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.69
I0111 23:51:48.174067 20429 solver.cpp:238]     Train net output #1: loss = 2.36752 (* 1 = 2.36752 loss)
I0111 23:51:48.174087 20429 sgd_solver.cpp:105] Iteration 6000, lr = 1e-07
I0111 23:56:17.243436 20429 solver.cpp:218] Iteration 6100 (0.371667 iter/s, 269.058s/100 iters), loss = 2.03496
I0111 23:56:17.243803 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.775
I0111 23:56:17.243860 20429 solver.cpp:238]     Train net output #1: loss = 2.03496 (* 1 = 2.03496 loss)
I0111 23:56:17.243875 20429 sgd_solver.cpp:105] Iteration 6100, lr = 1e-07
I0112 00:01:24.662561 20429 solver.cpp:218] Iteration 6200 (0.325303 iter/s, 307.406s/100 iters), loss = 2.35864
I0112 00:01:24.674962 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0112 00:01:24.675021 20429 solver.cpp:238]     Train net output #1: loss = 2.35864 (* 1 = 2.35864 loss)
I0112 00:01:24.675042 20429 sgd_solver.cpp:105] Iteration 6200, lr = 1e-07
I0112 00:05:53.109428 20434 data_layer.cpp:73] Restarting data prefetching from start.
I0112 00:06:29.539916 20429 solver.cpp:218] Iteration 6300 (0.328028 iter/s, 304.852s/100 iters), loss = 2.4622
I0112 00:06:29.540323 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.71
I0112 00:06:29.540352 20429 solver.cpp:238]     Train net output #1: loss = 2.4622 (* 1 = 2.4622 loss)
I0112 00:06:29.540365 20429 sgd_solver.cpp:105] Iteration 6300, lr = 1e-07
I0112 00:11:23.396535 20429 solver.cpp:218] Iteration 6400 (0.340322 iter/s, 293.839s/100 iters), loss = 2.00084
I0112 00:11:23.396927 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.8
I0112 00:11:23.396996 20429 solver.cpp:238]     Train net output #1: loss = 2.00084 (* 1 = 2.00084 loss)
I0112 00:11:23.397022 20429 sgd_solver.cpp:105] Iteration 6400, lr = 1e-07
I0112 00:16:25.495808 20429 solver.cpp:218] Iteration 6500 (0.331038 iter/s, 302.08s/100 iters), loss = 2.15693
I0112 00:16:25.496193 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.775
I0112 00:16:25.496253 20429 solver.cpp:238]     Train net output #1: loss = 2.15693 (* 1 = 2.15693 loss)
I0112 00:16:25.496266 20429 sgd_solver.cpp:105] Iteration 6500, lr = 1e-07
I0112 00:21:15.265682 20429 solver.cpp:218] Iteration 6600 (0.345121 iter/s, 289.753s/100 iters), loss = 2.34066
I0112 00:21:15.266144 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.745
I0112 00:21:15.266196 20429 solver.cpp:238]     Train net output #1: loss = 2.34066 (* 1 = 2.34066 loss)
I0112 00:21:15.266208 20429 sgd_solver.cpp:105] Iteration 6600, lr = 1e-07
I0112 00:26:02.936195 20429 solver.cpp:218] Iteration 6700 (0.347639 iter/s, 287.655s/100 iters), loss = 2.2651
I0112 00:26:02.936514 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.735
I0112 00:26:02.936559 20429 solver.cpp:238]     Train net output #1: loss = 2.2651 (* 1 = 2.2651 loss)
I0112 00:26:02.936573 20429 sgd_solver.cpp:105] Iteration 6700, lr = 1e-07
I0112 00:30:58.206360 20429 solver.cpp:218] Iteration 6800 (0.33869 iter/s, 295.255s/100 iters), loss = 2.20911
I0112 00:30:58.206651 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0112 00:30:58.206707 20429 solver.cpp:238]     Train net output #1: loss = 2.20911 (* 1 = 2.20911 loss)
I0112 00:30:58.206719 20429 sgd_solver.cpp:105] Iteration 6800, lr = 1e-07
I0112 00:35:45.790441 20429 solver.cpp:218] Iteration 6900 (0.347742 iter/s, 287.57s/100 iters), loss = 2.44448
I0112 00:35:45.790758 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.71
I0112 00:35:45.790808 20429 solver.cpp:238]     Train net output #1: loss = 2.44448 (* 1 = 2.44448 loss)
I0112 00:35:45.790822 20429 sgd_solver.cpp:105] Iteration 6900, lr = 1e-07
I0112 00:40:37.030869 20429 solver.cpp:331] Iteration 7000, Testing net (#0)
I0112 00:40:37.031131 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 00:50:45.503437 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 00:50:57.448195 20429 solver.cpp:400]     Test net output #0: accuracy = 0.48116
I0112 00:50:57.448295 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.72094
I0112 00:50:57.448313 20429 solver.cpp:400]     Test net output #2: loss = 2.34716 (* 1 = 2.34716 loss)
I0112 00:51:00.203783 20429 solver.cpp:218] Iteration 7000 (0.109365 iter/s, 914.371s/100 iters), loss = 2.519
I0112 00:51:00.203891 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.685
I0112 00:51:00.203920 20429 solver.cpp:238]     Train net output #1: loss = 2.519 (* 1 = 2.519 loss)
I0112 00:51:00.203940 20429 sgd_solver.cpp:105] Iteration 7000, lr = 1e-07
I0112 00:55:39.601048 20429 solver.cpp:218] Iteration 7100 (0.35793 iter/s, 279.384s/100 iters), loss = 1.87931
I0112 00:55:39.601487 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.805
I0112 00:55:39.601531 20429 solver.cpp:238]     Train net output #1: loss = 1.87931 (* 1 = 1.87931 loss)
I0112 00:55:39.601544 20429 sgd_solver.cpp:105] Iteration 7100, lr = 1e-07
I0112 01:00:31.182883 20429 solver.cpp:218] Iteration 7200 (0.342973 iter/s, 291.568s/100 iters), loss = 2.03277
I0112 01:00:31.183279 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0112 01:00:31.183312 20429 solver.cpp:238]     Train net output #1: loss = 2.03277 (* 1 = 2.03277 loss)
I0112 01:00:31.183328 20429 sgd_solver.cpp:105] Iteration 7200, lr = 1e-07
I0112 01:05:08.511466 20429 solver.cpp:218] Iteration 7300 (0.3606 iter/s, 277.315s/100 iters), loss = 2.38166
I0112 01:05:08.511886 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.715
I0112 01:05:08.511940 20429 solver.cpp:238]     Train net output #1: loss = 2.38166 (* 1 = 2.38166 loss)
I0112 01:05:08.511955 20429 sgd_solver.cpp:105] Iteration 7300, lr = 1e-07
I0112 01:09:56.824471 20429 solver.cpp:218] Iteration 7400 (0.346862 iter/s, 288.299s/100 iters), loss = 2.10079
I0112 01:09:56.824820 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.75
I0112 01:09:56.824846 20429 solver.cpp:238]     Train net output #1: loss = 2.10079 (* 1 = 2.10079 loss)
I0112 01:09:56.824859 20429 sgd_solver.cpp:105] Iteration 7400, lr = 1e-07
I0112 01:14:31.134224 20429 solver.cpp:218] Iteration 7500 (0.364569 iter/s, 274.297s/100 iters), loss = 2.2335
I0112 01:14:31.134548 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.72
I0112 01:14:31.134599 20429 solver.cpp:238]     Train net output #1: loss = 2.2335 (* 1 = 2.2335 loss)
I0112 01:14:31.134611 20429 sgd_solver.cpp:105] Iteration 7500, lr = 1e-07
I0112 01:19:11.743569 20429 solver.cpp:218] Iteration 7600 (0.35639 iter/s, 280.591s/100 iters), loss = 2.46282
I0112 01:19:11.744026 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.685
I0112 01:19:11.744088 20429 solver.cpp:238]     Train net output #1: loss = 2.46282 (* 1 = 2.46282 loss)
I0112 01:19:11.744104 20429 sgd_solver.cpp:105] Iteration 7600, lr = 1e-07
I0112 01:24:04.263216 20429 solver.cpp:218] Iteration 7700 (0.341883 iter/s, 292.498s/100 iters), loss = 2.1931
I0112 01:24:04.263648 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.775
I0112 01:24:04.263713 20429 solver.cpp:238]     Train net output #1: loss = 2.1931 (* 1 = 2.1931 loss)
I0112 01:24:04.263730 20429 sgd_solver.cpp:105] Iteration 7700, lr = 1e-07
I0112 01:28:45.034399 20429 solver.cpp:218] Iteration 7800 (0.356185 iter/s, 280.753s/100 iters), loss = 2.29754
I0112 01:28:45.034768 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.71
I0112 01:28:45.034818 20429 solver.cpp:238]     Train net output #1: loss = 2.29754 (* 1 = 2.29754 loss)
I0112 01:28:45.034832 20429 sgd_solver.cpp:105] Iteration 7800, lr = 1e-07
I0112 01:33:13.803689 20429 solver.cpp:218] Iteration 7900 (0.372089 iter/s, 268.753s/100 iters), loss = 2.17804
I0112 01:33:13.804062 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.735
I0112 01:33:13.804091 20429 solver.cpp:238]     Train net output #1: loss = 2.17804 (* 1 = 2.17804 loss)
I0112 01:33:13.804105 20429 sgd_solver.cpp:105] Iteration 7900, lr = 1e-07
I0112 01:37:40.539880 20429 solver.cpp:450] Snapshotting to binary proto file ../other_model/alexnet_a4w8_iter_8000.caffemodel
I0112 01:37:50.062667 20429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../other_model/alexnet_a4w8_iter_8000.solverstate
I0112 01:37:55.648927 20429 solver.cpp:331] Iteration 8000, Testing net (#0)
I0112 01:37:55.649032 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 01:47:24.498569 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 01:47:33.309098 20429 solver.cpp:400]     Test net output #0: accuracy = 0.47968
I0112 01:47:33.309178 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.72388
I0112 01:47:33.309200 20429 solver.cpp:400]     Test net output #2: loss = 2.34626 (* 1 = 2.34626 loss)
I0112 01:47:36.088856 20429 solver.cpp:218] Iteration 8000 (0.115977 iter/s, 862.238s/100 iters), loss = 2.35073
I0112 01:47:36.088966 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.705
I0112 01:47:36.088989 20429 solver.cpp:238]     Train net output #1: loss = 2.35073 (* 1 = 2.35073 loss)
I0112 01:47:36.089002 20429 sgd_solver.cpp:105] Iteration 8000, lr = 1e-07
I0112 01:52:15.759435 20429 solver.cpp:218] Iteration 8100 (0.35758 iter/s, 279.657s/100 iters), loss = 2.30778
I0112 01:52:15.759766 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0112 01:52:15.759817 20429 solver.cpp:238]     Train net output #1: loss = 2.30778 (* 1 = 2.30778 loss)
I0112 01:52:15.759830 20429 sgd_solver.cpp:105] Iteration 8100, lr = 1e-07
I0112 01:56:51.596560 20429 solver.cpp:218] Iteration 8200 (0.362538 iter/s, 275.833s/100 iters), loss = 2.25817
I0112 01:56:51.596900 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0112 01:56:51.596930 20429 solver.cpp:238]     Train net output #1: loss = 2.25817 (* 1 = 2.25817 loss)
I0112 01:56:51.596943 20429 sgd_solver.cpp:105] Iteration 8200, lr = 1e-07
I0112 02:01:46.682963 20429 solver.cpp:218] Iteration 8300 (0.338893 iter/s, 295.078s/100 iters), loss = 2.29022
I0112 02:01:46.683517 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0112 02:01:46.683565 20429 solver.cpp:238]     Train net output #1: loss = 2.29022 (* 1 = 2.29022 loss)
I0112 02:01:46.683579 20429 sgd_solver.cpp:105] Iteration 8300, lr = 1e-07
I0112 02:06:31.351362 20429 solver.cpp:218] Iteration 8400 (0.351299 iter/s, 284.658s/100 iters), loss = 2.34615
I0112 02:06:31.371256 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.72
I0112 02:06:31.371309 20429 solver.cpp:238]     Train net output #1: loss = 2.34615 (* 1 = 2.34615 loss)
I0112 02:06:31.371320 20429 sgd_solver.cpp:105] Iteration 8400, lr = 1e-07
I0112 02:11:13.160599 20429 solver.cpp:218] Iteration 8500 (0.354889 iter/s, 281.779s/100 iters), loss = 2.19502
I0112 02:11:13.160924 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.745
I0112 02:11:13.160959 20429 solver.cpp:238]     Train net output #1: loss = 2.19502 (* 1 = 2.19502 loss)
I0112 02:11:13.160970 20429 sgd_solver.cpp:105] Iteration 8500, lr = 1e-07
I0112 02:15:47.718184 20429 solver.cpp:218] Iteration 8600 (0.364238 iter/s, 274.546s/100 iters), loss = 2.58621
I0112 02:15:47.718547 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.68
I0112 02:15:47.718598 20429 solver.cpp:238]     Train net output #1: loss = 2.58621 (* 1 = 2.58621 loss)
I0112 02:15:47.718611 20429 sgd_solver.cpp:105] Iteration 8600, lr = 1e-07
I0112 02:20:26.477661 20429 solver.cpp:218] Iteration 8700 (0.358748 iter/s, 278.747s/100 iters), loss = 2.24245
I0112 02:20:26.478000 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0112 02:20:26.478027 20429 solver.cpp:238]     Train net output #1: loss = 2.24245 (* 1 = 2.24245 loss)
I0112 02:20:26.478040 20429 sgd_solver.cpp:105] Iteration 8700, lr = 1e-07
I0112 02:25:03.783170 20429 solver.cpp:218] Iteration 8800 (0.360629 iter/s, 277.293s/100 iters), loss = 2.49277
I0112 02:25:03.783656 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.675
I0112 02:25:03.783707 20429 solver.cpp:238]     Train net output #1: loss = 2.49277 (* 1 = 2.49277 loss)
I0112 02:25:03.783720 20429 sgd_solver.cpp:105] Iteration 8800, lr = 1e-07
I0112 02:29:53.766387 20429 solver.cpp:218] Iteration 8900 (0.344832 iter/s, 289.996s/100 iters), loss = 2.32783
I0112 02:29:53.766755 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.735
I0112 02:29:53.766813 20429 solver.cpp:238]     Train net output #1: loss = 2.32783 (* 1 = 2.32783 loss)
I0112 02:29:53.766826 20429 sgd_solver.cpp:105] Iteration 8900, lr = 1e-07
I0112 02:34:42.553231 20429 solver.cpp:331] Iteration 9000, Testing net (#0)
I0112 02:34:42.565387 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 02:44:13.244587 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 02:44:22.292675 20429 solver.cpp:400]     Test net output #0: accuracy = 0.4774
I0112 02:44:22.292757 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.7215
I0112 02:44:22.292773 20429 solver.cpp:400]     Test net output #2: loss = 2.35294 (* 1 = 2.35294 loss)
I0112 02:44:25.012059 20429 solver.cpp:218] Iteration 9000 (0.114778 iter/s, 871.251s/100 iters), loss = 2.01521
I0112 02:44:25.012152 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.775
I0112 02:44:25.012171 20429 solver.cpp:238]     Train net output #1: loss = 2.01521 (* 1 = 2.01521 loss)
I0112 02:44:25.012187 20429 sgd_solver.cpp:105] Iteration 9000, lr = 1e-07
I0112 02:48:52.649011 20429 solver.cpp:218] Iteration 9100 (0.373648 iter/s, 267.632s/100 iters), loss = 2.46072
I0112 02:48:52.649310 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0112 02:48:52.649377 20429 solver.cpp:238]     Train net output #1: loss = 2.46072 (* 1 = 2.46072 loss)
I0112 02:48:52.649395 20429 sgd_solver.cpp:105] Iteration 9100, lr = 1e-07
I0112 02:53:25.506789 20429 solver.cpp:218] Iteration 9200 (0.3665 iter/s, 272.851s/100 iters), loss = 2.34065
I0112 02:53:25.507171 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0112 02:53:25.507222 20429 solver.cpp:238]     Train net output #1: loss = 2.34065 (* 1 = 2.34065 loss)
I0112 02:53:25.507236 20429 sgd_solver.cpp:105] Iteration 9200, lr = 1e-07
I0112 02:57:52.463403 20429 solver.cpp:218] Iteration 9300 (0.374603 iter/s, 266.949s/100 iters), loss = 2.26937
I0112 02:57:52.463810 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0112 02:57:52.463837 20429 solver.cpp:238]     Train net output #1: loss = 2.26937 (* 1 = 2.26937 loss)
I0112 02:57:52.463850 20429 sgd_solver.cpp:105] Iteration 9300, lr = 1e-07
I0112 03:02:21.598676 20429 solver.cpp:218] Iteration 9400 (0.371571 iter/s, 269.128s/100 iters), loss = 2.08527
I0112 03:02:21.599125 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0112 03:02:21.599184 20429 solver.cpp:238]     Train net output #1: loss = 2.08527 (* 1 = 2.08527 loss)
I0112 03:02:21.599198 20429 sgd_solver.cpp:105] Iteration 9400, lr = 1e-07
I0112 03:06:49.505761 20429 solver.cpp:218] Iteration 9500 (0.373275 iter/s, 267.899s/100 iters), loss = 2.19376
I0112 03:06:49.506112 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0112 03:06:49.506142 20429 solver.cpp:238]     Train net output #1: loss = 2.19376 (* 1 = 2.19376 loss)
I0112 03:06:49.506155 20429 sgd_solver.cpp:105] Iteration 9500, lr = 1e-07
I0112 03:11:19.933504 20429 solver.cpp:218] Iteration 9600 (0.369796 iter/s, 270.42s/100 iters), loss = 2.19311
I0112 03:11:19.933923 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0112 03:11:19.933951 20429 solver.cpp:238]     Train net output #1: loss = 2.19311 (* 1 = 2.19311 loss)
I0112 03:11:19.933991 20429 sgd_solver.cpp:105] Iteration 9600, lr = 1e-07
I0112 03:16:03.029963 20429 solver.cpp:218] Iteration 9700 (0.353247 iter/s, 283.088s/100 iters), loss = 2.58541
I0112 03:16:03.030352 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.685
I0112 03:16:03.030405 20429 solver.cpp:238]     Train net output #1: loss = 2.58541 (* 1 = 2.58541 loss)
I0112 03:16:03.030428 20429 sgd_solver.cpp:105] Iteration 9700, lr = 1e-07
I0112 03:20:39.084156 20429 solver.cpp:218] Iteration 9800 (0.36239 iter/s, 275.946s/100 iters), loss = 2.04991
I0112 03:20:39.084542 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0112 03:20:39.084592 20429 solver.cpp:238]     Train net output #1: loss = 2.04991 (* 1 = 2.04991 loss)
I0112 03:20:39.084605 20429 sgd_solver.cpp:105] Iteration 9800, lr = 1e-07
I0112 03:25:21.932379 20429 solver.cpp:218] Iteration 9900 (0.35364 iter/s, 282.774s/100 iters), loss = 2.51973
I0112 03:25:21.941578 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.705
I0112 03:25:21.941629 20429 solver.cpp:238]     Train net output #1: loss = 2.51973 (* 1 = 2.51973 loss)
I0112 03:25:21.941642 20429 sgd_solver.cpp:105] Iteration 9900, lr = 1e-07
I0112 03:29:59.332077 20429 solver.cpp:450] Snapshotting to binary proto file ../other_model/alexnet_a4w8_iter_10000.caffemodel
I0112 03:30:08.481721 20429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../other_model/alexnet_a4w8_iter_10000.solverstate
I0112 03:30:13.924239 20429 solver.cpp:331] Iteration 10000, Testing net (#0)
I0112 03:30:13.924330 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 03:39:51.192119 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 03:40:00.929605 20429 solver.cpp:400]     Test net output #0: accuracy = 0.47726
I0112 03:40:00.929675 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.71994
I0112 03:40:00.929700 20429 solver.cpp:400]     Test net output #2: loss = 2.35766 (* 1 = 2.35766 loss)
I0112 03:40:03.786478 20429 solver.cpp:218] Iteration 10000 (0.113406 iter/s, 881.784s/100 iters), loss = 2.29673
I0112 03:40:03.786587 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.765
I0112 03:40:03.786612 20429 solver.cpp:238]     Train net output #1: loss = 2.29673 (* 1 = 2.29673 loss)
I0112 03:40:03.786625 20429 sgd_solver.cpp:105] Iteration 10000, lr = 1e-07
I0112 03:44:40.769831 20429 solver.cpp:218] Iteration 10100 (0.361049 iter/s, 276.971s/100 iters), loss = 2.06019
I0112 03:44:40.770308 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.77
I0112 03:44:40.770370 20429 solver.cpp:238]     Train net output #1: loss = 2.06019 (* 1 = 2.06019 loss)
I0112 03:44:40.770383 20429 sgd_solver.cpp:105] Iteration 10100, lr = 1e-07
I0112 03:49:11.323654 20429 solver.cpp:218] Iteration 10200 (0.369639 iter/s, 270.534s/100 iters), loss = 2.25925
I0112 03:49:11.323973 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.74
I0112 03:49:11.324033 20429 solver.cpp:238]     Train net output #1: loss = 2.25925 (* 1 = 2.25925 loss)
I0112 03:49:11.324048 20429 sgd_solver.cpp:105] Iteration 10200, lr = 1e-07
I0112 03:53:47.056860 20429 solver.cpp:218] Iteration 10300 (0.362684 iter/s, 275.722s/100 iters), loss = 2.38655
I0112 03:53:47.057287 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0112 03:53:47.057399 20429 solver.cpp:238]     Train net output #1: loss = 2.38655 (* 1 = 2.38655 loss)
I0112 03:53:47.057420 20429 sgd_solver.cpp:105] Iteration 10300, lr = 1e-07
I0112 03:58:33.870024 20429 solver.cpp:218] Iteration 10400 (0.34867 iter/s, 286.804s/100 iters), loss = 2.2747
I0112 03:58:33.870373 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0112 03:58:33.870424 20429 solver.cpp:238]     Train net output #1: loss = 2.2747 (* 1 = 2.2747 loss)
I0112 03:58:33.870436 20429 sgd_solver.cpp:105] Iteration 10400, lr = 1e-07
I0112 04:03:07.596417 20429 solver.cpp:218] Iteration 10500 (0.365347 iter/s, 273.713s/100 iters), loss = 2.41167
I0112 04:03:07.596796 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.715
I0112 04:03:07.596858 20429 solver.cpp:238]     Train net output #1: loss = 2.41167 (* 1 = 2.41167 loss)
I0112 04:03:07.596873 20429 sgd_solver.cpp:105] Iteration 10500, lr = 1e-07
I0112 04:07:42.391638 20429 solver.cpp:218] Iteration 10600 (0.363929 iter/s, 274.779s/100 iters), loss = 2.35992
I0112 04:07:42.392115 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.74
I0112 04:07:42.392180 20429 solver.cpp:238]     Train net output #1: loss = 2.35992 (* 1 = 2.35992 loss)
I0112 04:07:42.392206 20429 sgd_solver.cpp:105] Iteration 10600, lr = 1e-07
I0112 04:12:11.902570 20429 solver.cpp:218] Iteration 10700 (0.371067 iter/s, 269.493s/100 iters), loss = 2.16386
I0112 04:12:11.903038 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0112 04:12:11.903076 20429 solver.cpp:238]     Train net output #1: loss = 2.16386 (* 1 = 2.16386 loss)
I0112 04:12:11.903087 20429 sgd_solver.cpp:105] Iteration 10700, lr = 1e-07
I0112 04:16:40.033454 20429 solver.cpp:218] Iteration 10800 (0.372978 iter/s, 268.112s/100 iters), loss = 2.06776
I0112 04:16:40.079768 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.79
I0112 04:16:40.079830 20429 solver.cpp:238]     Train net output #1: loss = 2.06776 (* 1 = 2.06776 loss)
I0112 04:16:40.079844 20429 sgd_solver.cpp:105] Iteration 10800, lr = 1e-07
I0112 04:21:21.340270 20429 solver.cpp:218] Iteration 10900 (0.355567 iter/s, 281.241s/100 iters), loss = 2.12975
I0112 04:21:21.340710 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.735
I0112 04:21:21.340741 20429 solver.cpp:238]     Train net output #1: loss = 2.12975 (* 1 = 2.12975 loss)
I0112 04:21:21.340764 20429 sgd_solver.cpp:105] Iteration 10900, lr = 1e-07
I0112 04:26:03.941078 20429 solver.cpp:331] Iteration 11000, Testing net (#0)
I0112 04:26:03.941457 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 04:36:02.628057 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 04:36:11.691231 20429 solver.cpp:400]     Test net output #0: accuracy = 0.4779
I0112 04:36:11.691305 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.72182
I0112 04:36:11.691323 20429 solver.cpp:400]     Test net output #2: loss = 2.3602 (* 1 = 2.3602 loss)
I0112 04:36:14.482782 20429 solver.cpp:218] Iteration 11000 (0.111966 iter/s, 893.132s/100 iters), loss = 2.31351
I0112 04:36:14.482892 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.74
I0112 04:36:14.482913 20429 solver.cpp:238]     Train net output #1: loss = 2.31351 (* 1 = 2.31351 loss)
I0112 04:36:14.482935 20429 sgd_solver.cpp:105] Iteration 11000, lr = 1e-07
I0112 04:40:50.709409 20429 solver.cpp:218] Iteration 11100 (0.362032 iter/s, 276.219s/100 iters), loss = 2.43865
I0112 04:40:50.709604 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.675
I0112 04:40:50.709642 20429 solver.cpp:238]     Train net output #1: loss = 2.43865 (* 1 = 2.43865 loss)
I0112 04:40:50.709656 20429 sgd_solver.cpp:105] Iteration 11100, lr = 1e-07
I0112 04:45:25.584486 20429 solver.cpp:218] Iteration 11200 (0.363816 iter/s, 274.864s/100 iters), loss = 2.41546
I0112 04:45:25.584858 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0112 04:45:25.584887 20429 solver.cpp:238]     Train net output #1: loss = 2.41546 (* 1 = 2.41546 loss)
I0112 04:45:25.584905 20429 sgd_solver.cpp:105] Iteration 11200, lr = 1e-07
I0112 04:50:01.878583 20429 solver.cpp:218] Iteration 11300 (0.36195 iter/s, 276.281s/100 iters), loss = 2.46575
I0112 04:50:01.878926 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.665
I0112 04:50:01.878983 20429 solver.cpp:238]     Train net output #1: loss = 2.46575 (* 1 = 2.46575 loss)
I0112 04:50:01.878999 20429 sgd_solver.cpp:105] Iteration 11300, lr = 1e-07
I0112 04:54:37.266141 20429 solver.cpp:218] Iteration 11400 (0.363142 iter/s, 275.374s/100 iters), loss = 2.70406
I0112 04:54:37.266427 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0112 04:54:37.266479 20429 solver.cpp:238]     Train net output #1: loss = 2.70406 (* 1 = 2.70406 loss)
I0112 04:54:37.266492 20429 sgd_solver.cpp:105] Iteration 11400, lr = 1e-07
I0112 04:59:30.844353 20429 solver.cpp:218] Iteration 11500 (0.340642 iter/s, 293.563s/100 iters), loss = 2.03003
I0112 04:59:30.844621 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.8
I0112 04:59:30.844666 20429 solver.cpp:238]     Train net output #1: loss = 2.03003 (* 1 = 2.03003 loss)
I0112 04:59:30.844679 20429 sgd_solver.cpp:105] Iteration 11500, lr = 1e-07
I0112 05:04:08.138981 20429 solver.cpp:218] Iteration 11600 (0.360637 iter/s, 277.287s/100 iters), loss = 2.70946
I0112 05:04:08.139447 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.675
I0112 05:04:08.139513 20429 solver.cpp:238]     Train net output #1: loss = 2.70946 (* 1 = 2.70946 loss)
I0112 05:04:08.139528 20429 sgd_solver.cpp:105] Iteration 11600, lr = 1e-07
I0112 05:08:50.482476 20429 solver.cpp:218] Iteration 11700 (0.354192 iter/s, 282.333s/100 iters), loss = 2.12669
I0112 05:08:50.494026 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.775
I0112 05:08:50.494060 20429 solver.cpp:238]     Train net output #1: loss = 2.12669 (* 1 = 2.12669 loss)
I0112 05:08:50.494073 20429 sgd_solver.cpp:105] Iteration 11700, lr = 1e-07
I0112 05:13:53.431828 20429 solver.cpp:218] Iteration 11800 (0.330114 iter/s, 302.925s/100 iters), loss = 2.59212
I0112 05:13:53.432176 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.69
I0112 05:13:53.432234 20429 solver.cpp:238]     Train net output #1: loss = 2.59212 (* 1 = 2.59212 loss)
I0112 05:13:53.432246 20429 sgd_solver.cpp:105] Iteration 11800, lr = 1e-07
I0112 05:18:49.886529 20429 solver.cpp:218] Iteration 11900 (0.337335 iter/s, 296.441s/100 iters), loss = 2.34339
I0112 05:18:49.886967 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.745
I0112 05:18:49.887020 20429 solver.cpp:238]     Train net output #1: loss = 2.34339 (* 1 = 2.34339 loss)
I0112 05:18:49.887034 20429 sgd_solver.cpp:105] Iteration 11900, lr = 1e-07
I0112 05:23:33.460279 20429 solver.cpp:450] Snapshotting to binary proto file ../other_model/alexnet_a4w8_iter_12000.caffemodel
I0112 05:23:42.404208 20429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../other_model/alexnet_a4w8_iter_12000.solverstate
I0112 05:23:48.264449 20429 solver.cpp:331] Iteration 12000, Testing net (#0)
I0112 05:23:48.264542 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 05:33:31.093830 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 05:33:44.574607 20429 solver.cpp:400]     Test net output #0: accuracy = 0.47634
I0112 05:33:44.574975 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.71728
I0112 05:33:44.576170 20429 solver.cpp:400]     Test net output #2: loss = 2.3694 (* 1 = 2.3694 loss)
I0112 05:33:49.594172 20429 solver.cpp:218] Iteration 12000 (0.111152 iter/s, 899.666s/100 iters), loss = 2.29318
I0112 05:33:49.594269 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.745
I0112 05:33:49.594292 20429 solver.cpp:238]     Train net output #1: loss = 2.29318 (* 1 = 2.29318 loss)
I0112 05:33:49.594305 20429 sgd_solver.cpp:105] Iteration 12000, lr = 1e-07
I0112 05:38:25.254035 20429 solver.cpp:218] Iteration 12100 (0.362756 iter/s, 275.668s/100 iters), loss = 2.51355
I0112 05:38:25.254379 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0112 05:38:25.254431 20429 solver.cpp:238]     Train net output #1: loss = 2.51355 (* 1 = 2.51355 loss)
I0112 05:38:25.254443 20429 sgd_solver.cpp:105] Iteration 12100, lr = 1e-07
I0112 05:43:01.019438 20429 solver.cpp:218] Iteration 12200 (0.362626 iter/s, 275.766s/100 iters), loss = 2.48891
I0112 05:43:01.019639 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.685
I0112 05:43:01.019664 20429 solver.cpp:238]     Train net output #1: loss = 2.48891 (* 1 = 2.48891 loss)
I0112 05:43:01.019675 20429 sgd_solver.cpp:105] Iteration 12200, lr = 1e-07
I0112 05:47:37.071209 20429 solver.cpp:218] Iteration 12300 (0.362256 iter/s, 276.048s/100 iters), loss = 2.64082
I0112 05:47:37.071586 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.68
I0112 05:47:37.071616 20429 solver.cpp:238]     Train net output #1: loss = 2.64082 (* 1 = 2.64082 loss)
I0112 05:47:37.071630 20429 sgd_solver.cpp:105] Iteration 12300, lr = 1e-07
I0112 05:52:28.729701 20429 solver.cpp:218] Iteration 12400 (0.342874 iter/s, 291.652s/100 iters), loss = 2.59102
I0112 05:52:28.730175 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.685
I0112 05:52:28.730212 20429 solver.cpp:238]     Train net output #1: loss = 2.59102 (* 1 = 2.59102 loss)
I0112 05:52:28.730226 20429 sgd_solver.cpp:105] Iteration 12400, lr = 1e-07
I0112 05:57:17.715250 20429 solver.cpp:218] Iteration 12500 (0.346048 iter/s, 288.977s/100 iters), loss = 2.28766
I0112 05:57:17.755584 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.76
I0112 05:57:17.755659 20429 solver.cpp:238]     Train net output #1: loss = 2.28766 (* 1 = 2.28766 loss)
I0112 05:57:17.755682 20429 sgd_solver.cpp:105] Iteration 12500, lr = 1e-07
I0112 06:01:17.010329 20434 data_layer.cpp:73] Restarting data prefetching from start.
I0112 06:01:58.856312 20429 solver.cpp:218] Iteration 12600 (0.355755 iter/s, 281.092s/100 iters), loss = 2.42257
I0112 06:01:58.856676 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.695
I0112 06:01:58.856729 20429 solver.cpp:238]     Train net output #1: loss = 2.42257 (* 1 = 2.42257 loss)
I0112 06:01:58.856741 20429 sgd_solver.cpp:105] Iteration 12600, lr = 1e-07
I0112 06:06:43.075764 20429 solver.cpp:218] Iteration 12700 (0.351852 iter/s, 284.21s/100 iters), loss = 2.57627
I0112 06:06:43.076128 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.675
I0112 06:06:43.076161 20429 solver.cpp:238]     Train net output #1: loss = 2.57627 (* 1 = 2.57627 loss)
I0112 06:06:43.076187 20429 sgd_solver.cpp:105] Iteration 12700, lr = 1e-07
I0112 06:11:35.251449 20429 solver.cpp:218] Iteration 12800 (0.342301 iter/s, 292.141s/100 iters), loss = 2.57075
I0112 06:11:35.251783 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0112 06:11:35.251827 20429 solver.cpp:238]     Train net output #1: loss = 2.57075 (* 1 = 2.57075 loss)
I0112 06:11:35.251840 20429 sgd_solver.cpp:105] Iteration 12800, lr = 1e-07
I0112 06:16:30.522886 20429 solver.cpp:218] Iteration 12900 (0.338708 iter/s, 295.24s/100 iters), loss = 2.35687
I0112 06:16:30.523210 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.75
I0112 06:16:30.523239 20429 solver.cpp:238]     Train net output #1: loss = 2.35687 (* 1 = 2.35687 loss)
I0112 06:16:30.523252 20429 sgd_solver.cpp:105] Iteration 12900, lr = 1e-07
I0112 06:21:16.207103 20429 solver.cpp:331] Iteration 13000, Testing net (#0)
I0112 06:21:16.207495 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 06:31:12.569064 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 06:31:22.739665 20429 solver.cpp:400]     Test net output #0: accuracy = 0.4749
I0112 06:31:22.739747 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.7177
I0112 06:31:22.739763 20429 solver.cpp:400]     Test net output #2: loss = 2.38432 (* 1 = 2.38432 loss)
I0112 06:31:25.286895 20429 solver.cpp:218] Iteration 13000 (0.111769 iter/s, 894.703s/100 iters), loss = 3.02534
I0112 06:31:25.287044 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.64
I0112 06:31:25.287075 20429 solver.cpp:238]     Train net output #1: loss = 3.02534 (* 1 = 3.02534 loss)
I0112 06:31:25.287091 20429 sgd_solver.cpp:105] Iteration 13000, lr = 1e-07
I0112 06:36:17.454088 20429 solver.cpp:218] Iteration 13100 (0.342288 iter/s, 292.151s/100 iters), loss = 2.45509
I0112 06:36:17.454370 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.715
I0112 06:36:17.454427 20429 solver.cpp:238]     Train net output #1: loss = 2.45509 (* 1 = 2.45509 loss)
I0112 06:36:17.454440 20429 sgd_solver.cpp:105] Iteration 13100, lr = 1e-07
I0112 06:40:54.121925 20429 solver.cpp:218] Iteration 13200 (0.361463 iter/s, 276.653s/100 iters), loss = 2.42701
I0112 06:40:54.122280 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.72
I0112 06:40:54.122319 20429 solver.cpp:238]     Train net output #1: loss = 2.42701 (* 1 = 2.42701 loss)
I0112 06:40:54.122334 20429 sgd_solver.cpp:105] Iteration 13200, lr = 1e-07
I0112 06:45:39.258170 20429 solver.cpp:218] Iteration 13300 (0.350728 iter/s, 285.122s/100 iters), loss = 2.08006
I0112 06:45:39.258666 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.79
I0112 06:45:39.258718 20429 solver.cpp:238]     Train net output #1: loss = 2.08006 (* 1 = 2.08006 loss)
I0112 06:45:39.258730 20429 sgd_solver.cpp:105] Iteration 13300, lr = 1e-07
I0112 06:50:28.209509 20429 solver.cpp:218] Iteration 13400 (0.346097 iter/s, 288.937s/100 iters), loss = 2.30591
I0112 06:50:28.220459 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0112 06:50:28.220548 20429 solver.cpp:238]     Train net output #1: loss = 2.30591 (* 1 = 2.30591 loss)
I0112 06:50:28.220577 20429 sgd_solver.cpp:105] Iteration 13400, lr = 1e-07
I0112 06:55:22.988023 20429 solver.cpp:218] Iteration 13500 (0.339267 iter/s, 294.753s/100 iters), loss = 2.5668
I0112 06:55:22.988339 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.655
I0112 06:55:22.988389 20429 solver.cpp:238]     Train net output #1: loss = 2.5668 (* 1 = 2.5668 loss)
I0112 06:55:22.988400 20429 sgd_solver.cpp:105] Iteration 13500, lr = 1e-07
I0112 07:00:14.264729 20429 solver.cpp:218] Iteration 13600 (0.343333 iter/s, 291.262s/100 iters), loss = 2.24201
I0112 07:00:14.265142 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.745
I0112 07:00:14.265195 20429 solver.cpp:238]     Train net output #1: loss = 2.24201 (* 1 = 2.24201 loss)
I0112 07:00:14.265208 20429 sgd_solver.cpp:105] Iteration 13600, lr = 1e-07
I0112 07:05:08.593525 20429 solver.cpp:218] Iteration 13700 (0.339773 iter/s, 294.314s/100 iters), loss = 2.52489
I0112 07:05:08.593837 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.72
I0112 07:05:08.593888 20429 solver.cpp:238]     Train net output #1: loss = 2.52489 (* 1 = 2.52489 loss)
I0112 07:05:08.593899 20429 sgd_solver.cpp:105] Iteration 13700, lr = 1e-07
I0112 07:09:58.177582 20429 solver.cpp:218] Iteration 13800 (0.34534 iter/s, 289.57s/100 iters), loss = 2.31684
I0112 07:09:58.177884 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.71
I0112 07:09:58.177924 20429 solver.cpp:238]     Train net output #1: loss = 2.31684 (* 1 = 2.31684 loss)
I0112 07:09:58.177937 20429 sgd_solver.cpp:105] Iteration 13800, lr = 1e-07
I0112 07:14:56.916719 20429 solver.cpp:218] Iteration 13900 (0.334757 iter/s, 298.724s/100 iters), loss = 2.31554
I0112 07:14:56.917112 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.735
I0112 07:14:56.917141 20429 solver.cpp:238]     Train net output #1: loss = 2.31554 (* 1 = 2.31554 loss)
I0112 07:14:56.917153 20429 sgd_solver.cpp:105] Iteration 13900, lr = 1e-07
I0112 07:19:50.269862 20429 solver.cpp:450] Snapshotting to binary proto file ../other_model/alexnet_a4w8_iter_14000.caffemodel
I0112 07:19:58.471271 20429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../other_model/alexnet_a4w8_iter_14000.solverstate
I0112 07:20:04.398296 20429 solver.cpp:331] Iteration 14000, Testing net (#0)
I0112 07:20:04.398386 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 07:29:50.588788 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 07:30:00.186157 20429 solver.cpp:400]     Test net output #0: accuracy = 0.47322
I0112 07:30:00.186240 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.71626
I0112 07:30:00.186255 20429 solver.cpp:400]     Test net output #2: loss = 2.38697 (* 1 = 2.38697 loss)
I0112 07:30:02.760687 20429 solver.cpp:218] Iteration 14000 (0.110401 iter/s, 905.791s/100 iters), loss = 2.56285
I0112 07:30:02.760787 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.67
I0112 07:30:02.760807 20429 solver.cpp:238]     Train net output #1: loss = 2.56285 (* 1 = 2.56285 loss)
I0112 07:30:02.760821 20429 sgd_solver.cpp:105] Iteration 14000, lr = 1e-07
I0112 07:34:58.518993 20429 solver.cpp:218] Iteration 14100 (0.338133 iter/s, 295.742s/100 iters), loss = 2.52018
I0112 07:34:58.519361 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.715
I0112 07:34:58.519389 20429 solver.cpp:238]     Train net output #1: loss = 2.52018 (* 1 = 2.52018 loss)
I0112 07:34:58.519412 20429 sgd_solver.cpp:105] Iteration 14100, lr = 1e-07
I0112 07:39:59.637475 20429 solver.cpp:218] Iteration 14200 (0.332113 iter/s, 301.102s/100 iters), loss = 2.28494
I0112 07:39:59.637980 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0112 07:39:59.638010 20429 solver.cpp:238]     Train net output #1: loss = 2.28494 (* 1 = 2.28494 loss)
I0112 07:39:59.638023 20429 sgd_solver.cpp:105] Iteration 14200, lr = 1e-07
I0112 07:44:39.407308 20429 solver.cpp:218] Iteration 14300 (0.357456 iter/s, 279.755s/100 iters), loss = 2.61059
I0112 07:44:39.416628 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.66
I0112 07:44:39.416680 20429 solver.cpp:238]     Train net output #1: loss = 2.61059 (* 1 = 2.61059 loss)
I0112 07:44:39.416693 20429 sgd_solver.cpp:105] Iteration 14300, lr = 1e-07
I0112 07:49:47.531997 20429 solver.cpp:218] Iteration 14400 (0.324571 iter/s, 308.099s/100 iters), loss = 2.45601
I0112 07:49:47.532335 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.735
I0112 07:49:47.532377 20429 solver.cpp:238]     Train net output #1: loss = 2.45601 (* 1 = 2.45601 loss)
I0112 07:49:47.532390 20429 sgd_solver.cpp:105] Iteration 14400, lr = 1e-07
I0112 07:54:52.110102 20429 solver.cpp:218] Iteration 14500 (0.328333 iter/s, 304.569s/100 iters), loss = 2.72016
I0112 07:54:52.110461 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.63
I0112 07:54:52.110488 20429 solver.cpp:238]     Train net output #1: loss = 2.72016 (* 1 = 2.72016 loss)
I0112 07:54:52.110514 20429 sgd_solver.cpp:105] Iteration 14500, lr = 1e-07
I0112 07:59:45.347395 20429 solver.cpp:218] Iteration 14600 (0.341026 iter/s, 293.233s/100 iters), loss = 2.25344
I0112 07:59:45.347795 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.745
I0112 07:59:45.347844 20429 solver.cpp:238]     Train net output #1: loss = 2.25344 (* 1 = 2.25344 loss)
I0112 07:59:45.347857 20429 sgd_solver.cpp:105] Iteration 14600, lr = 1e-07
I0112 08:04:38.822222 20429 solver.cpp:218] Iteration 14700 (0.340755 iter/s, 293.466s/100 iters), loss = 2.44682
I0112 08:04:38.822577 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.72
I0112 08:04:38.822607 20429 solver.cpp:238]     Train net output #1: loss = 2.44682 (* 1 = 2.44682 loss)
I0112 08:04:38.822619 20429 sgd_solver.cpp:105] Iteration 14700, lr = 1e-07
I0112 08:09:37.471343 20429 solver.cpp:218] Iteration 14800 (0.334853 iter/s, 298.638s/100 iters), loss = 2.42714
I0112 08:09:37.471729 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0112 08:09:37.471786 20429 solver.cpp:238]     Train net output #1: loss = 2.42714 (* 1 = 2.42714 loss)
I0112 08:09:37.471799 20429 sgd_solver.cpp:105] Iteration 14800, lr = 1e-07
I0112 08:14:43.063777 20429 solver.cpp:218] Iteration 14900 (0.327247 iter/s, 305.58s/100 iters), loss = 2.28628
I0112 08:14:43.064151 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.74
I0112 08:14:43.064201 20429 solver.cpp:238]     Train net output #1: loss = 2.28628 (* 1 = 2.28628 loss)
I0112 08:14:43.064214 20429 sgd_solver.cpp:105] Iteration 14900, lr = 1e-07
I0112 08:19:11.526592 20429 solver.cpp:331] Iteration 15000, Testing net (#0)
I0112 08:19:11.526914 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 08:29:19.078485 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 08:29:27.983218 20429 solver.cpp:400]     Test net output #0: accuracy = 0.47244
I0112 08:29:27.983301 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.71526
I0112 08:29:27.983319 20429 solver.cpp:400]     Test net output #2: loss = 2.39809 (* 1 = 2.39809 loss)
I0112 08:29:30.356627 20429 solver.cpp:218] Iteration 15000 (0.112707 iter/s, 887.259s/100 iters), loss = 2.53102
I0112 08:29:30.356743 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.68
I0112 08:29:30.356765 20429 solver.cpp:238]     Train net output #1: loss = 2.53102 (* 1 = 2.53102 loss)
I0112 08:29:30.356789 20429 sgd_solver.cpp:105] Iteration 15000, lr = 1e-07
I0112 08:34:22.346577 20429 solver.cpp:218] Iteration 15100 (0.342487 iter/s, 291.982s/100 iters), loss = 2.49913
I0112 08:34:22.346998 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.71
I0112 08:34:22.347033 20429 solver.cpp:238]     Train net output #1: loss = 2.49913 (* 1 = 2.49913 loss)
I0112 08:34:22.347048 20429 sgd_solver.cpp:105] Iteration 15100, lr = 1e-07
I0112 08:39:19.119463 20429 solver.cpp:218] Iteration 15200 (0.33697 iter/s, 296.763s/100 iters), loss = 2.42322
I0112 08:39:19.140241 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0112 08:39:19.140271 20429 solver.cpp:238]     Train net output #1: loss = 2.42322 (* 1 = 2.42322 loss)
I0112 08:39:19.140285 20429 sgd_solver.cpp:105] Iteration 15200, lr = 1e-07
I0112 08:43:55.786747 20429 solver.cpp:218] Iteration 15300 (0.361485 iter/s, 276.637s/100 iters), loss = 2.44233
I0112 08:43:55.787114 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0112 08:43:55.787145 20429 solver.cpp:238]     Train net output #1: loss = 2.44233 (* 1 = 2.44233 loss)
I0112 08:43:55.787158 20429 sgd_solver.cpp:105] Iteration 15300, lr = 1e-07
I0112 08:48:32.418179 20429 solver.cpp:218] Iteration 15400 (0.361506 iter/s, 276.621s/100 iters), loss = 2.14617
I0112 08:48:32.418575 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.755
I0112 08:48:32.418612 20429 solver.cpp:238]     Train net output #1: loss = 2.14617 (* 1 = 2.14617 loss)
I0112 08:48:32.418624 20429 sgd_solver.cpp:105] Iteration 15400, lr = 1e-07
I0112 08:53:28.618156 20429 solver.cpp:218] Iteration 15500 (0.337623 iter/s, 296.188s/100 iters), loss = 2.5376
I0112 08:53:28.618592 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.705
I0112 08:53:28.618630 20429 solver.cpp:238]     Train net output #1: loss = 2.5376 (* 1 = 2.5376 loss)
I0112 08:53:28.618643 20429 sgd_solver.cpp:105] Iteration 15500, lr = 1e-07
I0112 08:58:17.206761 20429 solver.cpp:218] Iteration 15600 (0.346528 iter/s, 288.577s/100 iters), loss = 2.48595
I0112 08:58:17.207144 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0112 08:58:17.207201 20429 solver.cpp:238]     Train net output #1: loss = 2.48595 (* 1 = 2.48595 loss)
I0112 08:58:17.207212 20429 sgd_solver.cpp:105] Iteration 15600, lr = 1e-07
I0112 09:03:03.171396 20429 solver.cpp:218] Iteration 15700 (0.349704 iter/s, 285.956s/100 iters), loss = 2.41359
I0112 09:03:03.171844 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.725
I0112 09:03:03.171890 20429 solver.cpp:238]     Train net output #1: loss = 2.41359 (* 1 = 2.41359 loss)
I0112 09:03:03.171903 20429 sgd_solver.cpp:105] Iteration 15700, lr = 1e-07
I0112 09:08:02.569929 20429 solver.cpp:218] Iteration 15800 (0.334011 iter/s, 299.391s/100 iters), loss = 2.52936
I0112 09:08:02.570338 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.695
I0112 09:08:02.570395 20429 solver.cpp:238]     Train net output #1: loss = 2.52936 (* 1 = 2.52936 loss)
I0112 09:08:02.570408 20429 sgd_solver.cpp:105] Iteration 15800, lr = 1e-07
I0112 09:12:51.374512 20429 solver.cpp:218] Iteration 15900 (0.346265 iter/s, 288.796s/100 iters), loss = 2.39455
I0112 09:12:51.374848 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.73
I0112 09:12:51.374898 20429 solver.cpp:238]     Train net output #1: loss = 2.39455 (* 1 = 2.39455 loss)
I0112 09:12:51.374912 20429 sgd_solver.cpp:105] Iteration 15900, lr = 1e-07
I0112 09:17:55.903187 20429 solver.cpp:450] Snapshotting to binary proto file ../other_model/alexnet_a4w8_iter_16000.caffemodel
I0112 09:18:03.985013 20429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../other_model/alexnet_a4w8_iter_16000.solverstate
I0112 09:18:08.787195 20429 solver.cpp:331] Iteration 16000, Testing net (#0)
I0112 09:18:08.787286 20429 net.cpp:676] Ignoring source layer accuracy_5_TRAIN
I0112 09:27:50.562749 20435 data_layer.cpp:73] Restarting data prefetching from start.
I0112 09:27:59.020757 20429 solver.cpp:400]     Test net output #0: accuracy = 0.47056
I0112 09:27:59.020828 20429 solver.cpp:400]     Test net output #1: accuracy_5 = 0.71384
I0112 09:27:59.020850 20429 solver.cpp:400]     Test net output #2: loss = 2.4045 (* 1 = 2.4045 loss)
I0112 09:28:01.668998 20429 solver.cpp:218] Iteration 16000 (0.109858 iter/s, 910.263s/100 iters), loss = 2.63876
I0112 09:28:01.669103 20429 solver.cpp:238]     Train net output #0: accuracy_5_TRAIN = 0.7
I0112 09:28:01.669126 20429 solver.cpp:238]     Train net output #1: loss = 2.63876 (* 1 = 2.63876 loss)
I0112 09:28:01.669148 20429 sgd_solver.cpp:105] Iteration 16000, lr = 1e-07
  C-c C-cI0112 09:28:22.951084 20429 solver.cpp:450] Snapshotting to binary proto file ../other_model/alexnet_a4w8_iter_16009.caffemodel
I0112 09:28:30.862440 20429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../other_model/alexnet_a4w8_iter_16009.solverstate
I0112 09:28:36.707770 20429 solver.cpp:295] Optimization stopped early.
I0112 09:28:36.707852 20429 caffe.cpp:259] Optimization Done.