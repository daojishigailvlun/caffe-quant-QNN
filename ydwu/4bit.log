I1220 11:13:28.407239  4963 caffe.cpp:284] Use CPU.
I1220 11:13:30.490679  4963 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1220 11:13:30.491122  4963 net.cpp:51] Initializing net from parameters: 
state {
  phase: TEST
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/ydwu/database/imagenet/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/ydwu/database/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_val_lmdb/"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "quantized_data"
  type: "Quantization"
  bottom: "data"
  top: "quantized_data"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -129.93526
    range: 160.78912
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "quantized_data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv1"
  top: "quantized_conv1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -2732.7351
    range: 2531.042
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "quantized_conv1"
  top: "relu1"
}
layer {
  name: "quantized_relu1"
  type: "Quantization"
  bottom: "relu1"
  top: "quantized_relu1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 2531.042
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "quantized_relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool1"
  type: "Quantization"
  bottom: "pool1"
  top: "quantized_pool1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 2531.042
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "quantized_pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "quantized_norm1"
  type: "Quantization"
  bottom: "norm1"
  top: "quantized_norm1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 138.70576
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "quantized_norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv2"
  type: "Quantization"
  bottom: "conv2"
  top: "quantized_conv2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -647.61908
    range: 492.23111
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "quantized_conv2"
  top: "relu2"
}
layer {
  name: "quantized_relu2"
  type: "Quantization"
  bottom: "relu2"
  top: "quantized_relu2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 492.23111
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "quantized_relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool2"
  type: "Quantization"
  bottom: "pool2"
  top: "quantized_pool2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 492.23111
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "quantized_pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "quantized_norm2"
  type: "Quantization"
  bottom: "norm2"
  top: "quantized_norm2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 138.72636
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "quantized_norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv3"
  type: "Quantization"
  bottom: "conv3"
  top: "quantized_conv3"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -389.56516
    range: 331.98178
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "quantized_conv3"
  top: "relu3"
}
layer {
  name: "quantized_relu3"
  type: "Quantization"
  bottom: "relu3"
  top: "quantized_relu3"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 331.98178
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "quantized_relu3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv4"
  type: "Quantization"
  bottom: "conv4"
  top: "quantized_conv4"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -231.20598
    range: 283.87189
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "quantized_conv4"
  top: "relu4"
}
layer {
  name: "quantized_relu4"
  type: "Quantization"
  bottom: "relu4"
  top: "quantized_relu4"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 283.87189
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "quantized_relu4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv5"
  type: "Quantization"
  bottom: "conv5"
  top: "quantized_conv5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -172.60567
    range: 272.57315
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "quantized_conv5"
  top: "relu5"
}
layer {
  name: "quantized_relu5"
  type: "Quantization"
  bottom: "relu5"
  top: "quantized_relu5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 272.57315
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "quantized_relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool5"
  type: "Quantization"
  bottom: "pool5"
  top: "quantized_pool5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 272.57315
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "quantized_pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "quantized_fc6"
  type: "Quantization"
  bottom: "fc6"
  top: "quantized_fc6"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -114.02487
    range: 58.541733
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "quantized_fc6"
  top: "relu6"
}
layer {
  name: "quantized_relu6"
  type: "Quantization"
  bottom: "relu6"
  top: "quantized_relu6"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 58.541733
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "quantized_relu6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "quantized_fc7"
  type: "Quantization"
  bottom: "fc7"
  top: "quantized_fc7"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -26.019335
    range: 19.125257
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "quantized_fc7"
  top: "relu7"
}
layer {
  name: "quantized_relu7"
  type: "Quantization"
  bottom: "relu7"
  top: "quantized_relu7"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 19.125257
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "quantized_relu7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
  }
}
layer {
  name: "quantized_fc8"
  type: "Quantization"
  bottom: "fc8"
  top: "quantized_fc8"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -9.849781
    range: 45.688446
  }
}
layer {
  name: "probs"
  type: "Softmax"
  bottom: "quantized_fc8"
  top: "probs"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "acc_top1"
  type: "Accuracy"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "acc_top1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "acc_top5"
  type: "Accuracy"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "acc_top5"
  accuracy_param {
    top_k: 5
  }
}
I1220 11:13:30.491379  4963 layer_factory.hpp:77] Creating layer data
I1220 11:13:30.491503  4963 db_lmdb.cpp:35] Opened lmdb /home/ydwu/database/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_val_lmdb/
I1220 11:13:30.491541  4963 net.cpp:84] Creating Layer data
I1220 11:13:30.491557  4963 net.cpp:380] data -> data
I1220 11:13:30.491595  4963 net.cpp:380] data -> label
I1220 11:13:30.491618  4963 data_transformer.cpp:25] Loading mean file from: /home/ydwu/database/imagenet/ilsvrc12/imagenet_mean.binaryproto
I1220 11:13:30.494002  4963 data_layer.cpp:45] output data size: 50,3,227,227
I1220 11:13:30.526677  4963 net.cpp:122] Setting up data
I1220 11:13:30.526746  4963 net.cpp:129] Top shape: 50 3 227 227 (7729350)
I1220 11:13:30.526757  4963 net.cpp:129] Top shape: 50 (50)
I1220 11:13:30.526764  4963 net.cpp:137] Memory required for data: 30917600
I1220 11:13:30.526779  4963 layer_factory.hpp:77] Creating layer label_data_1_split
I1220 11:13:30.526803  4963 net.cpp:84] Creating Layer label_data_1_split
I1220 11:13:30.526813  4963 net.cpp:406] label_data_1_split <- label
I1220 11:13:30.526834  4963 net.cpp:380] label_data_1_split -> label_data_1_split_0
I1220 11:13:30.526854  4963 net.cpp:380] label_data_1_split -> label_data_1_split_1
I1220 11:13:30.526865  4963 net.cpp:380] label_data_1_split -> label_data_1_split_2
I1220 11:13:30.526883  4963 net.cpp:122] Setting up label_data_1_split
I1220 11:13:30.526893  4963 net.cpp:129] Top shape: 50 (50)
I1220 11:13:30.526901  4963 net.cpp:129] Top shape: 50 (50)
I1220 11:13:30.526909  4963 net.cpp:129] Top shape: 50 (50)
I1220 11:13:30.526916  4963 net.cpp:137] Memory required for data: 30918200
I1220 11:13:30.526922  4963 layer_factory.hpp:77] Creating layer quantized_data
I1220 11:13:30.526937  4963 net.cpp:84] Creating Layer quantized_data
I1220 11:13:30.526989  4963 net.cpp:406] quantized_data <- data
I1220 11:13:30.527003  4963 net.cpp:380] quantized_data -> quantized_data
I1220 11:13:30.527020  4963 net.cpp:122] Setting up quantized_data
I1220 11:13:30.527030  4963 net.cpp:129] Top shape: 50 3 227 227 (7729350)
I1220 11:13:30.527037  4963 net.cpp:137] Memory required for data: 61835600
I1220 11:13:30.527043  4963 layer_factory.hpp:77] Creating layer conv1
I1220 11:13:30.527068  4963 net.cpp:84] Creating Layer conv1
I1220 11:13:30.527076  4963 net.cpp:406] conv1 <- quantized_data
I1220 11:13:30.527087  4963 net.cpp:380] conv1 -> conv1
I1220 11:13:30.528208  4963 net.cpp:122] Setting up conv1
I1220 11:13:30.528223  4963 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I1220 11:13:30.528229  4963 net.cpp:137] Memory required for data: 119915600
I1220 11:13:30.528251  4963 layer_factory.hpp:77] Creating layer quantized_conv1
I1220 11:13:30.528264  4963 net.cpp:84] Creating Layer quantized_conv1
I1220 11:13:30.528271  4963 net.cpp:406] quantized_conv1 <- conv1
I1220 11:13:30.528281  4963 net.cpp:380] quantized_conv1 -> quantized_conv1
I1220 11:13:30.528292  4963 net.cpp:122] Setting up quantized_conv1
I1220 11:13:30.528301  4963 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I1220 11:13:30.528307  4963 net.cpp:137] Memory required for data: 177995600
I1220 11:13:30.528313  4963 layer_factory.hpp:77] Creating layer relu1
I1220 11:13:30.528324  4963 net.cpp:84] Creating Layer relu1
I1220 11:13:30.528332  4963 net.cpp:406] relu1 <- quantized_conv1
I1220 11:13:30.528342  4963 net.cpp:380] relu1 -> relu1
I1220 11:13:30.528352  4963 net.cpp:122] Setting up relu1
I1220 11:13:30.528360  4963 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I1220 11:13:30.528367  4963 net.cpp:137] Memory required for data: 236075600
I1220 11:13:30.528373  4963 layer_factory.hpp:77] Creating layer quantized_relu1
I1220 11:13:30.528383  4963 net.cpp:84] Creating Layer quantized_relu1
I1220 11:13:30.528389  4963 net.cpp:406] quantized_relu1 <- relu1
I1220 11:13:30.528398  4963 net.cpp:380] quantized_relu1 -> quantized_relu1
I1220 11:13:30.528409  4963 net.cpp:122] Setting up quantized_relu1
I1220 11:13:30.528417  4963 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I1220 11:13:30.528424  4963 net.cpp:137] Memory required for data: 294155600
I1220 11:13:30.528430  4963 layer_factory.hpp:77] Creating layer pool1
I1220 11:13:30.528441  4963 net.cpp:84] Creating Layer pool1
I1220 11:13:30.528447  4963 net.cpp:406] pool1 <- quantized_relu1
I1220 11:13:30.528456  4963 net.cpp:380] pool1 -> pool1
I1220 11:13:30.528481  4963 net.cpp:122] Setting up pool1
I1220 11:13:30.528491  4963 net.cpp:129] Top shape: 50 96 27 27 (3499200)
I1220 11:13:30.528497  4963 net.cpp:137] Memory required for data: 308152400
I1220 11:13:30.528504  4963 layer_factory.hpp:77] Creating layer quantized_pool1
I1220 11:13:30.528515  4963 net.cpp:84] Creating Layer quantized_pool1
I1220 11:13:30.528522  4963 net.cpp:406] quantized_pool1 <- pool1
I1220 11:13:30.528532  4963 net.cpp:380] quantized_pool1 -> quantized_pool1
I1220 11:13:30.528542  4963 net.cpp:122] Setting up quantized_pool1
I1220 11:13:30.528549  4963 net.cpp:129] Top shape: 50 96 27 27 (3499200)
I1220 11:13:30.528556  4963 net.cpp:137] Memory required for data: 322149200
I1220 11:13:30.528563  4963 layer_factory.hpp:77] Creating layer norm1
I1220 11:13:30.528573  4963 net.cpp:84] Creating Layer norm1
I1220 11:13:30.528580  4963 net.cpp:406] norm1 <- quantized_pool1
I1220 11:13:30.528589  4963 net.cpp:380] norm1 -> norm1
I1220 11:13:30.528605  4963 net.cpp:122] Setting up norm1
I1220 11:13:30.528615  4963 net.cpp:129] Top shape: 50 96 27 27 (3499200)
I1220 11:13:30.528621  4963 net.cpp:137] Memory required for data: 336146000
I1220 11:13:30.528628  4963 layer_factory.hpp:77] Creating layer quantized_norm1
I1220 11:13:30.528637  4963 net.cpp:84] Creating Layer quantized_norm1
I1220 11:13:30.528645  4963 net.cpp:406] quantized_norm1 <- norm1
I1220 11:13:30.528653  4963 net.cpp:380] quantized_norm1 -> quantized_norm1
I1220 11:13:30.528669  4963 net.cpp:122] Setting up quantized_norm1
I1220 11:13:30.528687  4963 net.cpp:129] Top shape: 50 96 27 27 (3499200)
I1220 11:13:30.528694  4963 net.cpp:137] Memory required for data: 350142800
I1220 11:13:30.528702  4963 layer_factory.hpp:77] Creating layer conv2
I1220 11:13:30.528715  4963 net.cpp:84] Creating Layer conv2
I1220 11:13:30.528723  4963 net.cpp:406] conv2 <- quantized_norm1
I1220 11:13:30.528733  4963 net.cpp:380] conv2 -> conv2
I1220 11:13:30.538014  4963 net.cpp:122] Setting up conv2
I1220 11:13:30.538031  4963 net.cpp:129] Top shape: 50 256 27 27 (9331200)
I1220 11:13:30.538038  4963 net.cpp:137] Memory required for data: 387467600
I1220 11:13:30.538049  4963 layer_factory.hpp:77] Creating layer quantized_conv2
I1220 11:13:30.538059  4963 net.cpp:84] Creating Layer quantized_conv2
I1220 11:13:30.538066  4963 net.cpp:406] quantized_conv2 <- conv2
I1220 11:13:30.538079  4963 net.cpp:380] quantized_conv2 -> quantized_conv2
I1220 11:13:30.538089  4963 net.cpp:122] Setting up quantized_conv2
I1220 11:13:30.538099  4963 net.cpp:129] Top shape: 50 256 27 27 (9331200)
I1220 11:13:30.538105  4963 net.cpp:137] Memory required for data: 424792400
I1220 11:13:30.538111  4963 layer_factory.hpp:77] Creating layer relu2
I1220 11:13:30.538120  4963 net.cpp:84] Creating Layer relu2
I1220 11:13:30.538126  4963 net.cpp:406] relu2 <- quantized_conv2
I1220 11:13:30.538137  4963 net.cpp:380] relu2 -> relu2
I1220 11:13:30.538148  4963 net.cpp:122] Setting up relu2
I1220 11:13:30.538157  4963 net.cpp:129] Top shape: 50 256 27 27 (9331200)
I1220 11:13:30.538164  4963 net.cpp:137] Memory required for data: 462117200
I1220 11:13:30.538170  4963 layer_factory.hpp:77] Creating layer quantized_relu2
I1220 11:13:30.538179  4963 net.cpp:84] Creating Layer quantized_relu2
I1220 11:13:30.538187  4963 net.cpp:406] quantized_relu2 <- relu2
I1220 11:13:30.538197  4963 net.cpp:380] quantized_relu2 -> quantized_relu2
I1220 11:13:30.538208  4963 net.cpp:122] Setting up quantized_relu2
I1220 11:13:30.538216  4963 net.cpp:129] Top shape: 50 256 27 27 (9331200)
I1220 11:13:30.538223  4963 net.cpp:137] Memory required for data: 499442000
I1220 11:13:30.538229  4963 layer_factory.hpp:77] Creating layer pool2
I1220 11:13:30.538239  4963 net.cpp:84] Creating Layer pool2
I1220 11:13:30.538245  4963 net.cpp:406] pool2 <- quantized_relu2
I1220 11:13:30.538256  4963 net.cpp:380] pool2 -> pool2
I1220 11:13:30.538267  4963 net.cpp:122] Setting up pool2
I1220 11:13:30.538276  4963 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I1220 11:13:30.538283  4963 net.cpp:137] Memory required for data: 508094800
I1220 11:13:30.538290  4963 layer_factory.hpp:77] Creating layer quantized_pool2
I1220 11:13:30.538300  4963 net.cpp:84] Creating Layer quantized_pool2
I1220 11:13:30.538306  4963 net.cpp:406] quantized_pool2 <- pool2
I1220 11:13:30.538318  4963 net.cpp:380] quantized_pool2 -> quantized_pool2
I1220 11:13:30.538329  4963 net.cpp:122] Setting up quantized_pool2
I1220 11:13:30.538338  4963 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I1220 11:13:30.538344  4963 net.cpp:137] Memory required for data: 516747600
I1220 11:13:30.538352  4963 layer_factory.hpp:77] Creating layer norm2
I1220 11:13:30.538362  4963 net.cpp:84] Creating Layer norm2
I1220 11:13:30.538369  4963 net.cpp:406] norm2 <- quantized_pool2
I1220 11:13:30.538378  4963 net.cpp:380] norm2 -> norm2
I1220 11:13:30.538388  4963 net.cpp:122] Setting up norm2
I1220 11:13:30.538396  4963 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I1220 11:13:30.538403  4963 net.cpp:137] Memory required for data: 525400400
I1220 11:13:30.538409  4963 layer_factory.hpp:77] Creating layer quantized_norm2
I1220 11:13:30.538420  4963 net.cpp:84] Creating Layer quantized_norm2
I1220 11:13:30.538429  4963 net.cpp:406] quantized_norm2 <- norm2
I1220 11:13:30.538436  4963 net.cpp:380] quantized_norm2 -> quantized_norm2
I1220 11:13:30.538445  4963 net.cpp:122] Setting up quantized_norm2
I1220 11:13:30.538455  4963 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I1220 11:13:30.538460  4963 net.cpp:137] Memory required for data: 534053200
I1220 11:13:30.538475  4963 layer_factory.hpp:77] Creating layer conv3
I1220 11:13:30.538503  4963 net.cpp:84] Creating Layer conv3
I1220 11:13:30.538512  4963 net.cpp:406] conv3 <- quantized_norm2
I1220 11:13:30.538524  4963 net.cpp:380] conv3 -> conv3
I1220 11:13:30.565374  4963 net.cpp:122] Setting up conv3
I1220 11:13:30.565397  4963 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I1220 11:13:30.565404  4963 net.cpp:137] Memory required for data: 547032400
I1220 11:13:30.565416  4963 layer_factory.hpp:77] Creating layer quantized_conv3
I1220 11:13:30.565426  4963 net.cpp:84] Creating Layer quantized_conv3
I1220 11:13:30.565434  4963 net.cpp:406] quantized_conv3 <- conv3
I1220 11:13:30.565443  4963 net.cpp:380] quantized_conv3 -> quantized_conv3
I1220 11:13:30.565454  4963 net.cpp:122] Setting up quantized_conv3
I1220 11:13:30.565462  4963 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I1220 11:13:30.565469  4963 net.cpp:137] Memory required for data: 560011600
I1220 11:13:30.565475  4963 layer_factory.hpp:77] Creating layer relu3
I1220 11:13:30.565487  4963 net.cpp:84] Creating Layer relu3
I1220 11:13:30.565495  4963 net.cpp:406] relu3 <- quantized_conv3
I1220 11:13:30.565502  4963 net.cpp:380] relu3 -> relu3
I1220 11:13:30.565512  4963 net.cpp:122] Setting up relu3
I1220 11:13:30.565521  4963 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I1220 11:13:30.565527  4963 net.cpp:137] Memory required for data: 572990800
I1220 11:13:30.565533  4963 layer_factory.hpp:77] Creating layer quantized_relu3
I1220 11:13:30.565546  4963 net.cpp:84] Creating Layer quantized_relu3
I1220 11:13:30.565552  4963 net.cpp:406] quantized_relu3 <- relu3
I1220 11:13:30.565560  4963 net.cpp:380] quantized_relu3 -> quantized_relu3
I1220 11:13:30.565570  4963 net.cpp:122] Setting up quantized_relu3
I1220 11:13:30.565579  4963 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I1220 11:13:30.565585  4963 net.cpp:137] Memory required for data: 585970000
I1220 11:13:30.565593  4963 layer_factory.hpp:77] Creating layer conv4
I1220 11:13:30.565606  4963 net.cpp:84] Creating Layer conv4
I1220 11:13:30.565614  4963 net.cpp:406] conv4 <- quantized_relu3
I1220 11:13:30.565625  4963 net.cpp:380] conv4 -> conv4
I1220 11:13:30.586156  4963 net.cpp:122] Setting up conv4
I1220 11:13:30.586177  4963 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I1220 11:13:30.586184  4963 net.cpp:137] Memory required for data: 598949200
I1220 11:13:30.586194  4963 layer_factory.hpp:77] Creating layer quantized_conv4
I1220 11:13:30.586205  4963 net.cpp:84] Creating Layer quantized_conv4
I1220 11:13:30.586211  4963 net.cpp:406] quantized_conv4 <- conv4
I1220 11:13:30.586225  4963 net.cpp:380] quantized_conv4 -> quantized_conv4
I1220 11:13:30.586237  4963 net.cpp:122] Setting up quantized_conv4
I1220 11:13:30.586246  4963 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I1220 11:13:30.586252  4963 net.cpp:137] Memory required for data: 611928400
I1220 11:13:30.586259  4963 layer_factory.hpp:77] Creating layer relu4
I1220 11:13:30.586267  4963 net.cpp:84] Creating Layer relu4
I1220 11:13:30.586277  4963 net.cpp:406] relu4 <- quantized_conv4
I1220 11:13:30.586285  4963 net.cpp:380] relu4 -> relu4
I1220 11:13:30.586295  4963 net.cpp:122] Setting up relu4
I1220 11:13:30.586304  4963 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I1220 11:13:30.586310  4963 net.cpp:137] Memory required for data: 624907600
I1220 11:13:30.586316  4963 layer_factory.hpp:77] Creating layer quantized_relu4
I1220 11:13:30.586325  4963 net.cpp:84] Creating Layer quantized_relu4
I1220 11:13:30.586333  4963 net.cpp:406] quantized_relu4 <- relu4
I1220 11:13:30.586344  4963 net.cpp:380] quantized_relu4 -> quantized_relu4
I1220 11:13:30.586354  4963 net.cpp:122] Setting up quantized_relu4
I1220 11:13:30.586362  4963 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I1220 11:13:30.586369  4963 net.cpp:137] Memory required for data: 637886800
I1220 11:13:30.586375  4963 layer_factory.hpp:77] Creating layer conv5
I1220 11:13:30.586390  4963 net.cpp:84] Creating Layer conv5
I1220 11:13:30.586397  4963 net.cpp:406] conv5 <- quantized_relu4
I1220 11:13:30.586423  4963 net.cpp:380] conv5 -> conv5
I1220 11:13:30.600118  4963 net.cpp:122] Setting up conv5
I1220 11:13:30.600137  4963 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I1220 11:13:30.600143  4963 net.cpp:137] Memory required for data: 646539600
I1220 11:13:30.600158  4963 layer_factory.hpp:77] Creating layer quantized_conv5
I1220 11:13:30.600169  4963 net.cpp:84] Creating Layer quantized_conv5
I1220 11:13:30.600177  4963 net.cpp:406] quantized_conv5 <- conv5
I1220 11:13:30.600188  4963 net.cpp:380] quantized_conv5 -> quantized_conv5
I1220 11:13:30.600199  4963 net.cpp:122] Setting up quantized_conv5
I1220 11:13:30.600209  4963 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I1220 11:13:30.600215  4963 net.cpp:137] Memory required for data: 655192400
I1220 11:13:30.600221  4963 layer_factory.hpp:77] Creating layer relu5
I1220 11:13:30.600230  4963 net.cpp:84] Creating Layer relu5
I1220 11:13:30.600237  4963 net.cpp:406] relu5 <- quantized_conv5
I1220 11:13:30.600245  4963 net.cpp:380] relu5 -> relu5
I1220 11:13:30.600260  4963 net.cpp:122] Setting up relu5
I1220 11:13:30.600270  4963 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I1220 11:13:30.600276  4963 net.cpp:137] Memory required for data: 663845200
I1220 11:13:30.600282  4963 layer_factory.hpp:77] Creating layer quantized_relu5
I1220 11:13:30.600291  4963 net.cpp:84] Creating Layer quantized_relu5
I1220 11:13:30.600298  4963 net.cpp:406] quantized_relu5 <- relu5
I1220 11:13:30.600308  4963 net.cpp:380] quantized_relu5 -> quantized_relu5
I1220 11:13:30.600319  4963 net.cpp:122] Setting up quantized_relu5
I1220 11:13:30.600327  4963 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I1220 11:13:30.600334  4963 net.cpp:137] Memory required for data: 672498000
I1220 11:13:30.600340  4963 layer_factory.hpp:77] Creating layer pool5
I1220 11:13:30.600352  4963 net.cpp:84] Creating Layer pool5
I1220 11:13:30.600358  4963 net.cpp:406] pool5 <- quantized_relu5
I1220 11:13:30.600368  4963 net.cpp:380] pool5 -> pool5
I1220 11:13:30.600383  4963 net.cpp:122] Setting up pool5
I1220 11:13:30.600391  4963 net.cpp:129] Top shape: 50 256 6 6 (460800)
I1220 11:13:30.600399  4963 net.cpp:137] Memory required for data: 674341200
I1220 11:13:30.600404  4963 layer_factory.hpp:77] Creating layer quantized_pool5
I1220 11:13:30.600419  4963 net.cpp:84] Creating Layer quantized_pool5
I1220 11:13:30.600425  4963 net.cpp:406] quantized_pool5 <- pool5
I1220 11:13:30.600435  4963 net.cpp:380] quantized_pool5 -> quantized_pool5
I1220 11:13:30.600445  4963 net.cpp:122] Setting up quantized_pool5
I1220 11:13:30.600453  4963 net.cpp:129] Top shape: 50 256 6 6 (460800)
I1220 11:13:30.600459  4963 net.cpp:137] Memory required for data: 676184400
I1220 11:13:30.600466  4963 layer_factory.hpp:77] Creating layer fc6
I1220 11:13:30.600483  4963 net.cpp:84] Creating Layer fc6
I1220 11:13:30.600492  4963 net.cpp:406] fc6 <- quantized_pool5
I1220 11:13:30.600500  4963 net.cpp:380] fc6 -> fc6
I1220 11:13:31.747041  4963 net.cpp:122] Setting up fc6
I1220 11:13:31.747103  4963 net.cpp:129] Top shape: 50 4096 (204800)
I1220 11:13:31.747112  4963 net.cpp:137] Memory required for data: 677003600
I1220 11:13:31.747126  4963 layer_factory.hpp:77] Creating layer quantized_fc6
I1220 11:13:31.747143  4963 net.cpp:84] Creating Layer quantized_fc6
I1220 11:13:31.747153  4963 net.cpp:406] quantized_fc6 <- fc6
I1220 11:13:31.747169  4963 net.cpp:380] quantized_fc6 -> quantized_fc6
I1220 11:13:31.747187  4963 net.cpp:122] Setting up quantized_fc6
I1220 11:13:31.747196  4963 net.cpp:129] Top shape: 50 4096 (204800)
I1220 11:13:31.747202  4963 net.cpp:137] Memory required for data: 677822800
I1220 11:13:31.747210  4963 layer_factory.hpp:77] Creating layer relu6
I1220 11:13:31.747218  4963 net.cpp:84] Creating Layer relu6
I1220 11:13:31.747225  4963 net.cpp:406] relu6 <- quantized_fc6
I1220 11:13:31.747234  4963 net.cpp:380] relu6 -> relu6
I1220 11:13:31.747244  4963 net.cpp:122] Setting up relu6
I1220 11:13:31.747253  4963 net.cpp:129] Top shape: 50 4096 (204800)
I1220 11:13:31.747273  4963 net.cpp:137] Memory required for data: 678642000
I1220 11:13:31.747297  4963 layer_factory.hpp:77] Creating layer quantized_relu6
I1220 11:13:31.747308  4963 net.cpp:84] Creating Layer quantized_relu6
I1220 11:13:31.747314  4963 net.cpp:406] quantized_relu6 <- relu6
I1220 11:13:31.747323  4963 net.cpp:380] quantized_relu6 -> quantized_relu6
I1220 11:13:31.747334  4963 net.cpp:122] Setting up quantized_relu6
I1220 11:13:31.747342  4963 net.cpp:129] Top shape: 50 4096 (204800)
I1220 11:13:31.747349  4963 net.cpp:137] Memory required for data: 679461200
I1220 11:13:31.747355  4963 layer_factory.hpp:77] Creating layer fc7
I1220 11:13:31.747371  4963 net.cpp:84] Creating Layer fc7
I1220 11:13:31.747378  4963 net.cpp:406] fc7 <- quantized_relu6
I1220 11:13:31.747388  4963 net.cpp:380] fc7 -> fc7
I1220 11:13:32.241705  4963 net.cpp:122] Setting up fc7
I1220 11:13:32.241765  4963 net.cpp:129] Top shape: 50 4096 (204800)
I1220 11:13:32.241773  4963 net.cpp:137] Memory required for data: 680280400
I1220 11:13:32.241787  4963 layer_factory.hpp:77] Creating layer quantized_fc7
I1220 11:13:32.241807  4963 net.cpp:84] Creating Layer quantized_fc7
I1220 11:13:32.241817  4963 net.cpp:406] quantized_fc7 <- fc7
I1220 11:13:32.241830  4963 net.cpp:380] quantized_fc7 -> quantized_fc7
I1220 11:13:32.241847  4963 net.cpp:122] Setting up quantized_fc7
I1220 11:13:32.241855  4963 net.cpp:129] Top shape: 50 4096 (204800)
I1220 11:13:32.241861  4963 net.cpp:137] Memory required for data: 681099600
I1220 11:13:32.241868  4963 layer_factory.hpp:77] Creating layer relu7
I1220 11:13:32.241876  4963 net.cpp:84] Creating Layer relu7
I1220 11:13:32.241883  4963 net.cpp:406] relu7 <- quantized_fc7
I1220 11:13:32.241891  4963 net.cpp:380] relu7 -> relu7
I1220 11:13:32.241901  4963 net.cpp:122] Setting up relu7
I1220 11:13:32.241909  4963 net.cpp:129] Top shape: 50 4096 (204800)
I1220 11:13:32.241915  4963 net.cpp:137] Memory required for data: 681918800
I1220 11:13:32.241920  4963 layer_factory.hpp:77] Creating layer quantized_relu7
I1220 11:13:32.241930  4963 net.cpp:84] Creating Layer quantized_relu7
I1220 11:13:32.241935  4963 net.cpp:406] quantized_relu7 <- relu7
I1220 11:13:32.241947  4963 net.cpp:380] quantized_relu7 -> quantized_relu7
I1220 11:13:32.241957  4963 net.cpp:122] Setting up quantized_relu7
I1220 11:13:32.241966  4963 net.cpp:129] Top shape: 50 4096 (204800)
I1220 11:13:32.241971  4963 net.cpp:137] Memory required for data: 682738000
I1220 11:13:32.241977  4963 layer_factory.hpp:77] Creating layer fc8
I1220 11:13:32.241988  4963 net.cpp:84] Creating Layer fc8
I1220 11:13:32.241996  4963 net.cpp:406] fc8 <- quantized_relu7
I1220 11:13:32.242003  4963 net.cpp:380] fc8 -> fc8
I1220 11:13:32.248673  4963 net.cpp:122] Setting up fc8
I1220 11:13:32.248739  4963 net.cpp:129] Top shape: 50 1000 (50000)
I1220 11:13:32.248746  4963 net.cpp:137] Memory required for data: 682938000
I1220 11:13:32.248760  4963 layer_factory.hpp:77] Creating layer quantized_fc8
I1220 11:13:32.248780  4963 net.cpp:84] Creating Layer quantized_fc8
I1220 11:13:32.248788  4963 net.cpp:406] quantized_fc8 <- fc8
I1220 11:13:32.248801  4963 net.cpp:380] quantized_fc8 -> quantized_fc8
I1220 11:13:32.248818  4963 net.cpp:122] Setting up quantized_fc8
I1220 11:13:32.248826  4963 net.cpp:129] Top shape: 50 1000 (50000)
I1220 11:13:32.248832  4963 net.cpp:137] Memory required for data: 683138000
I1220 11:13:32.248838  4963 layer_factory.hpp:77] Creating layer quantized_fc8_quantized_fc8_0_split
I1220 11:13:32.248850  4963 net.cpp:84] Creating Layer quantized_fc8_quantized_fc8_0_split
I1220 11:13:32.248857  4963 net.cpp:406] quantized_fc8_quantized_fc8_0_split <- quantized_fc8
I1220 11:13:32.248865  4963 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_0
I1220 11:13:32.248874  4963 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_1
I1220 11:13:32.248884  4963 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_2
I1220 11:13:32.248893  4963 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_3
I1220 11:13:32.248939  4963 net.cpp:122] Setting up quantized_fc8_quantized_fc8_0_split
I1220 11:13:32.248950  4963 net.cpp:129] Top shape: 50 1000 (50000)
I1220 11:13:32.248956  4963 net.cpp:129] Top shape: 50 1000 (50000)
I1220 11:13:32.248963  4963 net.cpp:129] Top shape: 50 1000 (50000)
I1220 11:13:32.248970  4963 net.cpp:129] Top shape: 50 1000 (50000)
I1220 11:13:32.248975  4963 net.cpp:137] Memory required for data: 683938000
I1220 11:13:32.248981  4963 layer_factory.hpp:77] Creating layer probs
I1220 11:13:32.248991  4963 net.cpp:84] Creating Layer probs
I1220 11:13:32.248998  4963 net.cpp:406] probs <- quantized_fc8_quantized_fc8_0_split_0
I1220 11:13:32.249008  4963 net.cpp:380] probs -> probs
I1220 11:13:32.249033  4963 net.cpp:122] Setting up probs
I1220 11:13:32.249042  4963 net.cpp:129] Top shape: 50 1000 (50000)
I1220 11:13:32.249048  4963 net.cpp:137] Memory required for data: 684138000
I1220 11:13:32.249055  4963 layer_factory.hpp:77] Creating layer loss
I1220 11:13:32.249066  4963 net.cpp:84] Creating Layer loss
I1220 11:13:32.249073  4963 net.cpp:406] loss <- quantized_fc8_quantized_fc8_0_split_1
I1220 11:13:32.249079  4963 net.cpp:406] loss <- label_data_1_split_0
I1220 11:13:32.249090  4963 net.cpp:380] loss -> loss
I1220 11:13:32.249110  4963 layer_factory.hpp:77] Creating layer loss
I1220 11:13:32.249264  4963 net.cpp:122] Setting up loss
I1220 11:13:32.249276  4963 net.cpp:129] Top shape: (1)
I1220 11:13:32.249284  4963 net.cpp:132]     with loss weight 1
I1220 11:13:32.249315  4963 net.cpp:137] Memory required for data: 684138004
I1220 11:13:32.249322  4963 layer_factory.hpp:77] Creating layer acc_top1
I1220 11:13:32.249335  4963 net.cpp:84] Creating Layer acc_top1
I1220 11:13:32.249341  4963 net.cpp:406] acc_top1 <- quantized_fc8_quantized_fc8_0_split_2
I1220 11:13:32.249349  4963 net.cpp:406] acc_top1 <- label_data_1_split_1
I1220 11:13:32.249357  4963 net.cpp:380] acc_top1 -> acc_top1
I1220 11:13:32.249372  4963 net.cpp:122] Setting up acc_top1
I1220 11:13:32.249380  4963 net.cpp:129] Top shape: (1)
I1220 11:13:32.249387  4963 net.cpp:137] Memory required for data: 684138008
I1220 11:13:32.249392  4963 layer_factory.hpp:77] Creating layer acc_top5
I1220 11:13:32.249403  4963 net.cpp:84] Creating Layer acc_top5
I1220 11:13:32.249411  4963 net.cpp:406] acc_top5 <- quantized_fc8_quantized_fc8_0_split_3
I1220 11:13:32.249418  4963 net.cpp:406] acc_top5 <- label_data_1_split_2
I1220 11:13:32.249426  4963 net.cpp:380] acc_top5 -> acc_top5
I1220 11:13:32.249439  4963 net.cpp:122] Setting up acc_top5
I1220 11:13:32.249446  4963 net.cpp:129] Top shape: (1)
I1220 11:13:32.249452  4963 net.cpp:137] Memory required for data: 684138012
I1220 11:13:32.249459  4963 net.cpp:200] acc_top5 does not need backward computation.
I1220 11:13:32.249465  4963 net.cpp:200] acc_top1 does not need backward computation.
I1220 11:13:32.249472  4963 net.cpp:198] loss needs backward computation.
I1220 11:13:32.249480  4963 net.cpp:200] probs does not need backward computation.
I1220 11:13:32.249485  4963 net.cpp:198] quantized_fc8_quantized_fc8_0_split needs backward computation.
I1220 11:13:32.249492  4963 net.cpp:198] quantized_fc8 needs backward computation.
I1220 11:13:32.249498  4963 net.cpp:198] fc8 needs backward computation.
I1220 11:13:32.249505  4963 net.cpp:200] quantized_relu7 does not need backward computation.
I1220 11:13:32.249511  4963 net.cpp:200] relu7 does not need backward computation.
I1220 11:13:32.249518  4963 net.cpp:200] quantized_fc7 does not need backward computation.
I1220 11:13:32.249524  4963 net.cpp:200] fc7 does not need backward computation.
I1220 11:13:32.249531  4963 net.cpp:200] quantized_relu6 does not need backward computation.
I1220 11:13:32.249538  4963 net.cpp:200] relu6 does not need backward computation.
I1220 11:13:32.249544  4963 net.cpp:200] quantized_fc6 does not need backward computation.
I1220 11:13:32.249552  4963 net.cpp:200] fc6 does not need backward computation.
I1220 11:13:32.249558  4963 net.cpp:200] quantized_pool5 does not need backward computation.
I1220 11:13:32.249578  4963 net.cpp:200] pool5 does not need backward computation.
I1220 11:13:32.249586  4963 net.cpp:200] quantized_relu5 does not need backward computation.
I1220 11:13:32.249593  4963 net.cpp:200] relu5 does not need backward computation.
I1220 11:13:32.249600  4963 net.cpp:200] quantized_conv5 does not need backward computation.
I1220 11:13:32.249606  4963 net.cpp:200] conv5 does not need backward computation.
I1220 11:13:32.249614  4963 net.cpp:200] quantized_relu4 does not need backward computation.
I1220 11:13:32.249620  4963 net.cpp:200] relu4 does not need backward computation.
I1220 11:13:32.249627  4963 net.cpp:200] quantized_conv4 does not need backward computation.
I1220 11:13:32.249634  4963 net.cpp:200] conv4 does not need backward computation.
I1220 11:13:32.249640  4963 net.cpp:200] quantized_relu3 does not need backward computation.
I1220 11:13:32.249647  4963 net.cpp:200] relu3 does not need backward computation.
I1220 11:13:32.249653  4963 net.cpp:200] quantized_conv3 does not need backward computation.
I1220 11:13:32.249660  4963 net.cpp:200] conv3 does not need backward computation.
I1220 11:13:32.249667  4963 net.cpp:200] quantized_norm2 does not need backward computation.
I1220 11:13:32.249675  4963 net.cpp:200] norm2 does not need backward computation.
I1220 11:13:32.249689  4963 net.cpp:200] quantized_pool2 does not need backward computation.
I1220 11:13:32.249697  4963 net.cpp:200] pool2 does not need backward computation.
I1220 11:13:32.249704  4963 net.cpp:200] quantized_relu2 does not need backward computation.
I1220 11:13:32.249711  4963 net.cpp:200] relu2 does not need backward computation.
I1220 11:13:32.249718  4963 net.cpp:200] quantized_conv2 does not need backward computation.
I1220 11:13:32.249725  4963 net.cpp:200] conv2 does not need backward computation.
I1220 11:13:32.249732  4963 net.cpp:200] quantized_norm1 does not need backward computation.
I1220 11:13:32.249738  4963 net.cpp:200] norm1 does not need backward computation.
I1220 11:13:32.249745  4963 net.cpp:200] quantized_pool1 does not need backward computation.
I1220 11:13:32.249752  4963 net.cpp:200] pool1 does not need backward computation.
I1220 11:13:32.249758  4963 net.cpp:200] quantized_relu1 does not need backward computation.
I1220 11:13:32.249765  4963 net.cpp:200] relu1 does not need backward computation.
I1220 11:13:32.249773  4963 net.cpp:200] quantized_conv1 does not need backward computation.
I1220 11:13:32.249778  4963 net.cpp:200] conv1 does not need backward computation.
I1220 11:13:32.249785  4963 net.cpp:200] quantized_data does not need backward computation.
I1220 11:13:32.249792  4963 net.cpp:200] label_data_1_split does not need backward computation.
I1220 11:13:32.249799  4963 net.cpp:200] data does not need backward computation.
I1220 11:13:32.249809  4963 net.cpp:242] This network produces output acc_top1
I1220 11:13:32.249814  4963 net.cpp:242] This network produces output acc_top5
I1220 11:13:32.249820  4963 net.cpp:242] This network produces output loss
I1220 11:13:32.249826  4963 net.cpp:242] This network produces output probs
I1220 11:13:32.249861  4963 net.cpp:255] Network initialization done.
I1220 11:13:32.723697  4963 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: qnn_try1/bvlc_reference_caffenet.caffemodel
I1220 11:13:32.723772  4963 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1220 11:13:32.723781  4963 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1220 11:13:32.723913  4963 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: qnn_try1/bvlc_reference_caffenet.caffemodel
I1220 11:13:32.945732  4963 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1220 11:13:32.989013  4963 net.cpp:744] Ignoring source layer drop6
I1220 11:13:33.006672  4963 net.cpp:744] Ignoring source layer drop7
I1220 11:13:33.013938  4963 caffe.cpp:290] Running for 5 iterations.
./build/tools/caffe: symbol lookup error: /home/ydwu/work/QNN-Caffe/caffe/.build_release/tools/../lib/libcaffe.so.1.0.0: undefined symbol: _ZN5caffe19Weight_QuantizationIPKfEET_S3_
