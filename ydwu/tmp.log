I1220 17:54:53.044662 53398 caffe.cpp:284] Use CPU.
I1220 17:54:55.219028 53398 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1220 17:54:55.219441 53398 net.cpp:51] Initializing net from parameters: 
state {
  phase: TEST
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/ydwu/database/imagenet/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/ydwu/database/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_val_lmdb/"
    batch_size: 20
    backend: LMDB
  }
}
layer {
  name: "quantized_data"
  type: "Quantization"
  bottom: "data"
  top: "quantized_data"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -129.93526
    range: 160.78912
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "quantized_data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv1"
  top: "quantized_conv1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -2732.7351
    range: 2531.042
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "quantized_conv1"
  top: "relu1"
}
layer {
  name: "quantized_relu1"
  type: "Quantization"
  bottom: "relu1"
  top: "quantized_relu1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 2531.042
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "quantized_relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool1"
  type: "Quantization"
  bottom: "pool1"
  top: "quantized_pool1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 2531.042
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "quantized_pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "quantized_norm1"
  type: "Quantization"
  bottom: "norm1"
  top: "quantized_norm1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 138.70576
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "quantized_norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv2"
  type: "Quantization"
  bottom: "conv2"
  top: "quantized_conv2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -647.61908
    range: 492.23111
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "quantized_conv2"
  top: "relu2"
}
layer {
  name: "quantized_relu2"
  type: "Quantization"
  bottom: "relu2"
  top: "quantized_relu2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 492.23111
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "quantized_relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool2"
  type: "Quantization"
  bottom: "pool2"
  top: "quantized_pool2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 492.23111
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "quantized_pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "quantized_norm2"
  type: "Quantization"
  bottom: "norm2"
  top: "quantized_norm2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 138.72636
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "quantized_norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv3"
  type: "Quantization"
  bottom: "conv3"
  top: "quantized_conv3"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -389.56516
    range: 331.98178
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "quantized_conv3"
  top: "relu3"
}
layer {
  name: "quantized_relu3"
  type: "Quantization"
  bottom: "relu3"
  top: "quantized_relu3"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 331.98178
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "quantized_relu3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv4"
  type: "Quantization"
  bottom: "conv4"
  top: "quantized_conv4"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -231.20598
    range: 283.87189
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "quantized_conv4"
  top: "relu4"
}
layer {
  name: "quantized_relu4"
  type: "Quantization"
  bottom: "relu4"
  top: "quantized_relu4"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 283.87189
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "quantized_relu4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv5"
  type: "Quantization"
  bottom: "conv5"
  top: "quantized_conv5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -172.60567
    range: 272.57315
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "quantized_conv5"
  top: "relu5"
}
layer {
  name: "quantized_relu5"
  type: "Quantization"
  bottom: "relu5"
  top: "quantized_relu5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 272.57315
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "quantized_relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool5"
  type: "Quantization"
  bottom: "pool5"
  top: "quantized_pool5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 272.57315
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "quantized_pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "quantized_fc6"
  type: "Quantization"
  bottom: "fc6"
  top: "quantized_fc6"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -114.02487
    range: 58.541733
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "quantized_fc6"
  top: "relu6"
}
layer {
  name: "quantized_relu6"
  type: "Quantization"
  bottom: "relu6"
  top: "quantized_relu6"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 58.541733
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "quantized_relu6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "quantized_fc7"
  type: "Quantization"
  bottom: "fc7"
  top: "quantized_fc7"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -26.019335
    range: 19.125257
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "quantized_fc7"
  top: "relu7"
}
layer {
  name: "quantized_relu7"
  type: "Quantization"
  bottom: "relu7"
  top: "quantized_relu7"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 0
    range: 19.125257
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "quantized_relu7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
  }
}
layer {
  name: "quantized_fc8"
  type: "Quantization"
  bottom: "fc8"
  top: "quantized_fc8"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -9.849781
    range: 45.688446
  }
}
layer {
  name: "probs"
  type: "Softmax"
  bottom: "quantized_fc8"
  top: "probs"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "acc_top1"
  type: "Accuracy"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "acc_top1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "acc_top5"
  type: "Accuracy"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "acc_top5"
  accuracy_param {
    top_k: 5
  }
}
I1220 17:54:55.219677 53398 layer_factory.hpp:77] Creating layer data
I1220 17:54:55.219797 53398 db_lmdb.cpp:35] Opened lmdb /home/ydwu/database/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_val_lmdb/
I1220 17:54:55.219835 53398 net.cpp:84] Creating Layer data
I1220 17:54:55.219851 53398 net.cpp:380] data -> data
I1220 17:54:55.219884 53398 net.cpp:380] data -> label
I1220 17:54:55.219905 53398 data_transformer.cpp:25] Loading mean file from: /home/ydwu/database/imagenet/ilsvrc12/imagenet_mean.binaryproto
I1220 17:54:55.222087 53398 data_layer.cpp:45] output data size: 20,3,227,227
I1220 17:54:55.236745 53398 net.cpp:122] Setting up data
I1220 17:54:55.236821 53398 net.cpp:129] Top shape: 20 3 227 227 (3091740)
I1220 17:54:55.236832 53398 net.cpp:129] Top shape: 20 (20)
I1220 17:54:55.236840 53398 net.cpp:137] Memory required for data: 12367040
I1220 17:54:55.236855 53398 layer_factory.hpp:77] Creating layer label_data_1_split
I1220 17:54:55.236888 53398 net.cpp:84] Creating Layer label_data_1_split
I1220 17:54:55.236899 53398 net.cpp:406] label_data_1_split <- label
I1220 17:54:55.236922 53398 net.cpp:380] label_data_1_split -> label_data_1_split_0
I1220 17:54:55.236940 53398 net.cpp:380] label_data_1_split -> label_data_1_split_1
I1220 17:54:55.236950 53398 net.cpp:380] label_data_1_split -> label_data_1_split_2
I1220 17:54:55.236966 53398 net.cpp:122] Setting up label_data_1_split
I1220 17:54:55.236977 53398 net.cpp:129] Top shape: 20 (20)
I1220 17:54:55.236989 53398 net.cpp:129] Top shape: 20 (20)
I1220 17:54:55.236996 53398 net.cpp:129] Top shape: 20 (20)
I1220 17:54:55.237002 53398 net.cpp:137] Memory required for data: 12367280
I1220 17:54:55.237009 53398 layer_factory.hpp:77] Creating layer quantized_data
I1220 17:54:55.237022 53398 net.cpp:84] Creating Layer quantized_data
I1220 17:54:55.237053 53398 net.cpp:406] quantized_data <- data
I1220 17:54:55.237063 53398 net.cpp:380] quantized_data -> quantized_data
I1220 17:54:55.237078 53398 net.cpp:122] Setting up quantized_data
I1220 17:54:55.237087 53398 net.cpp:129] Top shape: 20 3 227 227 (3091740)
I1220 17:54:55.237093 53398 net.cpp:137] Memory required for data: 24734240
I1220 17:54:55.237099 53398 layer_factory.hpp:77] Creating layer conv1
I1220 17:54:55.237128 53398 net.cpp:84] Creating Layer conv1
I1220 17:54:55.237135 53398 net.cpp:406] conv1 <- quantized_data
I1220 17:54:55.237144 53398 net.cpp:380] conv1 -> conv1
I1220 17:54:55.238271 53398 net.cpp:122] Setting up conv1
I1220 17:54:55.238287 53398 net.cpp:129] Top shape: 20 96 55 55 (5808000)
I1220 17:54:55.238293 53398 net.cpp:137] Memory required for data: 47966240
I1220 17:54:55.238317 53398 layer_factory.hpp:77] Creating layer quantized_conv1
I1220 17:54:55.238327 53398 net.cpp:84] Creating Layer quantized_conv1
I1220 17:54:55.238335 53398 net.cpp:406] quantized_conv1 <- conv1
I1220 17:54:55.238346 53398 net.cpp:380] quantized_conv1 -> quantized_conv1
I1220 17:54:55.238358 53398 net.cpp:122] Setting up quantized_conv1
I1220 17:54:55.238365 53398 net.cpp:129] Top shape: 20 96 55 55 (5808000)
I1220 17:54:55.238371 53398 net.cpp:137] Memory required for data: 71198240
I1220 17:54:55.238378 53398 layer_factory.hpp:77] Creating layer relu1
I1220 17:54:55.238385 53398 net.cpp:84] Creating Layer relu1
I1220 17:54:55.238392 53398 net.cpp:406] relu1 <- quantized_conv1
I1220 17:54:55.238407 53398 net.cpp:380] relu1 -> relu1
I1220 17:54:55.238417 53398 net.cpp:122] Setting up relu1
I1220 17:54:55.238426 53398 net.cpp:129] Top shape: 20 96 55 55 (5808000)
I1220 17:54:55.238432 53398 net.cpp:137] Memory required for data: 94430240
I1220 17:54:55.238438 53398 layer_factory.hpp:77] Creating layer quantized_relu1
I1220 17:54:55.238447 53398 net.cpp:84] Creating Layer quantized_relu1
I1220 17:54:55.238453 53398 net.cpp:406] quantized_relu1 <- relu1
I1220 17:54:55.238461 53398 net.cpp:380] quantized_relu1 -> quantized_relu1
I1220 17:54:55.238471 53398 net.cpp:122] Setting up quantized_relu1
I1220 17:54:55.238478 53398 net.cpp:129] Top shape: 20 96 55 55 (5808000)
I1220 17:54:55.238483 53398 net.cpp:137] Memory required for data: 117662240
I1220 17:54:55.238490 53398 layer_factory.hpp:77] Creating layer pool1
I1220 17:54:55.238502 53398 net.cpp:84] Creating Layer pool1
I1220 17:54:55.238509 53398 net.cpp:406] pool1 <- quantized_relu1
I1220 17:54:55.238519 53398 net.cpp:380] pool1 -> pool1
I1220 17:54:55.238544 53398 net.cpp:122] Setting up pool1
I1220 17:54:55.238554 53398 net.cpp:129] Top shape: 20 96 27 27 (1399680)
I1220 17:54:55.238560 53398 net.cpp:137] Memory required for data: 123260960
I1220 17:54:55.238567 53398 layer_factory.hpp:77] Creating layer quantized_pool1
I1220 17:54:55.238576 53398 net.cpp:84] Creating Layer quantized_pool1
I1220 17:54:55.238584 53398 net.cpp:406] quantized_pool1 <- pool1
I1220 17:54:55.238591 53398 net.cpp:380] quantized_pool1 -> quantized_pool1
I1220 17:54:55.238600 53398 net.cpp:122] Setting up quantized_pool1
I1220 17:54:55.238608 53398 net.cpp:129] Top shape: 20 96 27 27 (1399680)
I1220 17:54:55.238615 53398 net.cpp:137] Memory required for data: 128859680
I1220 17:54:55.238620 53398 layer_factory.hpp:77] Creating layer norm1
I1220 17:54:55.238632 53398 net.cpp:84] Creating Layer norm1
I1220 17:54:55.238641 53398 net.cpp:406] norm1 <- quantized_pool1
I1220 17:54:55.238651 53398 net.cpp:380] norm1 -> norm1
I1220 17:54:55.238667 53398 net.cpp:122] Setting up norm1
I1220 17:54:55.238674 53398 net.cpp:129] Top shape: 20 96 27 27 (1399680)
I1220 17:54:55.238680 53398 net.cpp:137] Memory required for data: 134458400
I1220 17:54:55.238687 53398 layer_factory.hpp:77] Creating layer quantized_norm1
I1220 17:54:55.238694 53398 net.cpp:84] Creating Layer quantized_norm1
I1220 17:54:55.238701 53398 net.cpp:406] quantized_norm1 <- norm1
I1220 17:54:55.238711 53398 net.cpp:380] quantized_norm1 -> quantized_norm1
I1220 17:54:55.238726 53398 net.cpp:122] Setting up quantized_norm1
I1220 17:54:55.238741 53398 net.cpp:129] Top shape: 20 96 27 27 (1399680)
I1220 17:54:55.238747 53398 net.cpp:137] Memory required for data: 140057120
I1220 17:54:55.238754 53398 layer_factory.hpp:77] Creating layer conv2
I1220 17:54:55.238768 53398 net.cpp:84] Creating Layer conv2
I1220 17:54:55.238775 53398 net.cpp:406] conv2 <- quantized_norm1
I1220 17:54:55.238785 53398 net.cpp:380] conv2 -> conv2
I1220 17:54:55.247764 53398 net.cpp:122] Setting up conv2
I1220 17:54:55.247781 53398 net.cpp:129] Top shape: 20 256 27 27 (3732480)
I1220 17:54:55.247787 53398 net.cpp:137] Memory required for data: 154987040
I1220 17:54:55.247799 53398 layer_factory.hpp:77] Creating layer quantized_conv2
I1220 17:54:55.247810 53398 net.cpp:84] Creating Layer quantized_conv2
I1220 17:54:55.247817 53398 net.cpp:406] quantized_conv2 <- conv2
I1220 17:54:55.247828 53398 net.cpp:380] quantized_conv2 -> quantized_conv2
I1220 17:54:55.247839 53398 net.cpp:122] Setting up quantized_conv2
I1220 17:54:55.247848 53398 net.cpp:129] Top shape: 20 256 27 27 (3732480)
I1220 17:54:55.247854 53398 net.cpp:137] Memory required for data: 169916960
I1220 17:54:55.247860 53398 layer_factory.hpp:77] Creating layer relu2
I1220 17:54:55.247869 53398 net.cpp:84] Creating Layer relu2
I1220 17:54:55.247874 53398 net.cpp:406] relu2 <- quantized_conv2
I1220 17:54:55.247885 53398 net.cpp:380] relu2 -> relu2
I1220 17:54:55.247896 53398 net.cpp:122] Setting up relu2
I1220 17:54:55.247905 53398 net.cpp:129] Top shape: 20 256 27 27 (3732480)
I1220 17:54:55.247911 53398 net.cpp:137] Memory required for data: 184846880
I1220 17:54:55.247917 53398 layer_factory.hpp:77] Creating layer quantized_relu2
I1220 17:54:55.247925 53398 net.cpp:84] Creating Layer quantized_relu2
I1220 17:54:55.247932 53398 net.cpp:406] quantized_relu2 <- relu2
I1220 17:54:55.247941 53398 net.cpp:380] quantized_relu2 -> quantized_relu2
I1220 17:54:55.247952 53398 net.cpp:122] Setting up quantized_relu2
I1220 17:54:55.247961 53398 net.cpp:129] Top shape: 20 256 27 27 (3732480)
I1220 17:54:55.247967 53398 net.cpp:137] Memory required for data: 199776800
I1220 17:54:55.247972 53398 layer_factory.hpp:77] Creating layer pool2
I1220 17:54:55.247984 53398 net.cpp:84] Creating Layer pool2
I1220 17:54:55.247992 53398 net.cpp:406] pool2 <- quantized_relu2
I1220 17:54:55.248005 53398 net.cpp:380] pool2 -> pool2
I1220 17:54:55.248016 53398 net.cpp:122] Setting up pool2
I1220 17:54:55.248025 53398 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1220 17:54:55.248031 53398 net.cpp:137] Memory required for data: 203237920
I1220 17:54:55.248037 53398 layer_factory.hpp:77] Creating layer quantized_pool2
I1220 17:54:55.248046 53398 net.cpp:84] Creating Layer quantized_pool2
I1220 17:54:55.248054 53398 net.cpp:406] quantized_pool2 <- pool2
I1220 17:54:55.248064 53398 net.cpp:380] quantized_pool2 -> quantized_pool2
I1220 17:54:55.248072 53398 net.cpp:122] Setting up quantized_pool2
I1220 17:54:55.248080 53398 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1220 17:54:55.248086 53398 net.cpp:137] Memory required for data: 206699040
I1220 17:54:55.248092 53398 layer_factory.hpp:77] Creating layer norm2
I1220 17:54:55.248103 53398 net.cpp:84] Creating Layer norm2
I1220 17:54:55.248111 53398 net.cpp:406] norm2 <- quantized_pool2
I1220 17:54:55.248118 53398 net.cpp:380] norm2 -> norm2
I1220 17:54:55.248127 53398 net.cpp:122] Setting up norm2
I1220 17:54:55.248136 53398 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1220 17:54:55.248142 53398 net.cpp:137] Memory required for data: 210160160
I1220 17:54:55.248147 53398 layer_factory.hpp:77] Creating layer quantized_norm2
I1220 17:54:55.248155 53398 net.cpp:84] Creating Layer quantized_norm2
I1220 17:54:55.248162 53398 net.cpp:406] quantized_norm2 <- norm2
I1220 17:54:55.248170 53398 net.cpp:380] quantized_norm2 -> quantized_norm2
I1220 17:54:55.248179 53398 net.cpp:122] Setting up quantized_norm2
I1220 17:54:55.248188 53398 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1220 17:54:55.248193 53398 net.cpp:137] Memory required for data: 213621280
I1220 17:54:55.248204 53398 layer_factory.hpp:77] Creating layer conv3
I1220 17:54:55.248226 53398 net.cpp:84] Creating Layer conv3
I1220 17:54:55.248234 53398 net.cpp:406] conv3 <- quantized_norm2
I1220 17:54:55.248244 53398 net.cpp:380] conv3 -> conv3
I1220 17:54:55.274020 53398 net.cpp:122] Setting up conv3
I1220 17:54:55.274039 53398 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1220 17:54:55.274044 53398 net.cpp:137] Memory required for data: 218812960
I1220 17:54:55.274055 53398 layer_factory.hpp:77] Creating layer quantized_conv3
I1220 17:54:55.274068 53398 net.cpp:84] Creating Layer quantized_conv3
I1220 17:54:55.274075 53398 net.cpp:406] quantized_conv3 <- conv3
I1220 17:54:55.274085 53398 net.cpp:380] quantized_conv3 -> quantized_conv3
I1220 17:54:55.274094 53398 net.cpp:122] Setting up quantized_conv3
I1220 17:54:55.274102 53398 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1220 17:54:55.274111 53398 net.cpp:137] Memory required for data: 224004640
I1220 17:54:55.274117 53398 layer_factory.hpp:77] Creating layer relu3
I1220 17:54:55.274124 53398 net.cpp:84] Creating Layer relu3
I1220 17:54:55.274132 53398 net.cpp:406] relu3 <- quantized_conv3
I1220 17:54:55.274142 53398 net.cpp:380] relu3 -> relu3
I1220 17:54:55.274150 53398 net.cpp:122] Setting up relu3
I1220 17:54:55.274158 53398 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1220 17:54:55.274164 53398 net.cpp:137] Memory required for data: 229196320
I1220 17:54:55.274170 53398 layer_factory.hpp:77] Creating layer quantized_relu3
I1220 17:54:55.274178 53398 net.cpp:84] Creating Layer quantized_relu3
I1220 17:54:55.274184 53398 net.cpp:406] quantized_relu3 <- relu3
I1220 17:54:55.274194 53398 net.cpp:380] quantized_relu3 -> quantized_relu3
I1220 17:54:55.274204 53398 net.cpp:122] Setting up quantized_relu3
I1220 17:54:55.274212 53398 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1220 17:54:55.274219 53398 net.cpp:137] Memory required for data: 234388000
I1220 17:54:55.274224 53398 layer_factory.hpp:77] Creating layer conv4
I1220 17:54:55.274237 53398 net.cpp:84] Creating Layer conv4
I1220 17:54:55.274245 53398 net.cpp:406] conv4 <- quantized_relu3
I1220 17:54:55.274253 53398 net.cpp:380] conv4 -> conv4
I1220 17:54:55.293360 53398 net.cpp:122] Setting up conv4
I1220 17:54:55.293381 53398 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1220 17:54:55.293387 53398 net.cpp:137] Memory required for data: 239579680
I1220 17:54:55.293396 53398 layer_factory.hpp:77] Creating layer quantized_conv4
I1220 17:54:55.293406 53398 net.cpp:84] Creating Layer quantized_conv4
I1220 17:54:55.293413 53398 net.cpp:406] quantized_conv4 <- conv4
I1220 17:54:55.293426 53398 net.cpp:380] quantized_conv4 -> quantized_conv4
I1220 17:54:55.293437 53398 net.cpp:122] Setting up quantized_conv4
I1220 17:54:55.293444 53398 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1220 17:54:55.293450 53398 net.cpp:137] Memory required for data: 244771360
I1220 17:54:55.293457 53398 layer_factory.hpp:77] Creating layer relu4
I1220 17:54:55.293464 53398 net.cpp:84] Creating Layer relu4
I1220 17:54:55.293470 53398 net.cpp:406] relu4 <- quantized_conv4
I1220 17:54:55.293480 53398 net.cpp:380] relu4 -> relu4
I1220 17:54:55.293489 53398 net.cpp:122] Setting up relu4
I1220 17:54:55.293498 53398 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1220 17:54:55.293503 53398 net.cpp:137] Memory required for data: 249963040
I1220 17:54:55.293509 53398 layer_factory.hpp:77] Creating layer quantized_relu4
I1220 17:54:55.293517 53398 net.cpp:84] Creating Layer quantized_relu4
I1220 17:54:55.293524 53398 net.cpp:406] quantized_relu4 <- relu4
I1220 17:54:55.293531 53398 net.cpp:380] quantized_relu4 -> quantized_relu4
I1220 17:54:55.293540 53398 net.cpp:122] Setting up quantized_relu4
I1220 17:54:55.293548 53398 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1220 17:54:55.293555 53398 net.cpp:137] Memory required for data: 255154720
I1220 17:54:55.293560 53398 layer_factory.hpp:77] Creating layer conv5
I1220 17:54:55.293576 53398 net.cpp:84] Creating Layer conv5
I1220 17:54:55.293582 53398 net.cpp:406] conv5 <- quantized_relu4
I1220 17:54:55.293597 53398 net.cpp:380] conv5 -> conv5
I1220 17:54:55.306272 53398 net.cpp:122] Setting up conv5
I1220 17:54:55.306289 53398 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1220 17:54:55.306295 53398 net.cpp:137] Memory required for data: 258615840
I1220 17:54:55.306309 53398 layer_factory.hpp:77] Creating layer quantized_conv5
I1220 17:54:55.306319 53398 net.cpp:84] Creating Layer quantized_conv5
I1220 17:54:55.306326 53398 net.cpp:406] quantized_conv5 <- conv5
I1220 17:54:55.306337 53398 net.cpp:380] quantized_conv5 -> quantized_conv5
I1220 17:54:55.306347 53398 net.cpp:122] Setting up quantized_conv5
I1220 17:54:55.306356 53398 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1220 17:54:55.306361 53398 net.cpp:137] Memory required for data: 262076960
I1220 17:54:55.306367 53398 layer_factory.hpp:77] Creating layer relu5
I1220 17:54:55.306375 53398 net.cpp:84] Creating Layer relu5
I1220 17:54:55.306382 53398 net.cpp:406] relu5 <- quantized_conv5
I1220 17:54:55.306390 53398 net.cpp:380] relu5 -> relu5
I1220 17:54:55.306401 53398 net.cpp:122] Setting up relu5
I1220 17:54:55.306411 53398 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1220 17:54:55.306417 53398 net.cpp:137] Memory required for data: 265538080
I1220 17:54:55.306423 53398 layer_factory.hpp:77] Creating layer quantized_relu5
I1220 17:54:55.306432 53398 net.cpp:84] Creating Layer quantized_relu5
I1220 17:54:55.306438 53398 net.cpp:406] quantized_relu5 <- relu5
I1220 17:54:55.306449 53398 net.cpp:380] quantized_relu5 -> quantized_relu5
I1220 17:54:55.306459 53398 net.cpp:122] Setting up quantized_relu5
I1220 17:54:55.306468 53398 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1220 17:54:55.306473 53398 net.cpp:137] Memory required for data: 268999200
I1220 17:54:55.306479 53398 layer_factory.hpp:77] Creating layer pool5
I1220 17:54:55.306488 53398 net.cpp:84] Creating Layer pool5
I1220 17:54:55.306494 53398 net.cpp:406] pool5 <- quantized_relu5
I1220 17:54:55.306501 53398 net.cpp:380] pool5 -> pool5
I1220 17:54:55.306514 53398 net.cpp:122] Setting up pool5
I1220 17:54:55.306521 53398 net.cpp:129] Top shape: 20 256 6 6 (184320)
I1220 17:54:55.306527 53398 net.cpp:137] Memory required for data: 269736480
I1220 17:54:55.306533 53398 layer_factory.hpp:77] Creating layer quantized_pool5
I1220 17:54:55.306546 53398 net.cpp:84] Creating Layer quantized_pool5
I1220 17:54:55.306555 53398 net.cpp:406] quantized_pool5 <- pool5
I1220 17:54:55.306563 53398 net.cpp:380] quantized_pool5 -> quantized_pool5
I1220 17:54:55.306572 53398 net.cpp:122] Setting up quantized_pool5
I1220 17:54:55.306581 53398 net.cpp:129] Top shape: 20 256 6 6 (184320)
I1220 17:54:55.306586 53398 net.cpp:137] Memory required for data: 270473760
I1220 17:54:55.306592 53398 layer_factory.hpp:77] Creating layer fc6
I1220 17:54:55.306607 53398 net.cpp:84] Creating Layer fc6
I1220 17:54:55.306613 53398 net.cpp:406] fc6 <- quantized_pool5
I1220 17:54:55.306623 53398 net.cpp:380] fc6 -> fc6
I1220 17:54:56.359537 53398 net.cpp:122] Setting up fc6
I1220 17:54:56.359597 53398 net.cpp:129] Top shape: 20 4096 (81920)
I1220 17:54:56.359604 53398 net.cpp:137] Memory required for data: 270801440
I1220 17:54:56.359621 53398 layer_factory.hpp:77] Creating layer quantized_fc6
I1220 17:54:56.359637 53398 net.cpp:84] Creating Layer quantized_fc6
I1220 17:54:56.359647 53398 net.cpp:406] quantized_fc6 <- fc6
I1220 17:54:56.359660 53398 net.cpp:380] quantized_fc6 -> quantized_fc6
I1220 17:54:56.359678 53398 net.cpp:122] Setting up quantized_fc6
I1220 17:54:56.359685 53398 net.cpp:129] Top shape: 20 4096 (81920)
I1220 17:54:56.359691 53398 net.cpp:137] Memory required for data: 271129120
I1220 17:54:56.359697 53398 layer_factory.hpp:77] Creating layer relu6
I1220 17:54:56.359714 53398 net.cpp:84] Creating Layer relu6
I1220 17:54:56.359721 53398 net.cpp:406] relu6 <- quantized_fc6
I1220 17:54:56.359730 53398 net.cpp:380] relu6 -> relu6
I1220 17:54:56.359740 53398 net.cpp:122] Setting up relu6
I1220 17:54:56.359747 53398 net.cpp:129] Top shape: 20 4096 (81920)
I1220 17:54:56.359753 53398 net.cpp:137] Memory required for data: 271456800
I1220 17:54:56.359783 53398 layer_factory.hpp:77] Creating layer quantized_relu6
I1220 17:54:56.359793 53398 net.cpp:84] Creating Layer quantized_relu6
I1220 17:54:56.359799 53398 net.cpp:406] quantized_relu6 <- relu6
I1220 17:54:56.359808 53398 net.cpp:380] quantized_relu6 -> quantized_relu6
I1220 17:54:56.359818 53398 net.cpp:122] Setting up quantized_relu6
I1220 17:54:56.359825 53398 net.cpp:129] Top shape: 20 4096 (81920)
I1220 17:54:56.359832 53398 net.cpp:137] Memory required for data: 271784480
I1220 17:54:56.359838 53398 layer_factory.hpp:77] Creating layer fc7
I1220 17:54:56.359851 53398 net.cpp:84] Creating Layer fc7
I1220 17:54:56.359858 53398 net.cpp:406] fc7 <- quantized_relu6
I1220 17:54:56.359868 53398 net.cpp:380] fc7 -> fc7
I1220 17:54:56.829916 53398 net.cpp:122] Setting up fc7
I1220 17:54:56.829977 53398 net.cpp:129] Top shape: 20 4096 (81920)
I1220 17:54:56.829983 53398 net.cpp:137] Memory required for data: 272112160
I1220 17:54:56.829998 53398 layer_factory.hpp:77] Creating layer quantized_fc7
I1220 17:54:56.830015 53398 net.cpp:84] Creating Layer quantized_fc7
I1220 17:54:56.830025 53398 net.cpp:406] quantized_fc7 <- fc7
I1220 17:54:56.830041 53398 net.cpp:380] quantized_fc7 -> quantized_fc7
I1220 17:54:56.830058 53398 net.cpp:122] Setting up quantized_fc7
I1220 17:54:56.830067 53398 net.cpp:129] Top shape: 20 4096 (81920)
I1220 17:54:56.830073 53398 net.cpp:137] Memory required for data: 272439840
I1220 17:54:56.830080 53398 layer_factory.hpp:77] Creating layer relu7
I1220 17:54:56.830088 53398 net.cpp:84] Creating Layer relu7
I1220 17:54:56.830094 53398 net.cpp:406] relu7 <- quantized_fc7
I1220 17:54:56.830102 53398 net.cpp:380] relu7 -> relu7
I1220 17:54:56.830112 53398 net.cpp:122] Setting up relu7
I1220 17:54:56.830121 53398 net.cpp:129] Top shape: 20 4096 (81920)
I1220 17:54:56.830126 53398 net.cpp:137] Memory required for data: 272767520
I1220 17:54:56.830132 53398 layer_factory.hpp:77] Creating layer quantized_relu7
I1220 17:54:56.830140 53398 net.cpp:84] Creating Layer quantized_relu7
I1220 17:54:56.830147 53398 net.cpp:406] quantized_relu7 <- relu7
I1220 17:54:56.830155 53398 net.cpp:380] quantized_relu7 -> quantized_relu7
I1220 17:54:56.830164 53398 net.cpp:122] Setting up quantized_relu7
I1220 17:54:56.830171 53398 net.cpp:129] Top shape: 20 4096 (81920)
I1220 17:54:56.830178 53398 net.cpp:137] Memory required for data: 273095200
I1220 17:54:56.830183 53398 layer_factory.hpp:77] Creating layer fc8
I1220 17:54:56.830199 53398 net.cpp:84] Creating Layer fc8
I1220 17:54:56.830205 53398 net.cpp:406] fc8 <- quantized_relu7
I1220 17:54:56.830214 53398 net.cpp:380] fc8 -> fc8
I1220 17:54:56.836905 53398 net.cpp:122] Setting up fc8
I1220 17:54:56.836967 53398 net.cpp:129] Top shape: 20 1000 (20000)
I1220 17:54:56.836974 53398 net.cpp:137] Memory required for data: 273175200
I1220 17:54:56.836989 53398 layer_factory.hpp:77] Creating layer quantized_fc8
I1220 17:54:56.837005 53398 net.cpp:84] Creating Layer quantized_fc8
I1220 17:54:56.837015 53398 net.cpp:406] quantized_fc8 <- fc8
I1220 17:54:56.837031 53398 net.cpp:380] quantized_fc8 -> quantized_fc8
I1220 17:54:56.837049 53398 net.cpp:122] Setting up quantized_fc8
I1220 17:54:56.837057 53398 net.cpp:129] Top shape: 20 1000 (20000)
I1220 17:54:56.837064 53398 net.cpp:137] Memory required for data: 273255200
I1220 17:54:56.837069 53398 layer_factory.hpp:77] Creating layer quantized_fc8_quantized_fc8_0_split
I1220 17:54:56.837079 53398 net.cpp:84] Creating Layer quantized_fc8_quantized_fc8_0_split
I1220 17:54:56.837085 53398 net.cpp:406] quantized_fc8_quantized_fc8_0_split <- quantized_fc8
I1220 17:54:56.837095 53398 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_0
I1220 17:54:56.837105 53398 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_1
I1220 17:54:56.837116 53398 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_2
I1220 17:54:56.837124 53398 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_3
I1220 17:54:56.837165 53398 net.cpp:122] Setting up quantized_fc8_quantized_fc8_0_split
I1220 17:54:56.837174 53398 net.cpp:129] Top shape: 20 1000 (20000)
I1220 17:54:56.837182 53398 net.cpp:129] Top shape: 20 1000 (20000)
I1220 17:54:56.837188 53398 net.cpp:129] Top shape: 20 1000 (20000)
I1220 17:54:56.837194 53398 net.cpp:129] Top shape: 20 1000 (20000)
I1220 17:54:56.837200 53398 net.cpp:137] Memory required for data: 273575200
I1220 17:54:56.837206 53398 layer_factory.hpp:77] Creating layer probs
I1220 17:54:56.837218 53398 net.cpp:84] Creating Layer probs
I1220 17:54:56.837225 53398 net.cpp:406] probs <- quantized_fc8_quantized_fc8_0_split_0
I1220 17:54:56.837234 53398 net.cpp:380] probs -> probs
I1220 17:54:56.837261 53398 net.cpp:122] Setting up probs
I1220 17:54:56.837270 53398 net.cpp:129] Top shape: 20 1000 (20000)
I1220 17:54:56.837276 53398 net.cpp:137] Memory required for data: 273655200
I1220 17:54:56.837282 53398 layer_factory.hpp:77] Creating layer loss
I1220 17:54:56.837293 53398 net.cpp:84] Creating Layer loss
I1220 17:54:56.837299 53398 net.cpp:406] loss <- quantized_fc8_quantized_fc8_0_split_1
I1220 17:54:56.837306 53398 net.cpp:406] loss <- label_data_1_split_0
I1220 17:54:56.837314 53398 net.cpp:380] loss -> loss
I1220 17:54:56.837335 53398 layer_factory.hpp:77] Creating layer loss
I1220 17:54:56.837404 53398 net.cpp:122] Setting up loss
I1220 17:54:56.837421 53398 net.cpp:129] Top shape: (1)
I1220 17:54:56.837429 53398 net.cpp:132]     with loss weight 1
I1220 17:54:56.837461 53398 net.cpp:137] Memory required for data: 273655204
I1220 17:54:56.837468 53398 layer_factory.hpp:77] Creating layer acc_top1
I1220 17:54:56.837483 53398 net.cpp:84] Creating Layer acc_top1
I1220 17:54:56.837491 53398 net.cpp:406] acc_top1 <- quantized_fc8_quantized_fc8_0_split_2
I1220 17:54:56.837498 53398 net.cpp:406] acc_top1 <- label_data_1_split_1
I1220 17:54:56.837509 53398 net.cpp:380] acc_top1 -> acc_top1
I1220 17:54:56.837527 53398 net.cpp:122] Setting up acc_top1
I1220 17:54:56.837535 53398 net.cpp:129] Top shape: (1)
I1220 17:54:56.837541 53398 net.cpp:137] Memory required for data: 273655208
I1220 17:54:56.837548 53398 layer_factory.hpp:77] Creating layer acc_top5
I1220 17:54:56.837555 53398 net.cpp:84] Creating Layer acc_top5
I1220 17:54:56.837563 53398 net.cpp:406] acc_top5 <- quantized_fc8_quantized_fc8_0_split_3
I1220 17:54:56.837569 53398 net.cpp:406] acc_top5 <- label_data_1_split_2
I1220 17:54:56.837577 53398 net.cpp:380] acc_top5 -> acc_top5
I1220 17:54:56.837586 53398 net.cpp:122] Setting up acc_top5
I1220 17:54:56.837594 53398 net.cpp:129] Top shape: (1)
I1220 17:54:56.837599 53398 net.cpp:137] Memory required for data: 273655212
I1220 17:54:56.837605 53398 net.cpp:200] acc_top5 does not need backward computation.
I1220 17:54:56.837612 53398 net.cpp:200] acc_top1 does not need backward computation.
I1220 17:54:56.837618 53398 net.cpp:198] loss needs backward computation.
I1220 17:54:56.837625 53398 net.cpp:200] probs does not need backward computation.
I1220 17:54:56.837631 53398 net.cpp:198] quantized_fc8_quantized_fc8_0_split needs backward computation.
I1220 17:54:56.837637 53398 net.cpp:198] quantized_fc8 needs backward computation.
I1220 17:54:56.837643 53398 net.cpp:198] fc8 needs backward computation.
I1220 17:54:56.837649 53398 net.cpp:200] quantized_relu7 does not need backward computation.
I1220 17:54:56.837656 53398 net.cpp:200] relu7 does not need backward computation.
I1220 17:54:56.837666 53398 net.cpp:200] quantized_fc7 does not need backward computation.
I1220 17:54:56.837672 53398 net.cpp:200] fc7 does not need backward computation.
I1220 17:54:56.837679 53398 net.cpp:200] quantized_relu6 does not need backward computation.
I1220 17:54:56.837685 53398 net.cpp:200] relu6 does not need backward computation.
I1220 17:54:56.837692 53398 net.cpp:200] quantized_fc6 does not need backward computation.
I1220 17:54:56.837699 53398 net.cpp:200] fc6 does not need backward computation.
I1220 17:54:56.837705 53398 net.cpp:200] quantized_pool5 does not need backward computation.
I1220 17:54:56.837730 53398 net.cpp:200] pool5 does not need backward computation.
I1220 17:54:56.837738 53398 net.cpp:200] quantized_relu5 does not need backward computation.
I1220 17:54:56.837744 53398 net.cpp:200] relu5 does not need backward computation.
I1220 17:54:56.837751 53398 net.cpp:200] quantized_conv5 does not need backward computation.
I1220 17:54:56.837759 53398 net.cpp:200] conv5 does not need backward computation.
I1220 17:54:56.837765 53398 net.cpp:200] quantized_relu4 does not need backward computation.
I1220 17:54:56.837771 53398 net.cpp:200] relu4 does not need backward computation.
I1220 17:54:56.837779 53398 net.cpp:200] quantized_conv4 does not need backward computation.
I1220 17:54:56.837785 53398 net.cpp:200] conv4 does not need backward computation.
I1220 17:54:56.837791 53398 net.cpp:200] quantized_relu3 does not need backward computation.
I1220 17:54:56.837798 53398 net.cpp:200] relu3 does not need backward computation.
I1220 17:54:56.837807 53398 net.cpp:200] quantized_conv3 does not need backward computation.
I1220 17:54:56.837815 53398 net.cpp:200] conv3 does not need backward computation.
I1220 17:54:56.837821 53398 net.cpp:200] quantized_norm2 does not need backward computation.
I1220 17:54:56.837828 53398 net.cpp:200] norm2 does not need backward computation.
I1220 17:54:56.837836 53398 net.cpp:200] quantized_pool2 does not need backward computation.
I1220 17:54:56.837841 53398 net.cpp:200] pool2 does not need backward computation.
I1220 17:54:56.837848 53398 net.cpp:200] quantized_relu2 does not need backward computation.
I1220 17:54:56.837854 53398 net.cpp:200] relu2 does not need backward computation.
I1220 17:54:56.837862 53398 net.cpp:200] quantized_conv2 does not need backward computation.
I1220 17:54:56.837868 53398 net.cpp:200] conv2 does not need backward computation.
I1220 17:54:56.837874 53398 net.cpp:200] quantized_norm1 does not need backward computation.
I1220 17:54:56.837882 53398 net.cpp:200] norm1 does not need backward computation.
I1220 17:54:56.837888 53398 net.cpp:200] quantized_pool1 does not need backward computation.
I1220 17:54:56.837895 53398 net.cpp:200] pool1 does not need backward computation.
I1220 17:54:56.837903 53398 net.cpp:200] quantized_relu1 does not need backward computation.
I1220 17:54:56.837908 53398 net.cpp:200] relu1 does not need backward computation.
I1220 17:54:56.837915 53398 net.cpp:200] quantized_conv1 does not need backward computation.
I1220 17:54:56.837921 53398 net.cpp:200] conv1 does not need backward computation.
I1220 17:54:56.837929 53398 net.cpp:200] quantized_data does not need backward computation.
I1220 17:54:56.837935 53398 net.cpp:200] label_data_1_split does not need backward computation.
I1220 17:54:56.837941 53398 net.cpp:200] data does not need backward computation.
I1220 17:54:56.837947 53398 net.cpp:242] This network produces output acc_top1
I1220 17:54:56.837954 53398 net.cpp:242] This network produces output acc_top5
I1220 17:54:56.837960 53398 net.cpp:242] This network produces output loss
I1220 17:54:56.837965 53398 net.cpp:242] This network produces output probs
I1220 17:54:56.838001 53398 net.cpp:255] Network initialization done.
I1220 17:54:57.676631 53398 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: qnn_try1/bvlc_reference_caffenet.caffemodel
I1220 17:54:57.676709 53398 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1220 17:54:57.676718 53398 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1220 17:54:57.676851 53398 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: qnn_try1/bvlc_reference_caffenet.caffemodel
I1220 17:54:57.922760 53398 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1220 17:54:57.965389 53398 net.cpp:744] Ignoring source layer drop6
I1220 17:54:57.982431 53398 net.cpp:744] Ignoring source layer drop7
I1220 17:54:57.989715 53398 caffe.cpp:290] Running for 1 iterations.
max:0.402661  min:-0.373572
new--cpu_data-0.00121359
comput--weight-0.375
max:0.41551  min:-0.284751
new--cpu_data-0.0111258
comput--weight-0.3125
max:0.512327  min:-0.185205
new--cpu_data-0.000527019
comput--weight-0.1875
max:0.353425  min:-0.14246
new--cpu_data0.00378311
comput--weight-0.125
max:0.314808  min:-0.132628
new--cpu_data-0.0134502
comput--weight-0.125
I1220 17:55:00.993553 53398 caffe.cpp:318] Loss: 8.16072
