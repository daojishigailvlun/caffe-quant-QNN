I1221 11:36:24.368836 39073 caffe.cpp:211] Use CPU.
I1221 11:36:25.780576 39073 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 100
base_lr: 0.01
display: 20
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 10000
snapshot_prefix: "qnn_try1/QNN-train"
solver_mode: CPU
net: "qnn_try1/train_quantized_caffenet.prototxt"
train_state {
  level: 0
  stage: ""
}
I1221 11:36:25.780767 39073 solver.cpp:87] Creating training net from net file: qnn_try1/train_quantized_caffenet.prototxt
I1221 11:36:25.781733 39073 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1221 11:36:25.782109 39073 net.cpp:51] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/ydwu/database/imagenet/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/ydwu/database/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_train_lmdb/"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "quantized_data"
  type: "Quantization"
  bottom: "data"
  top: "quantized_data"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -129.93526
    range: 160.78912
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "quantized_data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv1"
  top: "quantized_conv1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -2732.7351
    range: 2531.042
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "quantized_conv1"
  top: "relu1"
}
layer {
  name: "quantized_relu1"
  type: "Quantization"
  bottom: "relu1"
  top: "quantized_relu1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 2531.042
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "quantized_relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool1"
  type: "Quantization"
  bottom: "pool1"
  top: "quantized_pool1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 2531.042
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "quantized_pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "quantized_norm1"
  type: "Quantization"
  bottom: "norm1"
  top: "quantized_norm1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 138.70576
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "quantized_norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv2"
  type: "Quantization"
  bottom: "conv2"
  top: "quantized_conv2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -647.61908
    range: 492.23111
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "quantized_conv2"
  top: "relu2"
}
layer {
  name: "quantized_relu2"
  type: "Quantization"
  bottom: "relu2"
  top: "quantized_relu2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 492.23111
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "quantized_relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool2"
  type: "Quantization"
  bottom: "pool2"
  top: "quantized_pool2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 492.23111
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "quantized_pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "quantized_norm2"
  type: "Quantization"
  bottom: "norm2"
  top: "quantized_norm2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 138.72636
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "quantized_norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv3"
  type: "Quantization"
  bottom: "conv3"
  top: "quantized_conv3"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -389.56516
    range: 331.98178
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "quantized_conv3"
  top: "relu3"
}
layer {
  name: "quantized_relu3"
  type: "Quantization"
  bottom: "relu3"
  top: "quantized_relu3"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 331.98178
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "quantized_relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv4"
  type: "Quantization"
  bottom: "conv4"
  top: "quantized_conv4"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -231.20598
    range: 283.87189
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "quantized_conv4"
  top: "relu4"
}
layer {
  name: "quantized_relu4"
  type: "Quantization"
  bottom: "relu4"
  top: "quantized_relu4"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 283.87189
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "quantized_relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv5"
  type: "Quantization"
  bottom: "conv5"
  top: "quantized_conv5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -172.60567
    range: 272.57315
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "quantized_conv5"
  top: "relu5"
}
layer {
  name: "quantized_relu5"
  type: "Quantization"
  bottom: "relu5"
  top: "quantized_relu5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 272.57315
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "quantized_relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool5"
  type: "Quantization"
  bottom: "pool5"
  top: "quantized_pool5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 272.57315
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "quantized_pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 1.05e+13
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "quantized_fc6"
  type: "Quantization"
  bottom: "fc6"
  top: "quantized_fc6"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -114.02487
    range: 58.541733
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "quantized_fc6"
  top: "relu6"
}
layer {
  name: "quantized_relu6"
  type: "Quantization"
  bottom: "relu6"
  top: "quantized_relu6"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 58.541733
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "quantized_relu6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 1.05e+13
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "quantized_fc7"
  type: "Quantization"
  bottom: "fc7"
  top: "quantized_fc7"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -26.019335
    range: 19.125257
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "quantized_fc7"
  top: "relu7"
}
layer {
  name: "quantized_relu7"
  type: "Quantization"
  bottom: "relu7"
  top: "quantized_relu7"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 19.125257
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "quantized_relu7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
  }
}
layer {
  name: "quantized_fc8"
  type: "Quantization"
  bottom: "fc8"
  top: "quantized_fc8"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -9.849781
    range: 45.688446
  }
}
layer {
  name: "probs"
  type: "Softmax"
  bottom: "quantized_fc8"
  top: "probs"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "acc_top1"
  type: "Accuracy"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "acc_top1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "acc_top5"
  type: "Accuracy"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "acc_top5"
  accuracy_param {
    top_k: 5
  }
}
I1221 11:36:25.782402 39073 layer_factory.hpp:77] Creating layer data
I1221 11:36:25.782529 39073 db_lmdb.cpp:35] Opened lmdb /home/ydwu/database/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_train_lmdb/
I1221 11:36:25.782574 39073 net.cpp:84] Creating Layer data
I1221 11:36:25.782595 39073 net.cpp:380] data -> data
I1221 11:36:25.782631 39073 net.cpp:380] data -> label
I1221 11:36:25.782658 39073 data_transformer.cpp:25] Loading mean file from: /home/ydwu/database/imagenet/ilsvrc12/imagenet_mean.binaryproto
I1221 11:36:25.785338 39073 data_layer.cpp:45] output data size: 32,3,227,227
I1221 11:36:25.808035 39073 net.cpp:122] Setting up data
I1221 11:36:25.808117 39073 net.cpp:129] Top shape: 32 3 227 227 (4946784)
I1221 11:36:25.808131 39073 net.cpp:129] Top shape: 32 (32)
I1221 11:36:25.808140 39073 net.cpp:137] Memory required for data: 19787264
I1221 11:36:25.808158 39073 layer_factory.hpp:77] Creating layer label_data_1_split
I1221 11:36:25.808183 39073 net.cpp:84] Creating Layer label_data_1_split
I1221 11:36:25.808198 39073 net.cpp:406] label_data_1_split <- label
I1221 11:36:25.808223 39073 net.cpp:380] label_data_1_split -> label_data_1_split_0
I1221 11:36:25.808244 39073 net.cpp:380] label_data_1_split -> label_data_1_split_1
I1221 11:36:25.808259 39073 net.cpp:380] label_data_1_split -> label_data_1_split_2
I1221 11:36:25.808320 39073 net.cpp:122] Setting up label_data_1_split
I1221 11:36:25.808333 39073 net.cpp:129] Top shape: 32 (32)
I1221 11:36:25.808343 39073 net.cpp:129] Top shape: 32 (32)
I1221 11:36:25.808353 39073 net.cpp:129] Top shape: 32 (32)
I1221 11:36:25.808362 39073 net.cpp:137] Memory required for data: 19787648
I1221 11:36:25.808370 39073 layer_factory.hpp:77] Creating layer quantized_data
I1221 11:36:25.808387 39073 net.cpp:84] Creating Layer quantized_data
I1221 11:36:25.808398 39073 net.cpp:406] quantized_data <- data
I1221 11:36:25.808409 39073 net.cpp:380] quantized_data -> quantized_data
I1221 11:36:25.808429 39073 net.cpp:122] Setting up quantized_data
I1221 11:36:25.808440 39073 net.cpp:129] Top shape: 32 3 227 227 (4946784)
I1221 11:36:25.808449 39073 net.cpp:137] Memory required for data: 39574784
I1221 11:36:25.808459 39073 layer_factory.hpp:77] Creating layer conv1
I1221 11:36:25.808485 39073 net.cpp:84] Creating Layer conv1
I1221 11:36:25.808496 39073 net.cpp:406] conv1 <- quantized_data
I1221 11:36:25.808508 39073 net.cpp:380] conv1 -> conv1
I1221 11:36:25.809653 39073 net.cpp:122] Setting up conv1
I1221 11:36:25.809671 39073 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I1221 11:36:25.809680 39073 net.cpp:137] Memory required for data: 76745984
I1221 11:36:25.809703 39073 layer_factory.hpp:77] Creating layer quantized_conv1
I1221 11:36:25.809718 39073 net.cpp:84] Creating Layer quantized_conv1
I1221 11:36:25.809727 39073 net.cpp:406] quantized_conv1 <- conv1
I1221 11:36:25.809741 39073 net.cpp:380] quantized_conv1 -> quantized_conv1
I1221 11:36:25.809753 39073 net.cpp:122] Setting up quantized_conv1
I1221 11:36:25.809764 39073 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I1221 11:36:25.809773 39073 net.cpp:137] Memory required for data: 113917184
I1221 11:36:25.809782 39073 layer_factory.hpp:77] Creating layer relu1
I1221 11:36:25.809793 39073 net.cpp:84] Creating Layer relu1
I1221 11:36:25.809803 39073 net.cpp:406] relu1 <- quantized_conv1
I1221 11:36:25.809813 39073 net.cpp:380] relu1 -> relu1
I1221 11:36:25.809826 39073 net.cpp:122] Setting up relu1
I1221 11:36:25.809839 39073 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I1221 11:36:25.809846 39073 net.cpp:137] Memory required for data: 151088384
I1221 11:36:25.809855 39073 layer_factory.hpp:77] Creating layer quantized_relu1
I1221 11:36:25.809867 39073 net.cpp:84] Creating Layer quantized_relu1
I1221 11:36:25.809876 39073 net.cpp:406] quantized_relu1 <- relu1
I1221 11:36:25.809888 39073 net.cpp:380] quantized_relu1 -> quantized_relu1
I1221 11:36:25.809900 39073 net.cpp:122] Setting up quantized_relu1
I1221 11:36:25.809911 39073 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I1221 11:36:25.809919 39073 net.cpp:137] Memory required for data: 188259584
I1221 11:36:25.809928 39073 layer_factory.hpp:77] Creating layer pool1
I1221 11:36:25.809942 39073 net.cpp:84] Creating Layer pool1
I1221 11:36:25.809950 39073 net.cpp:406] pool1 <- quantized_relu1
I1221 11:36:25.809960 39073 net.cpp:380] pool1 -> pool1
I1221 11:36:25.809989 39073 net.cpp:122] Setting up pool1
I1221 11:36:25.810003 39073 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I1221 11:36:25.810011 39073 net.cpp:137] Memory required for data: 197217536
I1221 11:36:25.810020 39073 layer_factory.hpp:77] Creating layer quantized_pool1
I1221 11:36:25.810034 39073 net.cpp:84] Creating Layer quantized_pool1
I1221 11:36:25.810044 39073 net.cpp:406] quantized_pool1 <- pool1
I1221 11:36:25.810055 39073 net.cpp:380] quantized_pool1 -> quantized_pool1
I1221 11:36:25.810068 39073 net.cpp:122] Setting up quantized_pool1
I1221 11:36:25.810079 39073 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I1221 11:36:25.810087 39073 net.cpp:137] Memory required for data: 206175488
I1221 11:36:25.810096 39073 layer_factory.hpp:77] Creating layer norm1
I1221 11:36:25.810111 39073 net.cpp:84] Creating Layer norm1
I1221 11:36:25.810119 39073 net.cpp:406] norm1 <- quantized_pool1
I1221 11:36:25.810130 39073 net.cpp:380] norm1 -> norm1
I1221 11:36:25.810149 39073 net.cpp:122] Setting up norm1
I1221 11:36:25.810178 39073 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I1221 11:36:25.810189 39073 net.cpp:137] Memory required for data: 215133440
I1221 11:36:25.810197 39073 layer_factory.hpp:77] Creating layer quantized_norm1
I1221 11:36:25.810209 39073 net.cpp:84] Creating Layer quantized_norm1
I1221 11:36:25.810219 39073 net.cpp:406] quantized_norm1 <- norm1
I1221 11:36:25.810230 39073 net.cpp:380] quantized_norm1 -> quantized_norm1
I1221 11:36:25.810243 39073 net.cpp:122] Setting up quantized_norm1
I1221 11:36:25.810255 39073 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I1221 11:36:25.810262 39073 net.cpp:137] Memory required for data: 224091392
I1221 11:36:25.810271 39073 layer_factory.hpp:77] Creating layer conv2
I1221 11:36:25.810286 39073 net.cpp:84] Creating Layer conv2
I1221 11:36:25.810297 39073 net.cpp:406] conv2 <- quantized_norm1
I1221 11:36:25.810309 39073 net.cpp:380] conv2 -> conv2
I1221 11:36:25.819764 39073 net.cpp:122] Setting up conv2
I1221 11:36:25.819804 39073 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I1221 11:36:25.819814 39073 net.cpp:137] Memory required for data: 247979264
I1221 11:36:25.819833 39073 layer_factory.hpp:77] Creating layer quantized_conv2
I1221 11:36:25.819854 39073 net.cpp:84] Creating Layer quantized_conv2
I1221 11:36:25.819865 39073 net.cpp:406] quantized_conv2 <- conv2
I1221 11:36:25.819882 39073 net.cpp:380] quantized_conv2 -> quantized_conv2
I1221 11:36:25.819902 39073 net.cpp:122] Setting up quantized_conv2
I1221 11:36:25.819914 39073 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I1221 11:36:25.819923 39073 net.cpp:137] Memory required for data: 271867136
I1221 11:36:25.819934 39073 layer_factory.hpp:77] Creating layer relu2
I1221 11:36:25.819946 39073 net.cpp:84] Creating Layer relu2
I1221 11:36:25.819955 39073 net.cpp:406] relu2 <- quantized_conv2
I1221 11:36:25.819965 39073 net.cpp:380] relu2 -> relu2
I1221 11:36:25.819979 39073 net.cpp:122] Setting up relu2
I1221 11:36:25.819993 39073 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I1221 11:36:25.820001 39073 net.cpp:137] Memory required for data: 295755008
I1221 11:36:25.820010 39073 layer_factory.hpp:77] Creating layer quantized_relu2
I1221 11:36:25.820024 39073 net.cpp:84] Creating Layer quantized_relu2
I1221 11:36:25.820034 39073 net.cpp:406] quantized_relu2 <- relu2
I1221 11:36:25.820044 39073 net.cpp:380] quantized_relu2 -> quantized_relu2
I1221 11:36:25.820057 39073 net.cpp:122] Setting up quantized_relu2
I1221 11:36:25.820071 39073 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I1221 11:36:25.820096 39073 net.cpp:137] Memory required for data: 319642880
I1221 11:36:25.820104 39073 layer_factory.hpp:77] Creating layer pool2
I1221 11:36:25.820116 39073 net.cpp:84] Creating Layer pool2
I1221 11:36:25.820125 39073 net.cpp:406] pool2 <- quantized_relu2
I1221 11:36:25.820137 39073 net.cpp:380] pool2 -> pool2
I1221 11:36:25.820153 39073 net.cpp:122] Setting up pool2
I1221 11:36:25.820164 39073 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I1221 11:36:25.820173 39073 net.cpp:137] Memory required for data: 325180672
I1221 11:36:25.820183 39073 layer_factory.hpp:77] Creating layer quantized_pool2
I1221 11:36:25.820199 39073 net.cpp:84] Creating Layer quantized_pool2
I1221 11:36:25.820209 39073 net.cpp:406] quantized_pool2 <- pool2
I1221 11:36:25.820222 39073 net.cpp:380] quantized_pool2 -> quantized_pool2
I1221 11:36:25.820236 39073 net.cpp:122] Setting up quantized_pool2
I1221 11:36:25.820247 39073 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I1221 11:36:25.820256 39073 net.cpp:137] Memory required for data: 330718464
I1221 11:36:25.820264 39073 layer_factory.hpp:77] Creating layer norm2
I1221 11:36:25.820277 39073 net.cpp:84] Creating Layer norm2
I1221 11:36:25.820287 39073 net.cpp:406] norm2 <- quantized_pool2
I1221 11:36:25.820307 39073 net.cpp:380] norm2 -> norm2
I1221 11:36:25.820322 39073 net.cpp:122] Setting up norm2
I1221 11:36:25.820333 39073 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I1221 11:36:25.820340 39073 net.cpp:137] Memory required for data: 336256256
I1221 11:36:25.820382 39073 layer_factory.hpp:77] Creating layer quantized_norm2
I1221 11:36:25.820394 39073 net.cpp:84] Creating Layer quantized_norm2
I1221 11:36:25.820405 39073 net.cpp:406] quantized_norm2 <- norm2
I1221 11:36:25.820417 39073 net.cpp:380] quantized_norm2 -> quantized_norm2
I1221 11:36:25.820431 39073 net.cpp:122] Setting up quantized_norm2
I1221 11:36:25.820442 39073 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I1221 11:36:25.820451 39073 net.cpp:137] Memory required for data: 341794048
I1221 11:36:25.820461 39073 layer_factory.hpp:77] Creating layer conv3
I1221 11:36:25.820482 39073 net.cpp:84] Creating Layer conv3
I1221 11:36:25.820492 39073 net.cpp:406] conv3 <- quantized_norm2
I1221 11:36:25.820503 39073 net.cpp:380] conv3 -> conv3
I1221 11:36:25.848332 39073 net.cpp:122] Setting up conv3
I1221 11:36:25.848384 39073 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I1221 11:36:25.848394 39073 net.cpp:137] Memory required for data: 350100736
I1221 11:36:25.848414 39073 layer_factory.hpp:77] Creating layer quantized_conv3
I1221 11:36:25.848433 39073 net.cpp:84] Creating Layer quantized_conv3
I1221 11:36:25.848445 39073 net.cpp:406] quantized_conv3 <- conv3
I1221 11:36:25.848462 39073 net.cpp:380] quantized_conv3 -> quantized_conv3
I1221 11:36:25.848484 39073 net.cpp:122] Setting up quantized_conv3
I1221 11:36:25.848495 39073 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I1221 11:36:25.848503 39073 net.cpp:137] Memory required for data: 358407424
I1221 11:36:25.848515 39073 layer_factory.hpp:77] Creating layer relu3
I1221 11:36:25.848526 39073 net.cpp:84] Creating Layer relu3
I1221 11:36:25.848534 39073 net.cpp:406] relu3 <- quantized_conv3
I1221 11:36:25.848549 39073 net.cpp:380] relu3 -> relu3
I1221 11:36:25.848562 39073 net.cpp:122] Setting up relu3
I1221 11:36:25.848573 39073 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I1221 11:36:25.848582 39073 net.cpp:137] Memory required for data: 366714112
I1221 11:36:25.848590 39073 layer_factory.hpp:77] Creating layer quantized_relu3
I1221 11:36:25.848601 39073 net.cpp:84] Creating Layer quantized_relu3
I1221 11:36:25.848610 39073 net.cpp:406] quantized_relu3 <- relu3
I1221 11:36:25.848621 39073 net.cpp:380] quantized_relu3 -> quantized_relu3
I1221 11:36:25.848634 39073 net.cpp:122] Setting up quantized_relu3
I1221 11:36:25.848644 39073 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I1221 11:36:25.848654 39073 net.cpp:137] Memory required for data: 375020800
I1221 11:36:25.848662 39073 layer_factory.hpp:77] Creating layer conv4
I1221 11:36:25.848690 39073 net.cpp:84] Creating Layer conv4
I1221 11:36:25.848701 39073 net.cpp:406] conv4 <- quantized_relu3
I1221 11:36:25.848713 39073 net.cpp:380] conv4 -> conv4
I1221 11:36:25.869089 39073 net.cpp:122] Setting up conv4
I1221 11:36:25.869139 39073 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I1221 11:36:25.869149 39073 net.cpp:137] Memory required for data: 383327488
I1221 11:36:25.869164 39073 layer_factory.hpp:77] Creating layer quantized_conv4
I1221 11:36:25.869184 39073 net.cpp:84] Creating Layer quantized_conv4
I1221 11:36:25.869196 39073 net.cpp:406] quantized_conv4 <- conv4
I1221 11:36:25.869215 39073 net.cpp:380] quantized_conv4 -> quantized_conv4
I1221 11:36:25.869235 39073 net.cpp:122] Setting up quantized_conv4
I1221 11:36:25.869246 39073 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I1221 11:36:25.869254 39073 net.cpp:137] Memory required for data: 391634176
I1221 11:36:25.869263 39073 layer_factory.hpp:77] Creating layer relu4
I1221 11:36:25.869274 39073 net.cpp:84] Creating Layer relu4
I1221 11:36:25.869284 39073 net.cpp:406] relu4 <- quantized_conv4
I1221 11:36:25.869295 39073 net.cpp:380] relu4 -> relu4
I1221 11:36:25.869307 39073 net.cpp:122] Setting up relu4
I1221 11:36:25.869319 39073 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I1221 11:36:25.869328 39073 net.cpp:137] Memory required for data: 399940864
I1221 11:36:25.869336 39073 layer_factory.hpp:77] Creating layer quantized_relu4
I1221 11:36:25.869350 39073 net.cpp:84] Creating Layer quantized_relu4
I1221 11:36:25.869359 39073 net.cpp:406] quantized_relu4 <- relu4
I1221 11:36:25.869408 39073 net.cpp:380] quantized_relu4 -> quantized_relu4
I1221 11:36:25.869423 39073 net.cpp:122] Setting up quantized_relu4
I1221 11:36:25.869434 39073 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I1221 11:36:25.869443 39073 net.cpp:137] Memory required for data: 408247552
I1221 11:36:25.869452 39073 layer_factory.hpp:77] Creating layer conv5
I1221 11:36:25.869472 39073 net.cpp:84] Creating Layer conv5
I1221 11:36:25.869482 39073 net.cpp:406] conv5 <- quantized_relu4
I1221 11:36:25.869498 39073 net.cpp:380] conv5 -> conv5
I1221 11:36:25.883436 39073 net.cpp:122] Setting up conv5
I1221 11:36:25.883484 39073 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I1221 11:36:25.883494 39073 net.cpp:137] Memory required for data: 413785344
I1221 11:36:25.883515 39073 layer_factory.hpp:77] Creating layer quantized_conv5
I1221 11:36:25.883534 39073 net.cpp:84] Creating Layer quantized_conv5
I1221 11:36:25.883546 39073 net.cpp:406] quantized_conv5 <- conv5
I1221 11:36:25.883563 39073 net.cpp:380] quantized_conv5 -> quantized_conv5
I1221 11:36:25.883584 39073 net.cpp:122] Setting up quantized_conv5
I1221 11:36:25.883595 39073 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I1221 11:36:25.883605 39073 net.cpp:137] Memory required for data: 419323136
I1221 11:36:25.883613 39073 layer_factory.hpp:77] Creating layer relu5
I1221 11:36:25.883627 39073 net.cpp:84] Creating Layer relu5
I1221 11:36:25.883638 39073 net.cpp:406] relu5 <- quantized_conv5
I1221 11:36:25.883648 39073 net.cpp:380] relu5 -> relu5
I1221 11:36:25.883666 39073 net.cpp:122] Setting up relu5
I1221 11:36:25.883679 39073 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I1221 11:36:25.883688 39073 net.cpp:137] Memory required for data: 424860928
I1221 11:36:25.883697 39073 layer_factory.hpp:77] Creating layer quantized_relu5
I1221 11:36:25.883708 39073 net.cpp:84] Creating Layer quantized_relu5
I1221 11:36:25.883718 39073 net.cpp:406] quantized_relu5 <- relu5
I1221 11:36:25.883728 39073 net.cpp:380] quantized_relu5 -> quantized_relu5
I1221 11:36:25.883740 39073 net.cpp:122] Setting up quantized_relu5
I1221 11:36:25.883751 39073 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I1221 11:36:25.883760 39073 net.cpp:137] Memory required for data: 430398720
I1221 11:36:25.883769 39073 layer_factory.hpp:77] Creating layer pool5
I1221 11:36:25.883785 39073 net.cpp:84] Creating Layer pool5
I1221 11:36:25.883795 39073 net.cpp:406] pool5 <- quantized_relu5
I1221 11:36:25.883810 39073 net.cpp:380] pool5 -> pool5
I1221 11:36:25.883828 39073 net.cpp:122] Setting up pool5
I1221 11:36:25.883841 39073 net.cpp:129] Top shape: 32 256 6 6 (294912)
I1221 11:36:25.883849 39073 net.cpp:137] Memory required for data: 431578368
I1221 11:36:25.883858 39073 layer_factory.hpp:77] Creating layer quantized_pool5
I1221 11:36:25.883873 39073 net.cpp:84] Creating Layer quantized_pool5
I1221 11:36:25.883883 39073 net.cpp:406] quantized_pool5 <- pool5
I1221 11:36:25.883898 39073 net.cpp:380] quantized_pool5 -> quantized_pool5
I1221 11:36:25.883910 39073 net.cpp:122] Setting up quantized_pool5
I1221 11:36:25.883922 39073 net.cpp:129] Top shape: 32 256 6 6 (294912)
I1221 11:36:25.883931 39073 net.cpp:137] Memory required for data: 432758016
I1221 11:36:25.883939 39073 layer_factory.hpp:77] Creating layer fc6
I1221 11:36:25.883961 39073 net.cpp:84] Creating Layer fc6
I1221 11:36:25.883973 39073 net.cpp:406] fc6 <- quantized_pool5
I1221 11:36:25.883985 39073 net.cpp:380] fc6 -> fc6
I1221 11:36:27.008383 39073 net.cpp:122] Setting up fc6
I1221 11:36:27.008446 39073 net.cpp:129] Top shape: 32 4096 (131072)
I1221 11:36:27.008457 39073 net.cpp:137] Memory required for data: 433282304
I1221 11:36:27.008474 39073 layer_factory.hpp:77] Creating layer quantized_fc6
I1221 11:36:27.008497 39073 net.cpp:84] Creating Layer quantized_fc6
I1221 11:36:27.008510 39073 net.cpp:406] quantized_fc6 <- fc6
I1221 11:36:27.008527 39073 net.cpp:380] quantized_fc6 -> quantized_fc6
I1221 11:36:27.008548 39073 net.cpp:122] Setting up quantized_fc6
I1221 11:36:27.008559 39073 net.cpp:129] Top shape: 32 4096 (131072)
I1221 11:36:27.008606 39073 net.cpp:137] Memory required for data: 433806592
I1221 11:36:27.008616 39073 layer_factory.hpp:77] Creating layer relu6
I1221 11:36:27.008628 39073 net.cpp:84] Creating Layer relu6
I1221 11:36:27.008638 39073 net.cpp:406] relu6 <- quantized_fc6
I1221 11:36:27.008649 39073 net.cpp:380] relu6 -> relu6
I1221 11:36:27.008662 39073 net.cpp:122] Setting up relu6
I1221 11:36:27.008673 39073 net.cpp:129] Top shape: 32 4096 (131072)
I1221 11:36:27.008682 39073 net.cpp:137] Memory required for data: 434330880
I1221 11:36:27.008690 39073 layer_factory.hpp:77] Creating layer quantized_relu6
I1221 11:36:27.008704 39073 net.cpp:84] Creating Layer quantized_relu6
I1221 11:36:27.008713 39073 net.cpp:406] quantized_relu6 <- relu6
I1221 11:36:27.008725 39073 net.cpp:380] quantized_relu6 -> quantized_relu6
I1221 11:36:27.008738 39073 net.cpp:122] Setting up quantized_relu6
I1221 11:36:27.008749 39073 net.cpp:129] Top shape: 32 4096 (131072)
I1221 11:36:27.008757 39073 net.cpp:137] Memory required for data: 434855168
I1221 11:36:27.008766 39073 layer_factory.hpp:77] Creating layer fc7
I1221 11:36:27.008780 39073 net.cpp:84] Creating Layer fc7
I1221 11:36:27.008790 39073 net.cpp:406] fc7 <- quantized_relu6
I1221 11:36:27.008800 39073 net.cpp:380] fc7 -> fc7
I1221 11:36:27.504279 39073 net.cpp:122] Setting up fc7
I1221 11:36:27.504336 39073 net.cpp:129] Top shape: 32 4096 (131072)
I1221 11:36:27.504346 39073 net.cpp:137] Memory required for data: 435379456
I1221 11:36:27.504364 39073 layer_factory.hpp:77] Creating layer quantized_fc7
I1221 11:36:27.504382 39073 net.cpp:84] Creating Layer quantized_fc7
I1221 11:36:27.504395 39073 net.cpp:406] quantized_fc7 <- fc7
I1221 11:36:27.504410 39073 net.cpp:380] quantized_fc7 -> quantized_fc7
I1221 11:36:27.504431 39073 net.cpp:122] Setting up quantized_fc7
I1221 11:36:27.504441 39073 net.cpp:129] Top shape: 32 4096 (131072)
I1221 11:36:27.504451 39073 net.cpp:137] Memory required for data: 435903744
I1221 11:36:27.504458 39073 layer_factory.hpp:77] Creating layer relu7
I1221 11:36:27.504469 39073 net.cpp:84] Creating Layer relu7
I1221 11:36:27.504478 39073 net.cpp:406] relu7 <- quantized_fc7
I1221 11:36:27.504492 39073 net.cpp:380] relu7 -> relu7
I1221 11:36:27.504505 39073 net.cpp:122] Setting up relu7
I1221 11:36:27.504516 39073 net.cpp:129] Top shape: 32 4096 (131072)
I1221 11:36:27.504524 39073 net.cpp:137] Memory required for data: 436428032
I1221 11:36:27.504534 39073 layer_factory.hpp:77] Creating layer quantized_relu7
I1221 11:36:27.504544 39073 net.cpp:84] Creating Layer quantized_relu7
I1221 11:36:27.504552 39073 net.cpp:406] quantized_relu7 <- relu7
I1221 11:36:27.504565 39073 net.cpp:380] quantized_relu7 -> quantized_relu7
I1221 11:36:27.504576 39073 net.cpp:122] Setting up quantized_relu7
I1221 11:36:27.504587 39073 net.cpp:129] Top shape: 32 4096 (131072)
I1221 11:36:27.504595 39073 net.cpp:137] Memory required for data: 436952320
I1221 11:36:27.504603 39073 layer_factory.hpp:77] Creating layer fc8
I1221 11:36:27.504617 39073 net.cpp:84] Creating Layer fc8
I1221 11:36:27.504626 39073 net.cpp:406] fc8 <- quantized_relu7
I1221 11:36:27.504637 39073 net.cpp:380] fc8 -> fc8
I1221 11:36:27.511991 39073 net.cpp:122] Setting up fc8
I1221 11:36:27.512050 39073 net.cpp:129] Top shape: 32 1000 (32000)
I1221 11:36:27.512060 39073 net.cpp:137] Memory required for data: 437080320
I1221 11:36:27.512078 39073 layer_factory.hpp:77] Creating layer quantized_fc8
I1221 11:36:27.512099 39073 net.cpp:84] Creating Layer quantized_fc8
I1221 11:36:27.512112 39073 net.cpp:406] quantized_fc8 <- fc8
I1221 11:36:27.512128 39073 net.cpp:380] quantized_fc8 -> quantized_fc8
I1221 11:36:27.512150 39073 net.cpp:122] Setting up quantized_fc8
I1221 11:36:27.512161 39073 net.cpp:129] Top shape: 32 1000 (32000)
I1221 11:36:27.512168 39073 net.cpp:137] Memory required for data: 437208320
I1221 11:36:27.512177 39073 layer_factory.hpp:77] Creating layer quantized_fc8_quantized_fc8_0_split
I1221 11:36:27.512189 39073 net.cpp:84] Creating Layer quantized_fc8_quantized_fc8_0_split
I1221 11:36:27.512238 39073 net.cpp:406] quantized_fc8_quantized_fc8_0_split <- quantized_fc8
I1221 11:36:27.512253 39073 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_0
I1221 11:36:27.512266 39073 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_1
I1221 11:36:27.512279 39073 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_2
I1221 11:36:27.512291 39073 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_3
I1221 11:36:27.512312 39073 net.cpp:122] Setting up quantized_fc8_quantized_fc8_0_split
I1221 11:36:27.512325 39073 net.cpp:129] Top shape: 32 1000 (32000)
I1221 11:36:27.512333 39073 net.cpp:129] Top shape: 32 1000 (32000)
I1221 11:36:27.512342 39073 net.cpp:129] Top shape: 32 1000 (32000)
I1221 11:36:27.512351 39073 net.cpp:129] Top shape: 32 1000 (32000)
I1221 11:36:27.512359 39073 net.cpp:137] Memory required for data: 437720320
I1221 11:36:27.512368 39073 layer_factory.hpp:77] Creating layer probs
I1221 11:36:27.512380 39073 net.cpp:84] Creating Layer probs
I1221 11:36:27.512388 39073 net.cpp:406] probs <- quantized_fc8_quantized_fc8_0_split_0
I1221 11:36:27.512399 39073 net.cpp:380] probs -> probs
I1221 11:36:27.512428 39073 net.cpp:122] Setting up probs
I1221 11:36:27.512441 39073 net.cpp:129] Top shape: 32 1000 (32000)
I1221 11:36:27.512449 39073 net.cpp:137] Memory required for data: 437848320
I1221 11:36:27.512457 39073 layer_factory.hpp:77] Creating layer loss
I1221 11:36:27.512472 39073 net.cpp:84] Creating Layer loss
I1221 11:36:27.512481 39073 net.cpp:406] loss <- quantized_fc8_quantized_fc8_0_split_1
I1221 11:36:27.512491 39073 net.cpp:406] loss <- label_data_1_split_0
I1221 11:36:27.512504 39073 net.cpp:380] loss -> loss
I1221 11:36:27.512529 39073 layer_factory.hpp:77] Creating layer loss
I1221 11:36:27.512645 39073 net.cpp:122] Setting up loss
I1221 11:36:27.512662 39073 net.cpp:129] Top shape: (1)
I1221 11:36:27.512671 39073 net.cpp:132]     with loss weight 1
I1221 11:36:27.512706 39073 net.cpp:137] Memory required for data: 437848324
I1221 11:36:27.512715 39073 layer_factory.hpp:77] Creating layer acc_top1
I1221 11:36:27.512732 39073 net.cpp:84] Creating Layer acc_top1
I1221 11:36:27.512742 39073 net.cpp:406] acc_top1 <- quantized_fc8_quantized_fc8_0_split_2
I1221 11:36:27.512753 39073 net.cpp:406] acc_top1 <- label_data_1_split_1
I1221 11:36:27.512763 39073 net.cpp:380] acc_top1 -> acc_top1
I1221 11:36:27.512783 39073 net.cpp:122] Setting up acc_top1
I1221 11:36:27.512794 39073 net.cpp:129] Top shape: (1)
I1221 11:36:27.512802 39073 net.cpp:137] Memory required for data: 437848328
I1221 11:36:27.512811 39073 layer_factory.hpp:77] Creating layer acc_top5
I1221 11:36:27.512825 39073 net.cpp:84] Creating Layer acc_top5
I1221 11:36:27.512833 39073 net.cpp:406] acc_top5 <- quantized_fc8_quantized_fc8_0_split_3
I1221 11:36:27.512843 39073 net.cpp:406] acc_top5 <- label_data_1_split_2
I1221 11:36:27.512854 39073 net.cpp:380] acc_top5 -> acc_top5
I1221 11:36:27.512866 39073 net.cpp:122] Setting up acc_top5
I1221 11:36:27.512877 39073 net.cpp:129] Top shape: (1)
I1221 11:36:27.512886 39073 net.cpp:137] Memory required for data: 437848332
I1221 11:36:27.512894 39073 net.cpp:200] acc_top5 does not need backward computation.
I1221 11:36:27.512903 39073 net.cpp:200] acc_top1 does not need backward computation.
I1221 11:36:27.512912 39073 net.cpp:198] loss needs backward computation.
I1221 11:36:27.512922 39073 net.cpp:200] probs does not need backward computation.
I1221 11:36:27.512929 39073 net.cpp:198] quantized_fc8_quantized_fc8_0_split needs backward computation.
I1221 11:36:27.512938 39073 net.cpp:198] quantized_fc8 needs backward computation.
I1221 11:36:27.512946 39073 net.cpp:198] fc8 needs backward computation.
I1221 11:36:27.512955 39073 net.cpp:198] quantized_relu7 needs backward computation.
I1221 11:36:27.512964 39073 net.cpp:198] relu7 needs backward computation.
I1221 11:36:27.512974 39073 net.cpp:198] quantized_fc7 needs backward computation.
I1221 11:36:27.512995 39073 net.cpp:198] fc7 needs backward computation.
I1221 11:36:27.513005 39073 net.cpp:198] quantized_relu6 needs backward computation.
I1221 11:36:27.513013 39073 net.cpp:198] relu6 needs backward computation.
I1221 11:36:27.513022 39073 net.cpp:198] quantized_fc6 needs backward computation.
I1221 11:36:27.513031 39073 net.cpp:198] fc6 needs backward computation.
I1221 11:36:27.513039 39073 net.cpp:198] quantized_pool5 needs backward computation.
I1221 11:36:27.513048 39073 net.cpp:198] pool5 needs backward computation.
I1221 11:36:27.513057 39073 net.cpp:198] quantized_relu5 needs backward computation.
I1221 11:36:27.513067 39073 net.cpp:198] relu5 needs backward computation.
I1221 11:36:27.513075 39073 net.cpp:198] quantized_conv5 needs backward computation.
I1221 11:36:27.513083 39073 net.cpp:198] conv5 needs backward computation.
I1221 11:36:27.513097 39073 net.cpp:198] quantized_relu4 needs backward computation.
I1221 11:36:27.513105 39073 net.cpp:198] relu4 needs backward computation.
I1221 11:36:27.513114 39073 net.cpp:198] quantized_conv4 needs backward computation.
I1221 11:36:27.513123 39073 net.cpp:198] conv4 needs backward computation.
I1221 11:36:27.513131 39073 net.cpp:198] quantized_relu3 needs backward computation.
I1221 11:36:27.513140 39073 net.cpp:198] relu3 needs backward computation.
I1221 11:36:27.513149 39073 net.cpp:198] quantized_conv3 needs backward computation.
I1221 11:36:27.513157 39073 net.cpp:198] conv3 needs backward computation.
I1221 11:36:27.513166 39073 net.cpp:198] quantized_norm2 needs backward computation.
I1221 11:36:27.513175 39073 net.cpp:198] norm2 needs backward computation.
I1221 11:36:27.513183 39073 net.cpp:198] quantized_pool2 needs backward computation.
I1221 11:36:27.513192 39073 net.cpp:198] pool2 needs backward computation.
I1221 11:36:27.513201 39073 net.cpp:198] quantized_relu2 needs backward computation.
I1221 11:36:27.513211 39073 net.cpp:198] relu2 needs backward computation.
I1221 11:36:27.513218 39073 net.cpp:198] quantized_conv2 needs backward computation.
I1221 11:36:27.513227 39073 net.cpp:198] conv2 needs backward computation.
I1221 11:36:27.513236 39073 net.cpp:198] quantized_norm1 needs backward computation.
I1221 11:36:27.513244 39073 net.cpp:198] norm1 needs backward computation.
I1221 11:36:27.513253 39073 net.cpp:198] quantized_pool1 needs backward computation.
I1221 11:36:27.513262 39073 net.cpp:198] pool1 needs backward computation.
I1221 11:36:27.513270 39073 net.cpp:198] quantized_relu1 needs backward computation.
I1221 11:36:27.513279 39073 net.cpp:198] relu1 needs backward computation.
I1221 11:36:27.513288 39073 net.cpp:198] quantized_conv1 needs backward computation.
I1221 11:36:27.513296 39073 net.cpp:198] conv1 needs backward computation.
I1221 11:36:27.513305 39073 net.cpp:200] quantized_data does not need backward computation.
I1221 11:36:27.513315 39073 net.cpp:200] label_data_1_split does not need backward computation.
I1221 11:36:27.513324 39073 net.cpp:200] data does not need backward computation.
I1221 11:36:27.513332 39073 net.cpp:242] This network produces output acc_top1
I1221 11:36:27.513340 39073 net.cpp:242] This network produces output acc_top5
I1221 11:36:27.513350 39073 net.cpp:242] This network produces output loss
I1221 11:36:27.513357 39073 net.cpp:242] This network produces output probs
I1221 11:36:27.513393 39073 net.cpp:255] Network initialization done.
I1221 11:36:27.514315 39073 solver.cpp:172] Creating test net (#0) specified by net file: qnn_try1/train_quantized_caffenet.prototxt
I1221 11:36:27.514387 39073 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1221 11:36:27.514724 39073 net.cpp:51] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/ydwu/database/imagenet/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/ydwu/database/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_val_lmdb/"
    batch_size: 20
    backend: LMDB
  }
}
layer {
  name: "quantized_data"
  type: "Quantization"
  bottom: "data"
  top: "quantized_data"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -129.93526
    range: 160.78912
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "quantized_data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv1"
  type: "Quantization"
  bottom: "conv1"
  top: "quantized_conv1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -2732.7351
    range: 2531.042
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "quantized_conv1"
  top: "relu1"
}
layer {
  name: "quantized_relu1"
  type: "Quantization"
  bottom: "relu1"
  top: "quantized_relu1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 2531.042
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "quantized_relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool1"
  type: "Quantization"
  bottom: "pool1"
  top: "quantized_pool1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 2531.042
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "quantized_pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "quantized_norm1"
  type: "Quantization"
  bottom: "norm1"
  top: "quantized_norm1"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 138.70576
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "quantized_norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv2"
  type: "Quantization"
  bottom: "conv2"
  top: "quantized_conv2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -647.61908
    range: 492.23111
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "quantized_conv2"
  top: "relu2"
}
layer {
  name: "quantized_relu2"
  type: "Quantization"
  bottom: "relu2"
  top: "quantized_relu2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 492.23111
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "quantized_relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool2"
  type: "Quantization"
  bottom: "pool2"
  top: "quantized_pool2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 492.23111
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "quantized_pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "quantized_norm2"
  type: "Quantization"
  bottom: "norm2"
  top: "quantized_norm2"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 138.72636
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "quantized_norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv3"
  type: "Quantization"
  bottom: "conv3"
  top: "quantized_conv3"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -389.56516
    range: 331.98178
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "quantized_conv3"
  top: "relu3"
}
layer {
  name: "quantized_relu3"
  type: "Quantization"
  bottom: "relu3"
  top: "quantized_relu3"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 331.98178
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "quantized_relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv4"
  type: "Quantization"
  bottom: "conv4"
  top: "quantized_conv4"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -231.20598
    range: 283.87189
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "quantized_conv4"
  top: "relu4"
}
layer {
  name: "quantized_relu4"
  type: "Quantization"
  bottom: "relu4"
  top: "quantized_relu4"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 283.87189
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "quantized_relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1.1e+13
    }
    bias_filler {
      type: "constant"
      std: 0.1
    }
  }
}
layer {
  name: "quantized_conv5"
  type: "Quantization"
  bottom: "conv5"
  top: "quantized_conv5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -172.60567
    range: 272.57315
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "quantized_conv5"
  top: "relu5"
}
layer {
  name: "quantized_relu5"
  type: "Quantization"
  bottom: "relu5"
  top: "quantized_relu5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 272.57315
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "quantized_relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "quantized_pool5"
  type: "Quantization"
  bottom: "pool5"
  top: "quantized_pool5"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 272.57315
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "quantized_pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 1.05e+13
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "quantized_fc6"
  type: "Quantization"
  bottom: "fc6"
  top: "quantized_fc6"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -114.02487
    range: 58.541733
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "quantized_fc6"
  top: "relu6"
}
layer {
  name: "quantized_relu6"
  type: "Quantization"
  bottom: "relu6"
  top: "quantized_relu6"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 58.541733
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "quantized_relu6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 1.05e+13
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "quantized_fc7"
  type: "Quantization"
  bottom: "fc7"
  top: "quantized_fc7"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -26.019335
    range: 19.125257
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "quantized_fc7"
  top: "relu7"
}
layer {
  name: "quantized_relu7"
  type: "Quantization"
  bottom: "relu7"
  top: "quantized_relu7"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: 1
    range: 19.125257
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "quantized_relu7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
  }
}
layer {
  name: "quantized_fc8"
  type: "Quantization"
  bottom: "fc8"
  top: "quantized_fc8"
  quantization_param {
    round_method: FLOOR
    round_strategy: AGGRESSIVE
    bit_width: 8
    range: -9.849781
    range: 45.688446
  }
}
layer {
  name: "probs"
  type: "Softmax"
  bottom: "quantized_fc8"
  top: "probs"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "acc_top1"
  type: "Accuracy"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "acc_top1"
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "acc_top5"
  type: "Accuracy"
  bottom: "quantized_fc8"
  bottom: "label"
  top: "acc_top5"
  accuracy_param {
    top_k: 5
  }
}
I1221 11:36:27.514974 39073 layer_factory.hpp:77] Creating layer data
I1221 11:36:27.515061 39073 db_lmdb.cpp:35] Opened lmdb /home/ydwu/database/imagenet/ilsvrc12/lmdb_resize/ilsvrc12_val_lmdb/
I1221 11:36:27.515091 39073 net.cpp:84] Creating Layer data
I1221 11:36:27.515105 39073 net.cpp:380] data -> data
I1221 11:36:27.515120 39073 net.cpp:380] data -> label
I1221 11:36:27.515136 39073 data_transformer.cpp:25] Loading mean file from: /home/ydwu/database/imagenet/ilsvrc12/imagenet_mean.binaryproto
I1221 11:36:27.517438 39073 data_layer.cpp:45] output data size: 20,3,227,227
I1221 11:36:27.531005 39073 net.cpp:122] Setting up data
I1221 11:36:27.531069 39073 net.cpp:129] Top shape: 20 3 227 227 (3091740)
I1221 11:36:27.531086 39073 net.cpp:129] Top shape: 20 (20)
I1221 11:36:27.531095 39073 net.cpp:137] Memory required for data: 12367040
I1221 11:36:27.531108 39073 layer_factory.hpp:77] Creating layer label_data_1_split
I1221 11:36:27.531131 39073 net.cpp:84] Creating Layer label_data_1_split
I1221 11:36:27.531143 39073 net.cpp:406] label_data_1_split <- label
I1221 11:36:27.531157 39073 net.cpp:380] label_data_1_split -> label_data_1_split_0
I1221 11:36:27.531177 39073 net.cpp:380] label_data_1_split -> label_data_1_split_1
I1221 11:36:27.531189 39073 net.cpp:380] label_data_1_split -> label_data_1_split_2
I1221 11:36:27.531206 39073 net.cpp:122] Setting up label_data_1_split
I1221 11:36:27.531217 39073 net.cpp:129] Top shape: 20 (20)
I1221 11:36:27.531226 39073 net.cpp:129] Top shape: 20 (20)
I1221 11:36:27.531239 39073 net.cpp:129] Top shape: 20 (20)
I1221 11:36:27.531246 39073 net.cpp:137] Memory required for data: 12367280
I1221 11:36:27.531255 39073 layer_factory.hpp:77] Creating layer quantized_data
I1221 11:36:27.531270 39073 net.cpp:84] Creating Layer quantized_data
I1221 11:36:27.531278 39073 net.cpp:406] quantized_data <- data
I1221 11:36:27.531291 39073 net.cpp:380] quantized_data -> quantized_data
I1221 11:36:27.531306 39073 net.cpp:122] Setting up quantized_data
I1221 11:36:27.531316 39073 net.cpp:129] Top shape: 20 3 227 227 (3091740)
I1221 11:36:27.531324 39073 net.cpp:137] Memory required for data: 24734240
I1221 11:36:27.531333 39073 layer_factory.hpp:77] Creating layer conv1
I1221 11:36:27.531354 39073 net.cpp:84] Creating Layer conv1
I1221 11:36:27.531364 39073 net.cpp:406] conv1 <- quantized_data
I1221 11:36:27.531414 39073 net.cpp:380] conv1 -> conv1
I1221 11:36:27.532531 39073 net.cpp:122] Setting up conv1
I1221 11:36:27.532547 39073 net.cpp:129] Top shape: 20 96 55 55 (5808000)
I1221 11:36:27.532557 39073 net.cpp:137] Memory required for data: 47966240
I1221 11:36:27.532572 39073 layer_factory.hpp:77] Creating layer quantized_conv1
I1221 11:36:27.532588 39073 net.cpp:84] Creating Layer quantized_conv1
I1221 11:36:27.532599 39073 net.cpp:406] quantized_conv1 <- conv1
I1221 11:36:27.532610 39073 net.cpp:380] quantized_conv1 -> quantized_conv1
I1221 11:36:27.532624 39073 net.cpp:122] Setting up quantized_conv1
I1221 11:36:27.532635 39073 net.cpp:129] Top shape: 20 96 55 55 (5808000)
I1221 11:36:27.532644 39073 net.cpp:137] Memory required for data: 71198240
I1221 11:36:27.532652 39073 layer_factory.hpp:77] Creating layer relu1
I1221 11:36:27.532663 39073 net.cpp:84] Creating Layer relu1
I1221 11:36:27.532672 39073 net.cpp:406] relu1 <- quantized_conv1
I1221 11:36:27.532685 39073 net.cpp:380] relu1 -> relu1
I1221 11:36:27.532698 39073 net.cpp:122] Setting up relu1
I1221 11:36:27.532711 39073 net.cpp:129] Top shape: 20 96 55 55 (5808000)
I1221 11:36:27.532718 39073 net.cpp:137] Memory required for data: 94430240
I1221 11:36:27.532727 39073 layer_factory.hpp:77] Creating layer quantized_relu1
I1221 11:36:27.532737 39073 net.cpp:84] Creating Layer quantized_relu1
I1221 11:36:27.532747 39073 net.cpp:406] quantized_relu1 <- relu1
I1221 11:36:27.532759 39073 net.cpp:380] quantized_relu1 -> quantized_relu1
I1221 11:36:27.532773 39073 net.cpp:122] Setting up quantized_relu1
I1221 11:36:27.532783 39073 net.cpp:129] Top shape: 20 96 55 55 (5808000)
I1221 11:36:27.532791 39073 net.cpp:137] Memory required for data: 117662240
I1221 11:36:27.532799 39073 layer_factory.hpp:77] Creating layer pool1
I1221 11:36:27.532812 39073 net.cpp:84] Creating Layer pool1
I1221 11:36:27.532820 39073 net.cpp:406] pool1 <- quantized_relu1
I1221 11:36:27.532833 39073 net.cpp:380] pool1 -> pool1
I1221 11:36:27.532850 39073 net.cpp:122] Setting up pool1
I1221 11:36:27.532860 39073 net.cpp:129] Top shape: 20 96 27 27 (1399680)
I1221 11:36:27.532869 39073 net.cpp:137] Memory required for data: 123260960
I1221 11:36:27.532877 39073 layer_factory.hpp:77] Creating layer quantized_pool1
I1221 11:36:27.532888 39073 net.cpp:84] Creating Layer quantized_pool1
I1221 11:36:27.532897 39073 net.cpp:406] quantized_pool1 <- pool1
I1221 11:36:27.532912 39073 net.cpp:380] quantized_pool1 -> quantized_pool1
I1221 11:36:27.532923 39073 net.cpp:122] Setting up quantized_pool1
I1221 11:36:27.532934 39073 net.cpp:129] Top shape: 20 96 27 27 (1399680)
I1221 11:36:27.532943 39073 net.cpp:137] Memory required for data: 128859680
I1221 11:36:27.532950 39073 layer_factory.hpp:77] Creating layer norm1
I1221 11:36:27.532963 39073 net.cpp:84] Creating Layer norm1
I1221 11:36:27.532972 39073 net.cpp:406] norm1 <- quantized_pool1
I1221 11:36:27.532985 39073 net.cpp:380] norm1 -> norm1
I1221 11:36:27.532999 39073 net.cpp:122] Setting up norm1
I1221 11:36:27.533010 39073 net.cpp:129] Top shape: 20 96 27 27 (1399680)
I1221 11:36:27.533018 39073 net.cpp:137] Memory required for data: 134458400
I1221 11:36:27.533026 39073 layer_factory.hpp:77] Creating layer quantized_norm1
I1221 11:36:27.533040 39073 net.cpp:84] Creating Layer quantized_norm1
I1221 11:36:27.533048 39073 net.cpp:406] quantized_norm1 <- norm1
I1221 11:36:27.533059 39073 net.cpp:380] quantized_norm1 -> quantized_norm1
I1221 11:36:27.533071 39073 net.cpp:122] Setting up quantized_norm1
I1221 11:36:27.533082 39073 net.cpp:129] Top shape: 20 96 27 27 (1399680)
I1221 11:36:27.533089 39073 net.cpp:137] Memory required for data: 140057120
I1221 11:36:27.533097 39073 layer_factory.hpp:77] Creating layer conv2
I1221 11:36:27.533113 39073 net.cpp:84] Creating Layer conv2
I1221 11:36:27.533124 39073 net.cpp:406] conv2 <- quantized_norm1
I1221 11:36:27.533138 39073 net.cpp:380] conv2 -> conv2
I1221 11:36:27.542506 39073 net.cpp:122] Setting up conv2
I1221 11:36:27.542524 39073 net.cpp:129] Top shape: 20 256 27 27 (3732480)
I1221 11:36:27.542546 39073 net.cpp:137] Memory required for data: 154987040
I1221 11:36:27.542562 39073 layer_factory.hpp:77] Creating layer quantized_conv2
I1221 11:36:27.542577 39073 net.cpp:84] Creating Layer quantized_conv2
I1221 11:36:27.542587 39073 net.cpp:406] quantized_conv2 <- conv2
I1221 11:36:27.542598 39073 net.cpp:380] quantized_conv2 -> quantized_conv2
I1221 11:36:27.542611 39073 net.cpp:122] Setting up quantized_conv2
I1221 11:36:27.542623 39073 net.cpp:129] Top shape: 20 256 27 27 (3732480)
I1221 11:36:27.542630 39073 net.cpp:137] Memory required for data: 169916960
I1221 11:36:27.542639 39073 layer_factory.hpp:77] Creating layer relu2
I1221 11:36:27.542649 39073 net.cpp:84] Creating Layer relu2
I1221 11:36:27.542659 39073 net.cpp:406] relu2 <- quantized_conv2
I1221 11:36:27.542675 39073 net.cpp:380] relu2 -> relu2
I1221 11:36:27.542690 39073 net.cpp:122] Setting up relu2
I1221 11:36:27.542701 39073 net.cpp:129] Top shape: 20 256 27 27 (3732480)
I1221 11:36:27.542711 39073 net.cpp:137] Memory required for data: 184846880
I1221 11:36:27.542718 39073 layer_factory.hpp:77] Creating layer quantized_relu2
I1221 11:36:27.542732 39073 net.cpp:84] Creating Layer quantized_relu2
I1221 11:36:27.542742 39073 net.cpp:406] quantized_relu2 <- relu2
I1221 11:36:27.542752 39073 net.cpp:380] quantized_relu2 -> quantized_relu2
I1221 11:36:27.542765 39073 net.cpp:122] Setting up quantized_relu2
I1221 11:36:27.542775 39073 net.cpp:129] Top shape: 20 256 27 27 (3732480)
I1221 11:36:27.542783 39073 net.cpp:137] Memory required for data: 199776800
I1221 11:36:27.542793 39073 layer_factory.hpp:77] Creating layer pool2
I1221 11:36:27.542803 39073 net.cpp:84] Creating Layer pool2
I1221 11:36:27.542811 39073 net.cpp:406] pool2 <- quantized_relu2
I1221 11:36:27.542829 39073 net.cpp:380] pool2 -> pool2
I1221 11:36:27.542842 39073 net.cpp:122] Setting up pool2
I1221 11:36:27.542853 39073 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1221 11:36:27.542861 39073 net.cpp:137] Memory required for data: 203237920
I1221 11:36:27.542870 39073 layer_factory.hpp:77] Creating layer quantized_pool2
I1221 11:36:27.542883 39073 net.cpp:84] Creating Layer quantized_pool2
I1221 11:36:27.542892 39073 net.cpp:406] quantized_pool2 <- pool2
I1221 11:36:27.542903 39073 net.cpp:380] quantized_pool2 -> quantized_pool2
I1221 11:36:27.542915 39073 net.cpp:122] Setting up quantized_pool2
I1221 11:36:27.542925 39073 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1221 11:36:27.542933 39073 net.cpp:137] Memory required for data: 206699040
I1221 11:36:27.542943 39073 layer_factory.hpp:77] Creating layer norm2
I1221 11:36:27.542960 39073 net.cpp:84] Creating Layer norm2
I1221 11:36:27.542971 39073 net.cpp:406] norm2 <- quantized_pool2
I1221 11:36:27.542981 39073 net.cpp:380] norm2 -> norm2
I1221 11:36:27.542994 39073 net.cpp:122] Setting up norm2
I1221 11:36:27.543009 39073 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1221 11:36:27.543017 39073 net.cpp:137] Memory required for data: 210160160
I1221 11:36:27.543025 39073 layer_factory.hpp:77] Creating layer quantized_norm2
I1221 11:36:27.543036 39073 net.cpp:84] Creating Layer quantized_norm2
I1221 11:36:27.543045 39073 net.cpp:406] quantized_norm2 <- norm2
I1221 11:36:27.543056 39073 net.cpp:380] quantized_norm2 -> quantized_norm2
I1221 11:36:27.543066 39073 net.cpp:122] Setting up quantized_norm2
I1221 11:36:27.543076 39073 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1221 11:36:27.543084 39073 net.cpp:137] Memory required for data: 213621280
I1221 11:36:27.543092 39073 layer_factory.hpp:77] Creating layer conv3
I1221 11:36:27.543108 39073 net.cpp:84] Creating Layer conv3
I1221 11:36:27.543118 39073 net.cpp:406] conv3 <- quantized_norm2
I1221 11:36:27.543133 39073 net.cpp:380] conv3 -> conv3
I1221 11:36:27.568939 39073 net.cpp:122] Setting up conv3
I1221 11:36:27.568989 39073 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1221 11:36:27.568998 39073 net.cpp:137] Memory required for data: 218812960
I1221 11:36:27.569016 39073 layer_factory.hpp:77] Creating layer quantized_conv3
I1221 11:36:27.569067 39073 net.cpp:84] Creating Layer quantized_conv3
I1221 11:36:27.569080 39073 net.cpp:406] quantized_conv3 <- conv3
I1221 11:36:27.569093 39073 net.cpp:380] quantized_conv3 -> quantized_conv3
I1221 11:36:27.569118 39073 net.cpp:122] Setting up quantized_conv3
I1221 11:36:27.569128 39073 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1221 11:36:27.569136 39073 net.cpp:137] Memory required for data: 224004640
I1221 11:36:27.569144 39073 layer_factory.hpp:77] Creating layer relu3
I1221 11:36:27.569155 39073 net.cpp:84] Creating Layer relu3
I1221 11:36:27.569164 39073 net.cpp:406] relu3 <- quantized_conv3
I1221 11:36:27.569175 39073 net.cpp:380] relu3 -> relu3
I1221 11:36:27.569185 39073 net.cpp:122] Setting up relu3
I1221 11:36:27.569195 39073 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1221 11:36:27.569203 39073 net.cpp:137] Memory required for data: 229196320
I1221 11:36:27.569214 39073 layer_factory.hpp:77] Creating layer quantized_relu3
I1221 11:36:27.569227 39073 net.cpp:84] Creating Layer quantized_relu3
I1221 11:36:27.569237 39073 net.cpp:406] quantized_relu3 <- relu3
I1221 11:36:27.569247 39073 net.cpp:380] quantized_relu3 -> quantized_relu3
I1221 11:36:27.569259 39073 net.cpp:122] Setting up quantized_relu3
I1221 11:36:27.569270 39073 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1221 11:36:27.569278 39073 net.cpp:137] Memory required for data: 234388000
I1221 11:36:27.569286 39073 layer_factory.hpp:77] Creating layer conv4
I1221 11:36:27.569304 39073 net.cpp:84] Creating Layer conv4
I1221 11:36:27.569314 39073 net.cpp:406] conv4 <- quantized_relu3
I1221 11:36:27.569327 39073 net.cpp:380] conv4 -> conv4
I1221 11:36:27.588742 39073 net.cpp:122] Setting up conv4
I1221 11:36:27.588784 39073 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1221 11:36:27.588794 39073 net.cpp:137] Memory required for data: 239579680
I1221 11:36:27.588807 39073 layer_factory.hpp:77] Creating layer quantized_conv4
I1221 11:36:27.588824 39073 net.cpp:84] Creating Layer quantized_conv4
I1221 11:36:27.588834 39073 net.cpp:406] quantized_conv4 <- conv4
I1221 11:36:27.588847 39073 net.cpp:380] quantized_conv4 -> quantized_conv4
I1221 11:36:27.588865 39073 net.cpp:122] Setting up quantized_conv4
I1221 11:36:27.588874 39073 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1221 11:36:27.588882 39073 net.cpp:137] Memory required for data: 244771360
I1221 11:36:27.588891 39073 layer_factory.hpp:77] Creating layer relu4
I1221 11:36:27.588901 39073 net.cpp:84] Creating Layer relu4
I1221 11:36:27.588908 39073 net.cpp:406] relu4 <- quantized_conv4
I1221 11:36:27.588922 39073 net.cpp:380] relu4 -> relu4
I1221 11:36:27.588933 39073 net.cpp:122] Setting up relu4
I1221 11:36:27.588944 39073 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1221 11:36:27.588953 39073 net.cpp:137] Memory required for data: 249963040
I1221 11:36:27.588960 39073 layer_factory.hpp:77] Creating layer quantized_relu4
I1221 11:36:27.588971 39073 net.cpp:84] Creating Layer quantized_relu4
I1221 11:36:27.588979 39073 net.cpp:406] quantized_relu4 <- relu4
I1221 11:36:27.588992 39073 net.cpp:380] quantized_relu4 -> quantized_relu4
I1221 11:36:27.589004 39073 net.cpp:122] Setting up quantized_relu4
I1221 11:36:27.589015 39073 net.cpp:129] Top shape: 20 384 13 13 (1297920)
I1221 11:36:27.589022 39073 net.cpp:137] Memory required for data: 255154720
I1221 11:36:27.589030 39073 layer_factory.hpp:77] Creating layer conv5
I1221 11:36:27.589048 39073 net.cpp:84] Creating Layer conv5
I1221 11:36:27.589058 39073 net.cpp:406] conv5 <- quantized_relu4
I1221 11:36:27.589072 39073 net.cpp:380] conv5 -> conv5
I1221 11:36:27.601985 39073 net.cpp:122] Setting up conv5
I1221 11:36:27.602005 39073 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1221 11:36:27.602012 39073 net.cpp:137] Memory required for data: 258615840
I1221 11:36:27.602027 39073 layer_factory.hpp:77] Creating layer quantized_conv5
I1221 11:36:27.602039 39073 net.cpp:84] Creating Layer quantized_conv5
I1221 11:36:27.602049 39073 net.cpp:406] quantized_conv5 <- conv5
I1221 11:36:27.602062 39073 net.cpp:380] quantized_conv5 -> quantized_conv5
I1221 11:36:27.602110 39073 net.cpp:122] Setting up quantized_conv5
I1221 11:36:27.602123 39073 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1221 11:36:27.602131 39073 net.cpp:137] Memory required for data: 262076960
I1221 11:36:27.602139 39073 layer_factory.hpp:77] Creating layer relu5
I1221 11:36:27.602149 39073 net.cpp:84] Creating Layer relu5
I1221 11:36:27.602157 39073 net.cpp:406] relu5 <- quantized_conv5
I1221 11:36:27.602170 39073 net.cpp:380] relu5 -> relu5
I1221 11:36:27.602185 39073 net.cpp:122] Setting up relu5
I1221 11:36:27.602197 39073 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1221 11:36:27.602205 39073 net.cpp:137] Memory required for data: 265538080
I1221 11:36:27.602213 39073 layer_factory.hpp:77] Creating layer quantized_relu5
I1221 11:36:27.602223 39073 net.cpp:84] Creating Layer quantized_relu5
I1221 11:36:27.602232 39073 net.cpp:406] quantized_relu5 <- relu5
I1221 11:36:27.602246 39073 net.cpp:380] quantized_relu5 -> quantized_relu5
I1221 11:36:27.602257 39073 net.cpp:122] Setting up quantized_relu5
I1221 11:36:27.602268 39073 net.cpp:129] Top shape: 20 256 13 13 (865280)
I1221 11:36:27.602275 39073 net.cpp:137] Memory required for data: 268999200
I1221 11:36:27.602283 39073 layer_factory.hpp:77] Creating layer pool5
I1221 11:36:27.602298 39073 net.cpp:84] Creating Layer pool5
I1221 11:36:27.602306 39073 net.cpp:406] pool5 <- quantized_relu5
I1221 11:36:27.602316 39073 net.cpp:380] pool5 -> pool5
I1221 11:36:27.602331 39073 net.cpp:122] Setting up pool5
I1221 11:36:27.602341 39073 net.cpp:129] Top shape: 20 256 6 6 (184320)
I1221 11:36:27.602349 39073 net.cpp:137] Memory required for data: 269736480
I1221 11:36:27.602357 39073 layer_factory.hpp:77] Creating layer quantized_pool5
I1221 11:36:27.602375 39073 net.cpp:84] Creating Layer quantized_pool5
I1221 11:36:27.602383 39073 net.cpp:406] quantized_pool5 <- pool5
I1221 11:36:27.602394 39073 net.cpp:380] quantized_pool5 -> quantized_pool5
I1221 11:36:27.602406 39073 net.cpp:122] Setting up quantized_pool5
I1221 11:36:27.602421 39073 net.cpp:129] Top shape: 20 256 6 6 (184320)
I1221 11:36:27.602428 39073 net.cpp:137] Memory required for data: 270473760
I1221 11:36:27.602437 39073 layer_factory.hpp:77] Creating layer fc6
I1221 11:36:27.602449 39073 net.cpp:84] Creating Layer fc6
I1221 11:36:27.602458 39073 net.cpp:406] fc6 <- quantized_pool5
I1221 11:36:27.602469 39073 net.cpp:380] fc6 -> fc6
I1221 11:36:28.666059 39073 net.cpp:122] Setting up fc6
I1221 11:36:28.666162 39073 net.cpp:129] Top shape: 20 4096 (81920)
I1221 11:36:28.666177 39073 net.cpp:137] Memory required for data: 270801440
I1221 11:36:28.666195 39073 layer_factory.hpp:77] Creating layer quantized_fc6
I1221 11:36:28.666214 39073 net.cpp:84] Creating Layer quantized_fc6
I1221 11:36:28.666226 39073 net.cpp:406] quantized_fc6 <- fc6
I1221 11:36:28.666244 39073 net.cpp:380] quantized_fc6 -> quantized_fc6
I1221 11:36:28.666263 39073 net.cpp:122] Setting up quantized_fc6
I1221 11:36:28.666275 39073 net.cpp:129] Top shape: 20 4096 (81920)
I1221 11:36:28.666282 39073 net.cpp:137] Memory required for data: 271129120
I1221 11:36:28.666291 39073 layer_factory.hpp:77] Creating layer relu6
I1221 11:36:28.666306 39073 net.cpp:84] Creating Layer relu6
I1221 11:36:28.666314 39073 net.cpp:406] relu6 <- quantized_fc6
I1221 11:36:28.666328 39073 net.cpp:380] relu6 -> relu6
I1221 11:36:28.666342 39073 net.cpp:122] Setting up relu6
I1221 11:36:28.666352 39073 net.cpp:129] Top shape: 20 4096 (81920)
I1221 11:36:28.666360 39073 net.cpp:137] Memory required for data: 271456800
I1221 11:36:28.666368 39073 layer_factory.hpp:77] Creating layer quantized_relu6
I1221 11:36:28.666378 39073 net.cpp:84] Creating Layer quantized_relu6
I1221 11:36:28.666388 39073 net.cpp:406] quantized_relu6 <- relu6
I1221 11:36:28.666399 39073 net.cpp:380] quantized_relu6 -> quantized_relu6
I1221 11:36:28.666410 39073 net.cpp:122] Setting up quantized_relu6
I1221 11:36:28.666420 39073 net.cpp:129] Top shape: 20 4096 (81920)
I1221 11:36:28.666429 39073 net.cpp:137] Memory required for data: 271784480
I1221 11:36:28.666476 39073 layer_factory.hpp:77] Creating layer fc7
I1221 11:36:28.666491 39073 net.cpp:84] Creating Layer fc7
I1221 11:36:28.666501 39073 net.cpp:406] fc7 <- quantized_relu6
I1221 11:36:28.666514 39073 net.cpp:380] fc7 -> fc7
I1221 11:36:29.140435 39073 net.cpp:122] Setting up fc7
I1221 11:36:29.140540 39073 net.cpp:129] Top shape: 20 4096 (81920)
I1221 11:36:29.140552 39073 net.cpp:137] Memory required for data: 272112160
I1221 11:36:29.140573 39073 layer_factory.hpp:77] Creating layer quantized_fc7
I1221 11:36:29.140592 39073 net.cpp:84] Creating Layer quantized_fc7
I1221 11:36:29.140604 39073 net.cpp:406] quantized_fc7 <- fc7
I1221 11:36:29.140621 39073 net.cpp:380] quantized_fc7 -> quantized_fc7
I1221 11:36:29.140641 39073 net.cpp:122] Setting up quantized_fc7
I1221 11:36:29.140652 39073 net.cpp:129] Top shape: 20 4096 (81920)
I1221 11:36:29.140661 39073 net.cpp:137] Memory required for data: 272439840
I1221 11:36:29.140672 39073 layer_factory.hpp:77] Creating layer relu7
I1221 11:36:29.140691 39073 net.cpp:84] Creating Layer relu7
I1221 11:36:29.140699 39073 net.cpp:406] relu7 <- quantized_fc7
I1221 11:36:29.140712 39073 net.cpp:380] relu7 -> relu7
I1221 11:36:29.140723 39073 net.cpp:122] Setting up relu7
I1221 11:36:29.140734 39073 net.cpp:129] Top shape: 20 4096 (81920)
I1221 11:36:29.140743 39073 net.cpp:137] Memory required for data: 272767520
I1221 11:36:29.140751 39073 layer_factory.hpp:77] Creating layer quantized_relu7
I1221 11:36:29.140763 39073 net.cpp:84] Creating Layer quantized_relu7
I1221 11:36:29.140771 39073 net.cpp:406] quantized_relu7 <- relu7
I1221 11:36:29.140784 39073 net.cpp:380] quantized_relu7 -> quantized_relu7
I1221 11:36:29.140794 39073 net.cpp:122] Setting up quantized_relu7
I1221 11:36:29.140805 39073 net.cpp:129] Top shape: 20 4096 (81920)
I1221 11:36:29.140812 39073 net.cpp:137] Memory required for data: 273095200
I1221 11:36:29.140820 39073 layer_factory.hpp:77] Creating layer fc8
I1221 11:36:29.140835 39073 net.cpp:84] Creating Layer fc8
I1221 11:36:29.140842 39073 net.cpp:406] fc8 <- quantized_relu7
I1221 11:36:29.140856 39073 net.cpp:380] fc8 -> fc8
I1221 11:36:29.147377 39073 net.cpp:122] Setting up fc8
I1221 11:36:29.147444 39073 net.cpp:129] Top shape: 20 1000 (20000)
I1221 11:36:29.147454 39073 net.cpp:137] Memory required for data: 273175200
I1221 11:36:29.147470 39073 layer_factory.hpp:77] Creating layer quantized_fc8
I1221 11:36:29.147488 39073 net.cpp:84] Creating Layer quantized_fc8
I1221 11:36:29.147500 39073 net.cpp:406] quantized_fc8 <- fc8
I1221 11:36:29.147516 39073 net.cpp:380] quantized_fc8 -> quantized_fc8
I1221 11:36:29.147534 39073 net.cpp:122] Setting up quantized_fc8
I1221 11:36:29.147545 39073 net.cpp:129] Top shape: 20 1000 (20000)
I1221 11:36:29.147553 39073 net.cpp:137] Memory required for data: 273255200
I1221 11:36:29.147562 39073 layer_factory.hpp:77] Creating layer quantized_fc8_quantized_fc8_0_split
I1221 11:36:29.147575 39073 net.cpp:84] Creating Layer quantized_fc8_quantized_fc8_0_split
I1221 11:36:29.147585 39073 net.cpp:406] quantized_fc8_quantized_fc8_0_split <- quantized_fc8
I1221 11:36:29.147595 39073 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_0
I1221 11:36:29.147609 39073 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_1
I1221 11:36:29.147622 39073 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_2
I1221 11:36:29.147634 39073 net.cpp:380] quantized_fc8_quantized_fc8_0_split -> quantized_fc8_quantized_fc8_0_split_3
I1221 11:36:29.147647 39073 net.cpp:122] Setting up quantized_fc8_quantized_fc8_0_split
I1221 11:36:29.147657 39073 net.cpp:129] Top shape: 20 1000 (20000)
I1221 11:36:29.147666 39073 net.cpp:129] Top shape: 20 1000 (20000)
I1221 11:36:29.147675 39073 net.cpp:129] Top shape: 20 1000 (20000)
I1221 11:36:29.147683 39073 net.cpp:129] Top shape: 20 1000 (20000)
I1221 11:36:29.147691 39073 net.cpp:137] Memory required for data: 273575200
I1221 11:36:29.147699 39073 layer_factory.hpp:77] Creating layer probs
I1221 11:36:29.147749 39073 net.cpp:84] Creating Layer probs
I1221 11:36:29.147759 39073 net.cpp:406] probs <- quantized_fc8_quantized_fc8_0_split_0
I1221 11:36:29.147773 39073 net.cpp:380] probs -> probs
I1221 11:36:29.147794 39073 net.cpp:122] Setting up probs
I1221 11:36:29.147804 39073 net.cpp:129] Top shape: 20 1000 (20000)
I1221 11:36:29.147814 39073 net.cpp:137] Memory required for data: 273655200
I1221 11:36:29.147821 39073 layer_factory.hpp:77] Creating layer loss
I1221 11:36:29.147833 39073 net.cpp:84] Creating Layer loss
I1221 11:36:29.147842 39073 net.cpp:406] loss <- quantized_fc8_quantized_fc8_0_split_1
I1221 11:36:29.147851 39073 net.cpp:406] loss <- label_data_1_split_0
I1221 11:36:29.147861 39073 net.cpp:380] loss -> loss
I1221 11:36:29.147876 39073 layer_factory.hpp:77] Creating layer loss
I1221 11:36:29.147959 39073 net.cpp:122] Setting up loss
I1221 11:36:29.147974 39073 net.cpp:129] Top shape: (1)
I1221 11:36:29.147981 39073 net.cpp:132]     with loss weight 1
I1221 11:36:29.148001 39073 net.cpp:137] Memory required for data: 273655204
I1221 11:36:29.148010 39073 layer_factory.hpp:77] Creating layer acc_top1
I1221 11:36:29.148026 39073 net.cpp:84] Creating Layer acc_top1
I1221 11:36:29.148036 39073 net.cpp:406] acc_top1 <- quantized_fc8_quantized_fc8_0_split_2
I1221 11:36:29.148046 39073 net.cpp:406] acc_top1 <- label_data_1_split_1
I1221 11:36:29.148056 39073 net.cpp:380] acc_top1 -> acc_top1
I1221 11:36:29.148068 39073 net.cpp:122] Setting up acc_top1
I1221 11:36:29.148078 39073 net.cpp:129] Top shape: (1)
I1221 11:36:29.148087 39073 net.cpp:137] Memory required for data: 273655208
I1221 11:36:29.148094 39073 layer_factory.hpp:77] Creating layer acc_top5
I1221 11:36:29.148108 39073 net.cpp:84] Creating Layer acc_top5
I1221 11:36:29.148116 39073 net.cpp:406] acc_top5 <- quantized_fc8_quantized_fc8_0_split_3
I1221 11:36:29.148125 39073 net.cpp:406] acc_top5 <- label_data_1_split_2
I1221 11:36:29.148135 39073 net.cpp:380] acc_top5 -> acc_top5
I1221 11:36:29.148147 39073 net.cpp:122] Setting up acc_top5
I1221 11:36:29.148157 39073 net.cpp:129] Top shape: (1)
I1221 11:36:29.148165 39073 net.cpp:137] Memory required for data: 273655212
I1221 11:36:29.148174 39073 net.cpp:200] acc_top5 does not need backward computation.
I1221 11:36:29.148182 39073 net.cpp:200] acc_top1 does not need backward computation.
I1221 11:36:29.148190 39073 net.cpp:198] loss needs backward computation.
I1221 11:36:29.148198 39073 net.cpp:200] probs does not need backward computation.
I1221 11:36:29.148207 39073 net.cpp:198] quantized_fc8_quantized_fc8_0_split needs backward computation.
I1221 11:36:29.148216 39073 net.cpp:198] quantized_fc8 needs backward computation.
I1221 11:36:29.148223 39073 net.cpp:198] fc8 needs backward computation.
I1221 11:36:29.148232 39073 net.cpp:198] quantized_relu7 needs backward computation.
I1221 11:36:29.148241 39073 net.cpp:198] relu7 needs backward computation.
I1221 11:36:29.148249 39073 net.cpp:198] quantized_fc7 needs backward computation.
I1221 11:36:29.148257 39073 net.cpp:198] fc7 needs backward computation.
I1221 11:36:29.148265 39073 net.cpp:198] quantized_relu6 needs backward computation.
I1221 11:36:29.148274 39073 net.cpp:198] relu6 needs backward computation.
I1221 11:36:29.148283 39073 net.cpp:198] quantized_fc6 needs backward computation.
I1221 11:36:29.148291 39073 net.cpp:198] fc6 needs backward computation.
I1221 11:36:29.148300 39073 net.cpp:198] quantized_pool5 needs backward computation.
I1221 11:36:29.148308 39073 net.cpp:198] pool5 needs backward computation.
I1221 11:36:29.148317 39073 net.cpp:198] quantized_relu5 needs backward computation.
I1221 11:36:29.148325 39073 net.cpp:198] relu5 needs backward computation.
I1221 11:36:29.148334 39073 net.cpp:198] quantized_conv5 needs backward computation.
I1221 11:36:29.148342 39073 net.cpp:198] conv5 needs backward computation.
I1221 11:36:29.148351 39073 net.cpp:198] quantized_relu4 needs backward computation.
I1221 11:36:29.148360 39073 net.cpp:198] relu4 needs backward computation.
I1221 11:36:29.148367 39073 net.cpp:198] quantized_conv4 needs backward computation.
I1221 11:36:29.148388 39073 net.cpp:198] conv4 needs backward computation.
I1221 11:36:29.148398 39073 net.cpp:198] quantized_relu3 needs backward computation.
I1221 11:36:29.148407 39073 net.cpp:198] relu3 needs backward computation.
I1221 11:36:29.148416 39073 net.cpp:198] quantized_conv3 needs backward computation.
I1221 11:36:29.148424 39073 net.cpp:198] conv3 needs backward computation.
I1221 11:36:29.148433 39073 net.cpp:198] quantized_norm2 needs backward computation.
I1221 11:36:29.148442 39073 net.cpp:198] norm2 needs backward computation.
I1221 11:36:29.148453 39073 net.cpp:198] quantized_pool2 needs backward computation.
I1221 11:36:29.148463 39073 net.cpp:198] pool2 needs backward computation.
I1221 11:36:29.148471 39073 net.cpp:198] quantized_relu2 needs backward computation.
I1221 11:36:29.148479 39073 net.cpp:198] relu2 needs backward computation.
I1221 11:36:29.148488 39073 net.cpp:198] quantized_conv2 needs backward computation.
I1221 11:36:29.148496 39073 net.cpp:198] conv2 needs backward computation.
I1221 11:36:29.148504 39073 net.cpp:198] quantized_norm1 needs backward computation.
I1221 11:36:29.148512 39073 net.cpp:198] norm1 needs backward computation.
I1221 11:36:29.148521 39073 net.cpp:198] quantized_pool1 needs backward computation.
I1221 11:36:29.148530 39073 net.cpp:198] pool1 needs backward computation.
I1221 11:36:29.148538 39073 net.cpp:198] quantized_relu1 needs backward computation.
I1221 11:36:29.148546 39073 net.cpp:198] relu1 needs backward computation.
I1221 11:36:29.148555 39073 net.cpp:198] quantized_conv1 needs backward computation.
I1221 11:36:29.148563 39073 net.cpp:198] conv1 needs backward computation.
I1221 11:36:29.148571 39073 net.cpp:200] quantized_data does not need backward computation.
I1221 11:36:29.148581 39073 net.cpp:200] label_data_1_split does not need backward computation.
I1221 11:36:29.148589 39073 net.cpp:200] data does not need backward computation.
I1221 11:36:29.148597 39073 net.cpp:242] This network produces output acc_top1
I1221 11:36:29.148605 39073 net.cpp:242] This network produces output acc_top5
I1221 11:36:29.148613 39073 net.cpp:242] This network produces output loss
I1221 11:36:29.148622 39073 net.cpp:242] This network produces output probs
I1221 11:36:29.148656 39073 net.cpp:255] Network initialization done.
I1221 11:36:29.148824 39073 solver.cpp:56] Solver scaffolding done.
I1221 11:36:29.148885 39073 caffe.cpp:155] Finetuning from ./qnn_try1/bvlc_reference_caffenet.caffemodel
I1221 11:36:29.671887 39073 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ./qnn_try1/bvlc_reference_caffenet.caffemodel
I1221 11:36:29.672005 39073 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1221 11:36:29.672019 39073 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1221 11:36:29.672749 39073 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./qnn_try1/bvlc_reference_caffenet.caffemodel
I1221 11:36:29.901422 39073 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1221 11:36:29.943670 39073 net.cpp:744] Ignoring source layer drop6
I1221 11:36:29.960870 39073 net.cpp:744] Ignoring source layer drop7
I1221 11:36:30.457415 39073 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ./qnn_try1/bvlc_reference_caffenet.caffemodel
I1221 11:36:30.457550 39073 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W1221 11:36:30.457561 39073 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1221 11:36:30.457593 39073 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./qnn_try1/bvlc_reference_caffenet.caffemodel
I1221 11:36:30.685878 39073 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I1221 11:36:30.728663 39073 net.cpp:744] Ignoring source layer drop6
I1221 11:36:30.745748 39073 net.cpp:744] Ignoring source layer drop7
I1221 11:36:30.752768 39073 caffe.cpp:248] Starting Optimization
I1221 11:36:30.752835 39073 solver.cpp:273] Solving 
I1221 11:36:30.752846 39073 solver.cpp:274] Learning Rate Policy: step
I1221 11:36:30.842701 39073 solver.cpp:331] Iteration 0, Testing net (#0)
I1221 11:36:56.555910 39073 solver.cpp:400]     Test net output #0: acc_top1 = 0.5
I1221 11:36:56.556100 39073 solver.cpp:400]     Test net output #1: acc_top5 = 0.78
I1221 11:36:56.556124 39073 solver.cpp:400]     Test net output #2: loss = 1.85223 (* 1 = 1.85223 loss)
I1221 11:37:07.017653 39073 solver.cpp:218] Iteration 0 (0 iter/s, 36.264s/20 iters), loss = 1.07932
I1221 11:37:07.017758 39073 solver.cpp:238]     Train net output #0: acc_top1 = 0.75
I1221 11:37:07.017778 39073 solver.cpp:238]     Train net output #1: acc_top5 = 0.9375
I1221 11:37:07.017797 39073 solver.cpp:238]     Train net output #2: loss = 1.07932 (* 1 = 1.07932 loss)
I1221 11:37:07.027664 39073 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I1221 11:40:46.213413 39073 solver.cpp:218] Iteration 20 (0.091243 iter/s, 219.195s/20 iters), loss = 1.01301
I1221 11:40:46.213734 39073 solver.cpp:238]     Train net output #0: acc_top1 = 0.71875
I1221 11:40:46.213771 39073 solver.cpp:238]     Train net output #1: acc_top5 = 0.9375
I1221 11:40:46.213799 39073 solver.cpp:238]     Train net output #2: loss = 1.01301 (* 1 = 1.01301 loss)
I1221 11:40:46.224558 39073 sgd_solver.cpp:105] Iteration 20, lr = 0.01
I1221 11:44:21.653435 39073 solver.cpp:218] Iteration 40 (0.0928337 iter/s, 215.439s/20 iters), loss = 1.07533
I1221 11:44:21.653758 39073 solver.cpp:238]     Train net output #0: acc_top1 = 0.65625
I1221 11:44:21.653784 39073 solver.cpp:238]     Train net output #1: acc_top5 = 0.90625
I1221 11:44:21.653798 39073 solver.cpp:238]     Train net output #2: loss = 1.07533 (* 1 = 1.07533 loss)
I1221 11:44:21.664574 39073 sgd_solver.cpp:105] Iteration 40, lr = 0.01
I1221 11:47:58.429003 39073 solver.cpp:218] Iteration 60 (0.0922616 iter/s, 216.775s/20 iters), loss = 1.06982
I1221 11:47:58.429334 39073 solver.cpp:238]     Train net output #0: acc_top1 = 0.75
I1221 11:47:58.429360 39073 solver.cpp:238]     Train net output #1: acc_top5 = 0.875
I1221 11:47:58.429375 39073 solver.cpp:238]     Train net output #2: loss = 1.06982 (* 1 = 1.06982 loss)
I1221 11:47:58.439414 39073 sgd_solver.cpp:105] Iteration 60, lr = 0.01
I1221 11:51:34.865053 39073 solver.cpp:218] Iteration 80 (0.0924065 iter/s, 216.435s/20 iters), loss = 1.31919
I1221 11:51:34.865438 39073 solver.cpp:238]     Train net output #0: acc_top1 = 0.65625
I1221 11:51:34.865463 39073 solver.cpp:238]     Train net output #1: acc_top5 = 0.875
I1221 11:51:34.865478 39073 solver.cpp:238]     Train net output #2: loss = 1.31919 (* 1 = 1.31919 loss)
I1221 11:51:34.875608 39073 sgd_solver.cpp:105] Iteration 80, lr = 0.01
I1221 11:54:59.530695 39073 solver.cpp:331] Iteration 100, Testing net (#0)
I1221 11:55:25.077980 39073 solver.cpp:400]     Test net output #0: acc_top1 = 0.51
I1221 11:55:25.078054 39073 solver.cpp:400]     Test net output #1: acc_top5 = 0.785
I1221 11:55:25.078073 39073 solver.cpp:400]     Test net output #2: loss = 2.03521 (* 1 = 2.03521 loss)
I1221 11:55:35.243139 39073 solver.cpp:218] Iteration 100 (0.0832026 iter/s, 240.377s/20 iters), loss = 1.13916
I1221 11:55:35.243454 39073 solver.cpp:238]     Train net output #0: acc_top1 = 0.6875
I1221 11:55:35.243490 39073 solver.cpp:238]     Train net output #1: acc_top5 = 0.9375
I1221 11:55:35.243525 39073 solver.cpp:238]     Train net output #2: loss = 1.13916 (* 1 = 1.13916 loss)
I1221 11:55:35.254233 39073 sgd_solver.cpp:105] Iteration 100, lr = 0.01
I1221 11:59:09.784911 39073 solver.cpp:218] Iteration 120 (0.0932223 iter/s, 214.541s/20 iters), loss = 1.48681
I1221 11:59:09.785054 39073 solver.cpp:238]     Train net output #0: acc_top1 = 0.625
I1221 11:59:09.785070 39073 solver.cpp:238]     Train net output #1: acc_top5 = 0.84375
I1221 11:59:09.785087 39073 solver.cpp:238]     Train net output #2: loss = 1.48681 (* 1 = 1.48681 loss)
I1221 11:59:09.795001 39073 sgd_solver.cpp:105] Iteration 120, lr = 0.01
I1221 12:02:44.223392 39073 solver.cpp:218] Iteration 140 (0.0932671 iter/s, 214.438s/20 iters), loss = 1.10048
I1221 12:02:44.223589 39073 solver.cpp:238]     Train net output #0: acc_top1 = 0.71875
I1221 12:02:44.223626 39073 solver.cpp:238]     Train net output #1: acc_top5 = 0.90625
I1221 12:02:44.223661 39073 solver.cpp:238]     Train net output #2: loss = 1.10048 (* 1 = 1.10048 loss)
I1221 12:02:44.234267 39073 sgd_solver.cpp:105] Iteration 140, lr = 0.01
  C-c C-cI1221 12:02:55.560492 39073 solver.cpp:450] Snapshotting to binary proto file qnn_try1/QNN-train_iter_142.caffemodel
I1221 12:02:56.457034 39073 sgd_solver.cpp:273] Snapshotting solver state to binary proto file qnn_try1/QNN-train_iter_142.solverstate
I1221 12:02:56.868541 39073 solver.cpp:295] Optimization stopped early.
I1221 12:02:56.868594 39073 caffe.cpp:259] Optimization Done.